<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Brooke K. Ryan | Machine Visual Acuity</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Machine Visual Acuity | Brooke K. Ryan</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Machine Visual Acuity" />
<meta name="author" content="Brooke K. Ryan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this project, we aim to gain insights about human visual acuity by applying these tests to machines. The main goal is to first train a convolutional-based neural network to recognize optotypes with low amounts of distortions so that it can use its knowledge to classify an unseen optotype from a testing set with optotypes with medium to high amounts of distortions. We used transfer learning, with the use of a VGG network, to obtain a baseline model for the problem. Then, we experimented with mixing the testing set with the training set, to determine if that could help the network make better predictions." />
<meta property="og:description" content="In this project, we aim to gain insights about human visual acuity by applying these tests to machines. The main goal is to first train a convolutional-based neural network to recognize optotypes with low amounts of distortions so that it can use its knowledge to classify an unseen optotype from a testing set with optotypes with medium to high amounts of distortions. We used transfer learning, with the use of a VGG network, to obtain a baseline model for the problem. Then, we experimented with mixing the testing set with the training set, to determine if that could help the network make better predictions." />
<link rel="canonical" href="http://localhost:4000/research/machine-visual-acuity.html" />
<meta property="og:url" content="http://localhost:4000/research/machine-visual-acuity.html" />
<meta property="og:site_name" content="Brooke K. Ryan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-15T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Machine Visual Acuity" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Brooke K. Ryan"},"dateModified":"2021-06-15T00:00:00-07:00","datePublished":"2021-06-15T00:00:00-07:00","description":"In this project, we aim to gain insights about human visual acuity by applying these tests to machines. The main goal is to first train a convolutional-based neural network to recognize optotypes with low amounts of distortions so that it can use its knowledge to classify an unseen optotype from a testing set with optotypes with medium to high amounts of distortions. We used transfer learning, with the use of a VGG network, to obtain a baseline model for the problem. Then, we experimented with mixing the testing set with the training set, to determine if that could help the network make better predictions.","headline":"Machine Visual Acuity","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research/machine-visual-acuity.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"Brooke K. Ryan"},"url":"http://localhost:4000/research/machine-visual-acuity.html"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/brooke_ghostie.png">

    <!-- Font Awesome Icons -->
<!--    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.1.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!--    Academic Icons-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">

<!--    Font Share-->
    <link href="https://api.fontshare.com/v2/css?f[]=satoshi@300&display=swap" rel="stylesheet">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Corben:wght@400;700&family=DM+Mono:wght@300&family=DM+Serif+Display&family=Fraunces:opsz,wght@9..144,300;9..144,500;9..144,600;9..144,700;9..144,800&family=Inter:wght@200&family=Lustria&family=Manrope:wght@200;800&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">

<!--    Adobe Fonts (free w Creative cloud)-->
    <link rel="stylesheet" href="https://use.typekit.net/ybg7eys.css">


    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">


    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>



    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html">Brooke K. Ryan</a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="" href="/index.html">
    <img style="margin-right: 1rem; float:left" alt=" " src="/assets/images/brain.png" class="nav-img" height="62" width="62">
</a>
</li>

<li class="nav-item">
<a class="nav-link" href="/index.html">About</a>
</li>

<li class="nav-item">
    <a class="nav-link" href="/research/index.html">Research</a>
</li>

<li class="nav-item">
<a class="nav-link" href="/writing/index.html">Writing</a>
</li>


<!--<li class="nav-item">-->
<!--    <a class="nav-link" href="/about.html">About</a>-->
<!--</li>-->

<li class="nav-item">
<a class="nav-link" href="/teaching/index.html">Teaching</a>
</li>

<!--<li class="nav-item">-->
<!--    <a class="nav-link" href="https://github.com/brookekelseyryan">Programming</a>-->
<!--</li>-->

<!--<li class="nav-item">-->
<!--    <a class="nav-link" href="/media/index.html">Media</a>-->
<!--</li>-->

<li class="nav-item">
    <a class="nav-link" href="/assets/docs/Brooke_Ryan_Resume.pdf">Résumé</a>
</li>

<li class="nav-item">
<a class="nav-link pr-0" href="/assets/docs/Brooke_Ryan_CV.pdf">CV</a>
</li>
            </ul>
<!--            <ul class="navbar-nav ml-auto d-flex align-items-center">-->
<!--                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/author-brooke-ryan.html",
    "title": "Brooke",
    "body": "                        {{page. title}} Follow:         {{ site. authors. brooke. site }}         {{ site. authors. brooke. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , brooke  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 2,
    "url": "http://localhost:4000/writing/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 3,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 4,
    "url": "http://localhost:4000/writing/",
    "title": "Writing",
    "body": "       Blog:     {% for post in paginator. posts %}    {% include main-loop-card. html %}    {% endfor %}                {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev        {% else %}        &laquo;        {% endif %}        {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}        {% endfor %}        {% if paginator. next_page %}        Next &raquo;        {% else %}        &raquo;        {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 5,
    "url": "http://localhost:4000/research/",
    "title": "Research",
    "body": "            Research                                                                                    How do our distinctly human cognitive capabilities arise from the connections between the billions of neurons in the brain? What are the computational properties that enable precise yet generalizable knowledge about the world to be represented amongst neural connections? How does the symphony of internal experience arise from the architectures governing discrete regions of brain function? Such are the questions that fundamentally drive my curiosity and passion for exploring the brain, mind, and artificial representations of intelligence.                     My research leverages computational, behavioral, and neuroimaging techniques aimed at understanding the mechanisms that give rise to our uniquely human cognitive abilities. I am particularly drawn to areas where human intelligence supersedes that of state-of-the-art Artificial Intelligence, such as language acquisition, one-/few-shot learning, memory, and consciousness.                     Below are my past research projects and publications, which additionally span topics including Computer Science education and Deep Learning.                 &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;        Publications:       {% for page in site. publications %}      {% include pub-loop-card. html %}      {% endfor %}  Projects:   {% assign projs = site. projects | sort: 'date' | reverse %}  {% for page in projs %}  {% include pub-loop-card. html %}  {% endfor %}              {% include sidebar-scientist. html %}     "
    }, {
    "id": 6,
    "url": "http://localhost:4000/teaching/",
    "title": "Teaching",
    "body": "   Courses:       {% assign courses = site. teaching | sort: 'date' | reverse %}    {% for course in courses %}                                                  {% if course. source %}                          {% if course. has-code == true %}              {{ course. code | replace:  - ,     }}: {{ course. title }} :               {% else %}              {{ course. title }} :               {% endif %}                              {{ course. institution }},                 {{ course. department }}                                {{ course. quarter }}                             {{ course. description }}                        {% else %}                          {% if course. has-code == true %}              {{ course. code | replace:  - ,     }}: {{ course. title }} :               {% else %}              {{ course. title }} :               {% endif %}                              {{ course. institution }},                 {{ course. department }}                                {{ course. quarter }}                             {{ course. description }}                        {% endif %}                                {% endfor %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/media/",
    "title": "Media",
    "body": "             Media:       {% assign medias = site. media %}      {% assign mediaz = medias | sort: 'date' | reverse %}      {% for post in mediaz %}      {% include media-loop-card. html %}      {% endfor %}              {% include sidebar-brooke. html %}      "
    }, {
    "id": 8,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "  {% if page. url ==  /  %}                                                                                                    welcome!                                              My name is Brooke K. Ryan. I recently obtained my M. S. in Computer Science from UC Irvine, and I'm currently working as a researcher with the Mattar Lab at UC San Diego. Prior to that, I obtained my B. S. in Mathematics-Computer Science from UC San Diego.                                                 I'm interested in uncovering the computational and neural basis for human intelligence—particularly in areas where Artificial Intelligence struggles, such as one-shot learning and language acquisition. This site contains my an archive of my research projects, writing, and courses I've taught.                                                       {% endif %}       Research:     {% assign research = site. publications | concat: site. projects | sort: 'date' | reverse %}    {% for page in research %}    {% if page. hidden != true %}                                  {{ page. title }}        :         {{ page. authors }}                   {{ page. abstract | strip_html | strip_newlines | truncate: 141}}                           {% if page. venue %}          {{ page. venue }}          {% else %}          {{ page. date | date: '%b %Y' }}          {% endif %}                              {% if page. thumbnail %}                                            {% endif %}                          {% include icons. html %}              {% endif %}    {% endfor %}              Publications:               {% for page in site. publications %}                           {{ page. authors_no_links }}. {{ page. title }}. {{ page. venue }}.         &lt;/li&gt;        {% endfor %}      &lt;/ol&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;{% if site. mailchimp-list %}{% endif %}&lt;/div&gt;"
    }, {
    "id": 9,
    "url": "http://localhost:4000/index_backup/",
    "title": "Mundana Free Jekyll Theme",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}                            &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;                              {{ latest_post. title }}        :                   {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}                                       In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                        {{ latest_post. date | date: '%b %d, %Y' }}                                        {%- assign second_post = site. posts[1] -%}          {% if second_post. image %}                        &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                    {% endif %}                        {{ second_post. title }}        :                   In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                      {{ second_post. date | date: '%b %d, %Y' }}                          {%- assign third_post = site. posts[2] -%}          {% if third_post. image %}                        &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                    {% endif %}                        {{ third_post. title }}        :                   In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                      {{ third_post. date | date: '%b %d, %Y' }}                          {%- assign fourth_post = site. posts[3] -%}          {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                    {% endif %}                        {{ fourth_post. title }}        :                   In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                      {{ fourth_post. date | date: '%b %d, %Y' }}                    {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More                        {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}    PAGINATIN BOIIIIS    {% include main-loop-card. html %}    {% endfor %}                {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev        {% else %}        &laquo;        {% endif %}        {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}        {% endfor %}        {% if paginator. next_page %}        Next &raquo;        {% else %}        &raquo;        {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 10,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 11,
    "url": "http://localhost:4000/publications/categories.html",
    "title": "Publications by Categories",
    "body": "            Categories      {% for keyword in site. keywords %}      {{ keyword[0] }}:       {% assign pages_list = category[1] %}      {% for post in pages_list %}      {% if post. title != null %}      {% if group == null or group == post. group %}      {% include main-loop-card. html %}      {% endif %}      {% endif %}      {% endfor %}      {% assign pages_list = nil %}      {% assign group = nil %}      {% endfor %}              {% include sidebar-featured. html %}      "
    }, {
    "id": 12,
    "url": "http://localhost:4000/research/tags.html",
    "title": "Tags",
    "body": "      {% assign tags1 = site. publications | map: 'tags' | join: ',' | split: ',' | uniq %}      {% assign tags2 = site. projects | map: 'tags' | join: ',' | split: ',' | uniq %}      {% assign tags = tags1 | concat: tags2 | uniq %}      {% for tag in tags %}    &lt;h4 class= tag font-weight-bold spanborder text-capitalize  id= {{ tag | replace:    , -  }} &gt;Research tagged '{{ tag }}'&lt;/h4&gt;    {% assign pages_list1 = site. publications | where:  tags , tag %}    {% assign pages_list2 = site. projects | where:  tags , tag %}    {% assign pages_list = pages_list1 | concat: pages_list2 %}    {% for page in pages_list %}    {% if page. title != null %}     {% if group == null or group == post. group %}      {% include pub-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}    "
    }, {
    "id": 13,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 14,
    "url": "http://localhost:4000/blog/how-to-ssh-into-uci-openlab.html",
    "title": "How to Access UCI's OpenLab (via SSH or JupyterHub)",
    "body": "2022/06/22 - In this guide, I will walk you through a few different ways you can access UCI’s OpenLab, which is essentially the campus’s “remote computer lab. ” When I was an undergrad, this process always overwhelmed me (I barely knew how to program, how was I supposed to navigate the terminal and all this SSH business?!) Even now with several years of software engineering experience I always forget the steps for how to do this. So, I’m writing this for my students in ICS 33 this quarter, and also as a reminder for myself because I definitely will forget if I don’t put it somewhere. 🤣 The included YouTube video above walks you through the Prerequisites, as well as the SSH method to connect to OpenLab. In this post, I’ve also included a more user-friendly method to connect to OpenLab which will especially be relevant for my ICS 33 students. All the terminal commands and tools discussed in the video are also included in this post for convenience. So, without further ado, lets jump into the tutorial! PrerequisitesICS Login: The very first thing you need to do is to make sure that you actually have an ICS login. This is actually distinct from your @UCI. edu login. If your major falls under the Bren School of Information and Computer Sciences, then you already have an account (and it should be the same as your UCInetid). If that’s you, then skip to the next section.   First, visit the ICS page to register for a username if you don't already have one. If you’re not sure if you already have one or not, just type in your student ID in the form.   Error message that appeared for me since I already had an ICS login. If you don’t have one yet, then fill out the prompts on the form. According to this page, your account should be made within 2 hours, and should be the same name as your regular UCInetid. Required Tools: VPN: Next, you’re going to need a way to access the VPN. It allows you to connect to the UCI network when you’re not on campus. This in and of itself is not enough to get on OpenLab, but its a prerequisite step (we’ll talk about how to do this below). To get on the VPN, you need to download some software. There’s some really good UCI-made articles on this step, so I won’t re-write this all out.   Follow the steps on the UCI ServiceNow page to download the VPN software. After following these steps, you should have the Cisco AnyConnect VPN client (or equivalent, if it recommended a different one for your OS) downloaded to your machine. Terminal: Finally, you will likely want to have a terminal. If you’re just doing Jupyter Notebooks, its not 100% needed, but you will likely need the terminal to transfer files remotely. Plus, its a good tool to familiarize yourself with for your Computer Science education, anyways, and you will almost definitely be using it in your courses. If you’re using a Mac, you can use the built-in Terminal application (Linux will also have a built-in one you can use). I personally use iTerm2 and I really love it because of the extra functionality such as split-panes and additional appearance customizations. I haven’t been regularly using a Windows-based machine in a few years, but I enjoyed using the beta of Windows Terminal, which looks like now there is a full version available for everyone. I liked using this much better than PuTTY and Windows Git Bash. When in doubt, you can always ask your instructor what they’d recommend. OpenLab Tutorial (Two Different Ways)Now that the prerequisites are out of the way, lets jump into the actual tutorial! Below, I have provided two different methods of accessing OpenLab, depending on what your needs are. The first way gives you a full GUI and remote desktop experience. The second way I’d recommend for more advanced users or students in upper-division classes, which accesses OpenLab through the terminal using SSH. This is personally what I use, because its a bit snappier, but newer students will likely prefer the first method because its more user-friendly. Access a Fully Remote Desktop via JupyterHubThis part of the tutorial will cover how to access UCI’s JupyterHub. This is the way I’d recommend for my ICS 33 students, or others in lower-divison courses. Steps:  Open Cisco AnyConnect and connect to the VPN (documented in this guide, or watch the first part of my video).  Go to the browser, and type in https://hub. ics. uci. edu/And you’re in! From there, you can simply navigate over to Jupyter Notebooks to start working on your programming assignment.           Navigate to hub. ics. uci. edu. Then, click on Python 3 (outlined in red) to open Jupyter Notebooks. Result should look like what I have on the right. Or, you can even use the full desktop by clicking on the “Desktop” icon. This could be useful if you also want to interact with the Terminal or need to go to the browser to download your assignments from Canvas.   Full desktop, with Terminal and Firefox, that you can access via JupyterHub from your browser. Its a bit laggy, but great if you need to download something from Canvas or use the terminal. This article has some additional resources and how-to’s for more in-depth usage if something you need is not covered here. Although using the JupyterHub is probably a bit less snappy than it would be in the physical ICS labs, it is really user-friendly especially for newer students who are still getting used to programming, so it’s a fair trade for all the functionality you get. Access OpenLab via SSHFor students in upper-division classes, or those conducting research in a lab on campus, connecting to OpenLab through the Terminal is preferred. Its much quicker, and by learning a few simple remote file copy commands, you can really save yourself a lot of time. For this method, I made a YouTube video that explains all these same steps, so if you’d prefer you can watch that. I just outlined the same steps below for convenience and for copy-pasting the commands easily.  Tutorial video for accessing OpenLab via SSH. Terminal commands listed below. Steps:  Open Cisco AnyConnect and connect to the VPN (documented in this guide, or watch video above for example).      Note: If you want to circumvent this step in the future, see this guide on how to add an SSH Key to OpenLab. However, if you’re trying to do this for on-campus research, this won’t work because you’ll still need to connect to the VPN (I tried 😭).     Go to the terminal, and type in ssh UCInetid@openlab. ics. uci. edu (replace UCInetid with your ID. )     Note: if you need to connect to a specific server, for instance, circinus-37, then use ssh UCInetid@circinus-37. ics. uci. edu    Type in your ICS password (might not necessarily be the same as your UCInetid password).  If you see a terminal window that looks like mine below, you successfully got in!  You should see a message similar to this if you successfully connected to OpenLab. The @circinus-37 seen in mine will vary based on which specific server you got connected to. Remotely transfer files: Now that you’re connected, you might want to remotely transfer files to and from your local machine to OpenLab. I give the full demo in the video so if you’d like to see that in-action check that out. Below are the commands:  Copy single file from local to remote (run in your local terminal):scp myfile. txt UCINetID@openlab. ics. uci. edu:/remote/folder/  Copy single file from remote to local (run in your local terminal):scp UCINetID@openlab. ics. uci. edu:/remote/folder/remote. txt local. txt  Copy multiple files from local to remote. scp myfile1. txt myfile2. txt UCINetID@openlab. ics. uci. edu:/remote/folder/  Copy all files from local to remote using scp. scp * UCINetID@openlab. ics. uci. edu:/remote/folder/  Copy all files and folders recursively from local to remote. scp -r * UCINetID@openlab. ics. uci. edu:/remote/folder/ For more information, search “scp command (linux)” on Google. ConclusionA Case for the Physical Computer Labs: Despite the length and depth of this tutorial, OpenLab is no replacement for going to the physical computer labs. Yes, I know, it’s smelly, it’s a long walk, it’s dark. The keyboard is sticky for god knows what reason. When I was an undergraduate and one of the few women in my introductory CS courses, it could feel like the whole lab was watching my every move just waiting for me to mess up or ask some dumb question. I viscerally remember some of my first experiences in the CS lab at UCSD, I felt so embarrassed to ask the most basic question of them all—“where is the Terminal?” It can feel easier to hide in your house and do your programming there instead, especially now in a post-Covid world where that has become a lot more socially acceptable. But, for all the flaws of the labs, it’s also where a lot of learning takes place. For one, its usually where office hours or tutoring hours take place. I know for me, I would not have passed Basic Data Structures without the patient assistance of the tutors in the labs. In your earlier classes, you really need to make an effort (as hard as it is) to get up and go to the labs so you can get that precious help. As you get more experienced in your upper-division courses, you can gradually wean yourself off the labs and work more frequently from the comfort of your home (and you’ll have really earned it!) The labs also don’t have to be as scary as it was for me in my early years. Although it sucks feeling like one of the only women/person-who-looks-like-you in your Computer Science class, it can become in and of itself a way to bond with the few other marginalized people in the class because there is that instant sense of understanding and camaraderie from that experience. Find some other folks, form a study group and some sense of community. Form a ritual—maybe you all grab lunch together after class and then afterwards lug your way over to the labs together to grind out your programming assignments. You’ll form some study buddies, and you’ll be much more likely to stay on top of your assignments and get the help you need early and often. In a few years you all will laugh about your times in the CS labs (I remember one time sneaking some beer in paper bags into the labs to attempt to make our homework more fun, don’t try this, kids). Some of the buddies I made in my intro CS classes ended up being my friends throughout my whole college experience. So, hopefully this tutorial will help you with connecting to OpenLab for times that you’ve got the sniffles or want to work on your assignment outside of lab hours, but still plan on draggin’ your butt over to the labs for most of the quarter! Please let me know if this tutorial helped, or if there is any additional content you’d like to see covered next time! 👋 "
    }, {
    "id": 15,
    "url": "http://localhost:4000/blog/connect-zotero-overleaf.html",
    "title": "Two Ways to Sync Your Zotero Group Library in Overleaf",
    "body": "2022/06/08 - Here’s a very quick guide on how to connect your Zotero Group Library to Overleaf. I have used the alternative solution detailed below in the past when the default “From Zotero” integration in Overleaf did not work for me. I wanted to post this as a reference—for myself, because I always forget how to do this when I’m about to write a paper—and also in case it happens to be useful for anyone else. Import from ZoteroFirst, you should try to import your Zotero group bibliography the “usual” way. To do this, click on “New File”, and select “From Zotero. ” You’ll select the appropriate Library from here.   To get to this dialog, select  New File  from your project page. After, your bibliography should appear in your project page:  Most of the time this should work, but I have had an issue on at least one project where this method didn’t work. The second methodology listed below will help in those cases. Import from External URLIf the first method didn’t work, here’s an easy workaround. Navigate to the same “New File” dialog, but then click “From External URL. ” Using the url https://api. zotero. org/groups/INSERT_GROUPID/items/top?format=bibtex&amp;style=numeric&amp;limit=100, and replacing the INSERT_GROUPID with the appropriate ID from the desired Zotero group, you should be able to import your bibliography this way.   Use the url https://api. zotero. org/groups/INSERT_GROUPID/items/top?format=bibtex&amp;style=numeric&amp;limit=100 in the From External URL option. Example: To get the group ID from your Zotero group, you’ll need to go onto your Zotero account on the web interface. For instance, my group library AI Education Project is hosted at https://www. zotero. org/groups/4669023/ai_education_project. The Group ID in this case is the number hash after the groups/, part of the URL, in this case, 4669023. In order for this workaround to be successful, the Zotero group settings need to be set to “Public. ” In the example I’m using, my group type was Public and the Membership was closed.   Group settings on Zotero must be Public. Now, go back into Overleaf and paste in this URL with the group ID you just retrieved into the “From External URL” form.   Pasting the group ID, 4669023, into the URL: https://api. zotero. org/groups/4669023/items/top?format=bibtex&amp;style=numeric&amp;limit=100Then, navigating to our Overleaf project, we see that the file ai_education_project. bib was successfully imported!  ConclusionHopefully this guide helped anyone who is on a paper deadline and wants to import their group bibliography in an efficient workflow. Happy paper writing! ReferencesI initially saw this tip written about buried at the bottom of blog post written by Yuxuan Mei [1]. Since I believe this tip is so useful, and in that post its just included at the very bottom, I wanted to make a post dedicated to it entirely for clarity. [1] Yuxuan Mei, “Using Overleaf with Zotero. ” "
    }, {
    "id": 16,
    "url": "http://localhost:4000/blog/how-to-find-easy-first-issue-github.html",
    "title": "How to Find an Easy First Issue in an Open-Source GitHub Project",
    "body": "2022/05/21 - As a new developer to an open-source project, it can be quite intimidating to figure out where to begin, particularly for your first contribution to the project. And the first step before even doing your first contribution is actually finding that first issue to work on—a task which I’ve found is often underestimated for its difficulty. While there do exist many blog posts on this issue, they tend to begin and end at filtering the list of issues by using the “good first issue” label on GitHub. However, as we will see in the example we’ll walk through, that doesn’t universally work for all open-source projects—and beginners are then left in the dust about what to do next. My goal with this post is to demonstrate for you how you’re going to be walking through a given open source project to identify a couple of “easy” first issues to work on. I will also walk you through some of the nuances of issues, and importantly the social context of the project, and how these play an important role in navigating your open-source project of choice. Context and PrerequisitesI wrote this blog post as an adaptation of a lecture segment I’ve given in the Reverse Engineering and Modeling course at UC Irvine with Professor André van der Hoek, where we teach students how to become productive contributors to large, unfamiliar software systems. In fact, we even wrote a paper on that course that appeared in the International Conference on Software Engineering! So, I’m writing this post for an audience similar to our student cohort in the Professional Master of Software Engineering program—folks who already know how to program, but maybe have not had much experience working in an open-source system. I’m also assuming that you already have an open-source project of choice to work on. (If this is something you need help with, let me know in the comments and I can work on this as a related post!) The open source project I’m going to be using to walk through is Tensorflow. It’s an open-source library for Deep Learning that is written mostly in C++ and is maintained by Google. If that sounds super scary, don’t worry—I’m using this as the example project mainly because of the abundance of activity the project has so that I can demonstrate various scenarios you may encounter. Even if you are working in a project that is a completely different language or product, all these same tips should still apply.   TensorFlow GitHub project page. Why do we need to vet the issues at all?It seems like so much extra effect to go through all this detective work to find “the best” issue to work on, so why should we even bother? Why not just start coding on the first issue we see? The biggest reason why we need to actually vet the issues before we begin working on one is because anyone can post an issue to GitHub. It can be hard, I think, for newcomers to open-source to understand this. When you’re a beginner, its easy to assume that everyone active on the GitHub page is official and knows what they’re doing. Let’s look at a few real, mostly silly examples from the Tensorflow page itself to show you this is definitely not the case: Bad Issue Example 1: An Angry Customer:   TensorFlow issue #77. Here’s an example that made me chuckle. This individual is demanding “1st class windows support” be provided. This is obviously an example of something that cannot actually be coded up in the scope of a simple GitHub issue, but perhaps was an individual airing their greivances in a misguided forum. Maybe instead of GitHub, they could have found the TensorFlow customer support page? Bad Issue Example 2: Red Alert! Unrelated webpage down!:   TensorFlow issue #209. Red alert! The personal webpage for a professor is DOWN! Now, I’m not being entirely fair putting this in the bad issue section. For context, Yann LeCun is a professor of Computer Science, and he is one of the original creators of the MNIST database, which is an important database that TensorFlow gives their users access to. At least as of this writing, there are examples of TensorFlow’s official documentation pointing to the dataset hosted on Professor LeCun’s webpage. Despite the relevancy of this webpage to the TensorFlow project, this is not a good example of an issue you’d want to find yourself blindly working on as a newcomer to the project. How would someone even go about fixing this without access to LeCun’s webpage code? This is a good one to leave to the project maintainers to figure out. Bad Issue Example 3: “Good First Issue” that is actually mega-hard:   TensorFlow issue #22926. At first glance, its not really obvious why this issue belongs in this list. This issue (as of the time of writing), is actually still open on the TensorFlow page. For an issue to be open for nearly 4 years is almost unheard of in the fast-paced world of open-source. As we scroll down perusing the comments on this issue, we find lots of moderators and experienced developers trying to gain more context about how to both understand and reproduce the issue—a critical component we’ll discuss more later on. About halfway through the comments, we stumble upon a comment from a user @Gomesz785 that aptly summarizes the theme here:   TLDR: A good Samaritan user warns newcomers that despite the  good first issue  label, this is actually a  god-level  issue. Silly examples aside, this is actually my favorite example because it really drives home the true reason why we can’t always just “trust” minimal filtering of issues. Summary: So, to summarize, here’s why it’s not the best idea to grab the first issue we see on the project:  Anyone can post an issue Big projects like on Tensorflow have dozens of posts weekly Could be that someone just didn’t understand how the feature worked Could be some setting on their local machine Maybe they didn’t have the proper dependencies installed Could be a bug Could be a feature request! (generally more work)A Heuristic to Search for Easy IssuesOk so there’s a lot of potential reasons why we shouldn’t just grab the first random issue we see. But what is it we do want to be looking for instead? And how can we go about this process? In this section, I’m going to outline a heuristic for finding a suitable easy issue. I’ll walk through each step/theme in detail with TensorFlow, and then at the bottom I’ll leave an action-item you can try in your chosen open-source system alongside. Documentation: The Information Kiosk of Open-Source: I like to think of documentation in an open-source GitHub project as the equivalent of the Information Kiosk or Customer Service desk. This is the easy, no-brainer, first-stop we’re going to to get some information. Are they always 100% right? Certainly not, we can all think of examples where Customer Service has given us well-intended, but ultimately misguided information about how to proceed on a given topic. But usually, the Information Kiosk is going to have the latest company-provided information on a variety of different topics.   Information kiosk from one of my favorite childhood games, Rollercoaster Tycoon. From RedditIn TensorFlow, and in many open-source projects, the guidelines for contributors can be found in a doc titled CONTRIBUTING. md. Taking a look at the below snippet from the Contribution doc for TensorFlow:  If you want to contribute, start working through the TensorFlow codebase, navigate to the Github “issues” tab …start by trying one of the smaller/easier issues here i. e.  issues with the “good first issue” label and then take a look at the issues with the “contributions welcome” label. But, when we go to the Issues tab for TensorFlow and search by “Good First Issue”, we are actually only left with one issue, and this is the same god-level issue we talked about above. So, unfortunately, for TensorFlow, our Information Kiosk was not particularly trustworthy on this topic. One of the most important things you’ll learn from working on enough open-source, is that the code is always the ultimate source of truth. Documentation is nice, sometimes it looks really official, but it is subject to being out-of-date or flat-out wrong. Action Item: For your open source project: go ahead and try this first step. Look at the documentation for Contributors, and if they suggest that you look at issues by the common “Good First Issue” or “Contributions Welcome” labels, go ahead and actually try filtering the Issues with these. Take note of what is left, and whether its several issues, or not many at all like in this case.  When applying this in Tensorflow, here’s what we have. Easy Bugs: The White Whale of Open-Source Issues: Fundamentally, what we’re really looking for when we’re trying to find an “easy first issue”, is actually an “easy bug. ” Why not Features?: Generally, we’re going to want to avoid features at this point. If its your very first contribution, bug fixes are generally going to be less involved, require less familiarity with the code base, and will be a great way to gain expertise in your given project. Working on a Feature would be a great goal for a second or third contribution later on! What makes a bug an “easy bug”?: An “easy bug” does not necessarily mean that its easy to fix. In this context, an easy bug means the following:  Specific: Usually in the official formatting as outlined in the documentation of the repository Reproducible: We have precise steps outlined for us to reproduce this issue on our own machine Some official recognition from admins: Shows that someone else has been able to reproduce the issue on their own machineSo, an “easy bug” in this case is really easy to spot and reproduce on your machine. How difficult it actually is to fix is something that you can try to decipher by reading the comments for the given issue, and mostly by working on the bug itself.   Mr. Krabs and his elusive white whale, Moby Dollar. From YouTubeA quick and dirty way to find bugs is to simply use the label for it in GitHub. Usually, its titled “Bugs”, or something like that. In TensorFlow, its actually titled “type:bug. ” Now, this will filter all the issues just categorized as “bug”, but they are certainly not automatically meeting the criteria of an “easy bug” listed above. I call it a “white whale” because for it to meet all of these criteria can actually be quite rare. In TensorFlow, here’s an example of a bug that meets our criteria of an “easy bug. ” Action Item: In your open-source project issues list from above, now apply the label “Bugs” (or whatever the equivalent is in your project) and see what you’re left with. We’re going to narrow down this list further in the next step, so don’t filter out by “easy bugs” just quite yet.  Since the list from TensorFlow above only contained 1 issue, I’m going to clear those two “good first issue” and “contributions welcome” labels. Applying only the “type:bugs” label, here’s what we have for TensorFlow at this point. Releases: The Fountain of Youth: There’s a lot of issues in a given GitHub project that are inevitably out-of-date. Even if we do find our precious Easy Bug, how can we tell which ones are still relevant to us? The quick and easy answer to this: look at the latest Release. For any open-source project, the latest Release contains the most recent version of the project’s code. The reason that the latest release is going to be the best place to look for bugs essentially comes down to the Reproducible criteria we discussed above for Easy Bugs. Even if we have a perfectly outlined bug, if all the instructions for reproducing that bug are related to code from 7 releases ago, how can we be sure this is still relevant? By grabbing issues that are related to the latest release, we can be sure that the information provided with it and the code we write are going to be most relevant to the current state of the project. For TensorFlow, and many projects on GitHub, there should be a /releases pages we can look at. Here’s the Release page for TensorFlow, where we can find the number of the latest release. Usually its going to be some number with decimals. In TensorFlow, they actually have labels for each release. We find the label corresponding with the latest release number to further filter our issues. Action Item: Depending on the conventions of the project you’re working on, there could be several different ways we can filter issues by the release.  One way, if its like TensorFlow, is through labels. If this is the case, simply find the label corresponding to the most recent release, and filter your list even further.  If your project does not use Labels corresponding to the Release on the Issues page, try digging around your project documentation to see if there is a way that they separate out issues with the corresponding Release.  If all else fails, look at the date of the latest release on the Releases page of your project. Then, look at issues only posted after this date.  For TensorFlow, here’s our filtered list at this point. This is pretty incredible, we started with over 2,000 open issues, and we’ve filtered it down to just 6!And finally, the moment we’ve all been waiting for!: At this point in the process, if you’ve been following along with the Action Items at the end of each heuristic, you should have a much more manageable list of Issues left. At this point, with the issues you have left, you’re going to want to more carefully apply the criteria for an Easy Bug we discussed above. As long as you find something that mostly meets this criteria, you should be good to go ahead and start actually coding on this! Parting AdviceRemember, this is a heuristic—its not meant to be an exact replica of steps you can duplicate for any open-source system. There might be things you have to tweak—for instance, you might have to include the latest few releases, instead of just the latest one in order to find an appropriate open issue to work on. One of my favorite experts on the practice, Joshua Bloch, aptly puts it:  “Learning the art of programming, like most other disciplines, consists of first learning the rules and then learning when to break them. ” So, hopefully these general steps will give you a much more thorough and nuanced outline in order to start contributing to your open-source system, but they will not ever be a 100% exact process to duplicate. If you enjoyed this post, have ideas for a future topic, or want me to dive deeper on any of the topics discussed above, please comment below and let me know! "
    }, {
    "id": 17,
    "url": "http://localhost:4000/blog/bored-to-death",
    "title": "Bored to Death: Why Working as a Software Engineer Nearly Cost Me My Life",
    "body": "2021/11/01 - I’m often asked by my students how I wound up in grad school, and especially why I chose to come to academia when I had a prestigious, 6-figure job in tech right out of undergrad. Sometimes even my fellow grad students will ask me this question after I declare my goal is to one day become a professor—why not just go to industry where I’ll make way more money doing “the same” work? I was inspired to write about this because I’ve been listening to this wonderful podcast that I discovered recently (thanks in part to our reptilian algorithm overlords) called ADHD reWired with Eric Tivers. I’ve been listening to episode after episode of brilliant, successful, and happy individuals with ADHD talk about their life story and rise to success. Oftentimes though, these folks had to go through really dark periods in their lives before they were able to learn to “ride the dragon” of the brain that is ADHD. When I applied for the NSF GRFP earlier this month, the personal statement I wrote for it required me to do some deep thinking about why I really was in academia, and why I wanted to pursue a PhD. A big part of my essay that I worked on was talking about how I re-found myself and my true interests while I was backpacking along the John Muir Trail. This really did happen, and honestly it kind of sounds like a movie—I had recurring dreams nearly nightly on the month-long trip about my love of science and teaching, and that my years of self-doubt in the field of computer science had made me forget my true, original dream to one day become a college professor. (I plan on posting my essay later if I get the fellowship, fingers crossed!) What that story didn’t cover, however, is why was I really was even on the JMT in the first place? This isn’t something you do on a weekend, most people can’t walk the trail while having full-time employment—it takes about 20-30 days for a backpacker to walk the 211 miles it spans.     View this post on Instagram      A post shared by JMT Hiker (@hikers_of_the_jmt)  After listening to the stories in the podcast, I realized there was more to my own “origin story” than I’d initially talked about. A big goal in my writing is to broaden access in the field, and I’ve had some recent realizations about how many of my struggles to fit in in Computer Science were not solely due to the fact that I am a queer woman in a predominantly cis-masculine hetero-normal field, but also because of the unique set of challenges that having an ADHD brain presents itself with. There’s so much to this story that I want to cover, and I probably will be doing more posts about this. But for now, the story that answers: why did I leave industry and end up in academia? For prior context, in many software engineering teams, work is broken into what is called in Agile Methodology “sprints. ” Engineers are assigned a certain number of small tasks to be completed during this time period, typically about a two week long period. The amount of tasks they get assigned is dependent upon level of familiarity with the system and often seniority. So, a UC San Diego recent graduate, assigned a couple easy bugs a sprint and being paid an astronomically high salary at a resume-making tech company—its the dream gig, right? For many of my fellow 22-year old peers who’d also scored a highly competitive, six-figure job right out of college, this was an easy-coasting ride on the gravy train. I saw friends in my cohort who absolutely crushed this game, I saw several rise to the coveted rank of senior and even staff level positions in under a year. So what was Brooke doing during a typical sprint, 6 months into her tenure? Hunched over in my dimly-lit corner cubicle, this particular sprint, I was hard at work. Not exactly doing any bug fixes, or new feature requests, or really any programming at all. No, a task much more important than this had me absolutely fixated. I spent an entire two week period—the length of the an entire agile “sprint” in that company—researching the best dog food for my new puppy Henry. During our team’s morning stand-up meetings, I literally would report imaginary roadblocks I was facing to justify why this task that was supposed to take a few hours was now taking the entire sprint: “Yeah, we gotta loop Mark down in IT on this one, this issue is looking a lot more complicated than we initially budgeted for. ” But really, “Mark in IT” was the author of a book on naturopathic dog food. I would scuffle back to my desk, anytime a senior teammate walked by I’d pull up the code I hadn’t touched since it was assigned weeks ago, and go back to spending the entire day researching dog food. I did not get my work done at the end of the sprint, but I was now an expert on DIY raw natural dog food. Even after all this, Henry actually wouldn’t be any better off either, I realized because of the time involved in implementing all the “best” methodologies (I could barely clean up my own plates after dinner, how the hell was I going to do it for a dog), this would be totally unrealistic to even attempt. Henry just eats kibble now.   My dog Henry and me at the beach. This cycle went on for nearly a year. Nine and a half days of dicking around at my desk, then the last half-day of the sprint when the literal fear of god and unemployment were in my veins that I would be able to have the energy to start on my tasks. And unsurprisingly, as probably any working software engineer can attest, tasks are almost always significantly more complicated and time-consuming than originally estimated, and often I’d have nothing completed at the end of a sprint. “What the fuck was wrong with me?” I would think on a daily basis. I didn’t like lying to my team, hell I didn’t even really intend to in the first place. I knew this task wasn’t supposed to take me very long. But I just could not get myself interested in what I was “supposed” to be doing. Often unconsciously, I would get sucked in to whatever interested me at the time—one moment it might be dog food, other times it might be ultralight backpacking gear. It was really only when the assigned work intrinsically interested me where I was able to actually produce work that matched my capabilities. After a few good weeks, inevitably, I’d get sucked into something tangentially related to the project, such as researching the best possible testing framework, learning the theoretical underpinnings of cybersecurity despite being irrelevant for the looming project milestones. Those “little” but ever-important tasks would catch up to me, and I’d have nothing to show for it. I felt dishonest, like I was letting both my team and myself down. No matter how “simple” the task appeared, the endless monotony of working in a large, risk-averse, decades-old legacy system was slowly killing me. Despite the brilliant marketing that tech companies have produced to make software engineering seem fast-paced, cutting-edge, and dare I say sexy, I found myself slowly drowning in a sea of self-hate and office mundanity. Why the fuck is it so hard for me to just fix a simple bug? Why can’t I just be a functional adult? In my efforts to force myself to fit that neurotypical mold was when things became dangerous for me. As the months carried on, little was able to capture my attention at all. Towards the end of my brief tenure at this company, I had to have my mother drive me to work because I would sob the entire way there. At lunch, I’d go off-campus somewhere and sob. On the last day in the office, I felt like I had physically left my body. An empty shell of my former joyful, jovial self, the only thing that could capture my focus now were things that could harm myself, like maybe that plastic butter knife in the break room… Hearing myself think that, I left work, and went immediately to my doctor, who put me on a medical leave of absence. It took me about a year and a half after that dark, gray day to find what I really wanted to do. For a while, that was nothing. I was able to use the funds I’d saved from working that job to take time off to backpack. While I had no trouble with my resume getting interviews from more tech companies (I got flown up to Google during this period), the only job that remotely interested me was a summer teaching job I eventually I stumbled upon for Girls Who Code. By that time I’d committed to doing the John Muir Trail that August, so a seasonal gig was an accidental perfect fit for me at the time.   Group picture from Summer 2019 with Girls Who Code. In my dreams at night along the trail, I recalled the joy I experienced teaching those girls that summer. I didn’t have very good grades in college because of my undiagnosed ADHD—the last-minute cramming that’d gotten me A’s in high school didn’t work for me anymore with college-level programming assignments. I usually got Bs and Cs in my Computer Science classes, and so I never thought I’d be “good enough” to get into graduate school, a pre-requisite for teaching at the college level. My dreams along the JMT transported me to a time long before I thought I was inadqeuate to become a professor. I’m still discovering so much more about my own mind. I’ve also learned that women are generally underdiagnosed with ADHD because we tend to internalize the symptoms rather than realize we may need to seek professional help (Jessica from How to ADHD has a great video on this phenomenon). Sometimes, its not just a toxic internal dialogue but also an external one, I believe that being a queer woman in Computer Science just exacterbated these feelings of isolation and unworthiness. I’ve discovered, as I did on the JMT, that being a teacher is an integral part of my identity. I love the joyful process of scientific investigation and I love learning. I love the creativity and freedom that crafting my own research agenda and experiments affords. I love the diversity of the kinds of work I get to do in a day—I can tutor students, craft engaging lectures, create my own software framework from the ground-up to conduct my neural networks experiments, travel the world to present my research findings, ponder whether silicon will ever acquire consciousness. It might not sound like a huge difference to my neurotypical peers, but even just having the ability to truly, deeply focus on work that interests me without having to be interrupted by the thousands of little-deaths of Slack notifications, emails, “brief” meetings that take twice as long to “unwind” from and regain focus again—this is also part of the freedom that I love being in academia. The only “boss” I have to explain my working habits to really is the scorecard at the end of the year, wherein my objective output in terms of research or teaching productivity is measured. Not whether I punched in on-time every morning at 9 and clocked out at 6. I don’t have to explain to anyone that talking a 2 hour walk in the park in the middle of the day to stare at the geese is a crucial part of my daily routine—of course depending on what day it is, and perhaps whether listening to a podcast about UFOs or reading a book about the mysterious origins of eels might be more fun that day. The greater salary potential in industry is nothing compared to this. As Eric Tivers of ADHD reWired put it so aptly, ADHDers don’t have the luxury of doing work that is uninteresting to them. We have to find work that interests us in order to survive. I’m still learning so much about myself and what “it” is that I truly want to do in this life. I’ve learned I cannot tame the dragon that is ADHD, but I’ve learned I can ride it to where my interests take me. Right now, they’ve taken me to academia, studying neural networks and pondering the nature of consciousness, while teaching Software Engineering courses at UCI through my TAship I’ve been so lucky to acquire. I’ve had a lifelong love of teaching, I know I will continue to be guided there. I love learning, science, discovery, and helping others along the way. From my point of view right now on my “dragon”, a professorship is faintly outlined on the horizon, before that, the mountains of a PhD. And the only compass that I have is not that where the money lies, or where the security is, or where the most prestige lies, but where the beautiful beast of my interests take me. It is the only compass I need, and it is why I was led to where I am in grad school right now. Further Resources: If you read this and related to many of the symptoms discussed, please connect with a psychiatrist and get the help you deserve. Many people with ADHD are not diagnosed until later in life, particularly for women. Receiving the proper treatment can be life-changing, and I highly reccommend you seek professional help if you believe you may also have this. If you are also a college student who already has an ADHD diagnosis, this podast from ADHD reWired has some really helpful resources about self-advocacy and accommodations. Finally, if you are experiencing similar symptoms of depression or suicidal thoughts like I discussed in this post, please seek help! Even if you do not have health insurance, many therapists offer sliding-scale for low-income patients. If you are a college or graduate student, many universities provide mental health services that are included in the cost of tuition. In last resort scenarios, the Suicide Prevention Hotline can help you if you are in an emergency situation, and can also be reached at 1-800-273-8255. Future Posts: If you enjoyed this post, please let me know in the forum or reactions below! In a future post, I’d love to talk more about my personal journey of accepting my ADHD, as well as strategies I’ve used to help me in school and get good grades. If these ideas interest you, or if you have another idea for a post topic, please comment and let me know! "
    }, {
    "id": 18,
    "url": "http://localhost:4000/blog/on-leveraging-source-code.html",
    "title": "Stealing Code—What All Software Engineering Students Should Know",
    "body": "2021/01/25 - One of the most important principles you’ll in Software Engineering, is to leverage existing source code. As a rule of thumb, if you’re writing something completely from scratch, you’re probably doing it wrong! It can feel like a strange transition to deliberately copy another’s code as an end to your own means, a vast departure from the world of academia and introductory Computer Science courses, where you are mandated to write low-level code and prohibited from copying others’ solutions. However for most programming you’ll be doing in the Software industry as a whole, you will not be re-inventing the wheel, and its oftentimes better to use these pre-written libraries. Why is this? Its not necessarily that Software shuns original, independent thinking. Take the JSON-Java project, for instance.  Looking at their GitHub project page, this is a library that is used by at least 137,000 projects–and has a team of 41 developers working on maintaining the code.    JSON Java GitHub project contributors and users. Software is constantly evolving, new bugs appear as quickly as their predecessors squashed, dependencies are continuously updated—a project as large as this one is verified not only by the team of developers that maintain it, but by the hundreds of thousands of other open-source projects currently leveraging it successfully.  There’s always a risk of bugs—but such a project will likely have infrastructure in place such as Issue Tracking to capture and assess defects.  If there’s a common operation to be performed in the scope of JSON parsing and processing, its likely been implemented, through several rounds of releases and testing, and is still being actively watched and maintained by this large development group.  Generally, this example demonstrates why we prefer to leverage existing libraries to writing our own completely from scratch. Nuances: There are always exceptions to the rule, as Joshua Bloch’s famously puts it in Effective Java: “Learning the art of programming, like most other disciplines, consists of first learning the rules and then learning when to break them. ” Generally, we want to keep dependencies on outside libraries to a minimum—particularly when considering a large project like the JSON project. Really what we’re trying to do is mitigate the risk that introducing new, minimally tested, potentially bug-prone code can do to the project. So while we don’t want to “reinvent the wheel”, we do want to ensure that we’re looking for that wheel with progressive effort—first check the garage for spares, then in the auto shop down the street, then if we have to, shipping it in from out of state.  Taking the Java-JSON project, here is a general heuristic I would follow:    First, check to see if what you’re looking to do exists already inside of the JSON project.  This is the most preferred as it will minimize the amount of new code introduced to the project.     For source code (i. e. code in the “src/main” directory):      Java built-in libraries, which are included java. util, java. lang, java. io, and the likes.    Utilize code from libraries that the project already depends on.  You can see these in the pom. xml file under the tag, or in the build. gradle file under dependencies{. . . }.    Well-established third-party java libraries that are not already included as a project dependency, such as Google Guava.  Code such as this is tested almost as much as the java libraries themselves, and hardly introduces a risk to the project.       For test code (“src/test” directory):      Look at existing tests and utility methods in those test classes.    For asserts() or things like that, look in the JUnit library or Mockito library, which is already included in the project as a dependency.       If you cannot find the operation you’re looking for after searching through these sources, then look for a reputable third-party library. *   *In the real world, you’d probably have to evaluate this as a tradeoff with your team.  Depending on the company and size of the project you’re working on, sometimes it might be less risky to just build something from scratch.     Build from scratch.  Additional Resources: I had Effective Java, Third Edition by Joshua Bloch reccommended to me early in my career, and I still reference it regularly.  The Third Edition was updated for the Java 8 release, and its still relevant for the newer Java versions.  It does an excellent job of delineating those principles of good design and effective programming that can appear amongst more experienced programmers to be a-priori. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>-->
<!--            </ul>-->
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        

<div class="container bg-lightpink">


	<div class="container-lg pt-4 pb-4">
		<div class="row justify-content-center">
			<div class="col-md-12 col-lg-10">

				<h1 class="display-4 mb-3 article-headline">Machine Visual Acuity</h1>
				<h4 class="display-6 article-subtitle mb-4 font-weight-normal"><b>Brooke K. Ryan</b>*, <a class="text-dark" href="mailto:andersnv@hs.uci.edu ">Anderson Vu, MD</a>*, and Andreana Chua</h4>
				<div class="pr-0 align-self-center">
					
					<img class="rounded border-brooke" src="/assets/images/machine_visual_acuity_thumb.png" alt="Machine Visual Acuity">
					
				</div>
				<div class="col-md-12 pl-0 pr-0 pr-md-4 pt-4 pb-4 align-self-center">

					<div class="d-flex align-items-center">
						<h6 class="text-muted space-mono">Advised by <a href="https://www.faculty.uci.edu/profile.cfm?faculty_id=6370">Professor Andrew Browne, MD</a> and <a href="https://www.igb.uci.edu/~pfbaldi/">Professor Pierre Baldi</a>; 2021</h6>
					</div>
					<div class=" mb-1 mt-2">
					
<span><a href="/research/projects/Machine_Visual_Acuity_Ryan_21.pdf" class="no-a-hover btn border-brooke btn-lg btn-faicon-brooke"><i class=" far fa-file-pdf "></i></a></span>



<span><a href="/research/presentations/Machine_Visual_Acuity_Ryan_21.pdf" class="no-a-hover btn border-brooke btn-lg btn-faicon-brooke"><i class=" far fa-file-powerpoint"></i></a></span>



<span><a href="https://github.com/brookekelseyryan/VisualAcuity" class="no-a-hover btn border-brooke btn-lg btn-faicon-brooke"><i class=" fab fa-github"></i></a></span>







<span><a href="https://wandb.ai/brookeryan/VisualAcuity/reports/Visual-Acuity-Progress-Report--Vmlldzo3NzkxMzk" class="no-a-hover btn border-brooke btn-lg btn-faicon-brooke"><i class="thin-icon fas fa-flask"></i></a></span>









					</div>

					<!-- Tags -->
					<div class>
				<span class="taglist">
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/research/tags.html#deep-learning">deep learning</a>
					
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/research/tags.html#computer-vision">computer vision</a>
					
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/research/tags.html#biomedical-imaging">biomedical imaging</a>
					
				</span>
					</div>

				</div>
				<div class="h-100">

				</div>
			</div>
		</div>
	</div>


	<!--<div class="container-lg pt-4 pb-4">-->

	<div class="row justify-content-center">

		<!-- Share -->
		<!--		<div class="col-lg-2 pr-4 mb-4 col-md-12">-->
		<!--			<div class="sticky-top sticky-top-offset text-center">-->
		<!--				<div class="text-muted">-->
		<!--					Share this-->
		<!--				</div>-->
		<!--				<div class="share d-inline-block">-->
		<!--					&lt;!&ndash; AddToAny BEGIN &ndash;&gt;-->
		<!--					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">-->
		<!--						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>-->
		<!--						<a class="a2a_button_facebook"></a>-->
		<!--						<a class="a2a_button_twitter"></a>-->
		<!--					</div>-->
		<!--					<script async src="https://static.addtoany.com/menu/page.js"></script>-->
		<!--					&lt;!&ndash; AddToAny END &ndash;&gt;-->
		<!--				</div>-->
		<!--			</div>-->
		<!--		</div>-->


		<div class="col-md-12 col-lg-10">

			<!-- Article -->
			<article class="article-post">
				<p>In this project, we aim to gain insights about human visual acuity by applying these tests to machines. The main goal is to first train a convolutional-based neural network to recognize optotypes with low amounts of distortions so that it can use its knowledge to classify an unseen optotype from a testing set with optotypes with medium to high amounts of distortions. We used transfer learning, with the use of a VGG network, to obtain a baseline model for the problem. Then, we experimented with mixing the testing set with the training set, to determine if that could help the network make better predictions.</p>

<h1 id="introduction">Introduction</h1>

<h2 id="motivation">Motivation</h2>

<p>In the field of Ophthalmology, the most commonly used methodology to test a patients’ visual sharpness, known here-throughout as “Visual Acuity”, is to perform a test such as depicted by the Snellen chart, shown in Figure 1. This method is so pervasive in the field that visual acuity tests are generally thought to be interchangeable with the testing of alphanumeric characters. However, it has been observed in the Browne Lab of Ophthalmology here at UC Irvine that the results of Visual Acuity tests do not yield consistent results for different character sets, known as “Optotypes.” Typically, these Optotypes do not even utilize the full range of characters in the alphabet, and instead are limited to about 5-10 different letters.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/VA.png" alt="" />
  <figcaption>Figure 1: Snellen Chart, commonly used by Ophthalmologists to measure a patient's visual acuity, or ability to discern character sharpness.</figcaption>
</figure>

<h2 id="related-work">Related Work</h2>
<p>Other Visual Acuity tests outside of the standard use of alphanumeric characters have been developed to address specific needs for a small subset of patients.  For instance, the Teller Acuity tests, depicted in Figure 2, use a series of parallel lines in varying width and gradients. This test was developed specifically for patients such as infants with low cognitive abilities who are not yet able to discern alphanumeric characters. However, as shown, even in images without any applied optical distortion, these are difficult to discern from each other. Patients who lack literacy, for example, do not necessarily possess low cognitive abilities, but would be at a disadvantage in participating in the Snellen-like acuity tests.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/TellerEx.jpg" alt="" />
  <figcaption>Figure 2: Teller acuity cards, used to measure visual acuity in patients with low cognitive abilities such as infants.</figcaption>
</figure>

<h2 id="project-overview-and-goals">Project Overview and Goals</h2>

<p>This project—conducted as a research project Brooke undertook in the Baldi Lab towards her Master’s thesis—is being jointly researched by the Browne Lab of Ophthalmology. Broadly, the goal is to explore what insights can be gained from applying a range of different acuity tests, as depicted in Figure 3, to both humans and machines. The machine tests are specifically conducted using Deep Neural Models, as this is the most accurate representation of human visual processing in the computer vision literature. For both the human and machine tests, it was necessary to digitally recreate the varying degrees of distortion that a patient would encounter in an Ophthalmologist’s office, which were graciously obtained from the Browne lab. The dataset is an aggregation of images of the optotypes with increasingly distorted images to increase the difficulty for the subject to discern the character.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/optotypes.png" alt="" />
  <figcaption>Figure 3: Column 1 depicts the list of various Acuity libraries tested, with the corresponding row delineating the optotype characters that belong to it.</figcaption>
</figure>

<h2 id="experimentation">Experimentation</h2>

<p>The primary Deep Learning technique that was applied to solve the problem, at least initially, was Transfer Learning. I primarily explored the use of VGG-16, while Andreana produced further successful results by using the deeper VGG-19 model and applying further fine-tuning of the model. This proved to be the most successful model that stays within the confines of the requirements of the original project. Andreana further explored if lifting some of those constraints resulted in higher accuracy, which ultimately it did.</p>

<p>Additionally, using techniques and concepts from this course, I applied data augmentation to the set of training images in order to bolster the range of both training and validation images available to the model. While this drastically improved validation accuracy of the low-distortion optotpyes, it resulted in lower accuracy for the high-distortion optotypes.</p>

<h2 id="evaluation-and-results">Evaluation and Results</h2>

<p>This quarter, as well as in the context of this course, the three primary goals achieved were:</p>
<ol>
  <li>Extensive pre-processing and categorization of the data for rapid experimentation to sufficiently explore the hypotheses presented.</li>
  <li>Exploration and build of a successful Transfer learning model for higher accuracy.</li>
  <li>Successful application of Image Understanding techniques and concepts for non-standard data augmentation of imbalanced optotypes.</li>
</ol>

<p>Though data prepossessing in and of itself does not produce results, extensive data cleanup and categorization had to be performed in order to allow for an additional partner to participate in the project, which had the benefit of obtaining the results that were produced in my partner’s subsequent report.</p>

<h1 id="dataset">Dataset</h1>

<p>Obtained from the Browne Lab of Ophthalmology here at UC Irvine, our dataset is a collection of images of optotypes. These images have gradually increasing levels of optical distortion applied. The given dataset is separated into two sets, one for training and one for testing.</p>

<p>In the training dataset, there are 1500 images total, each belonging to one of 64 different optotypes.  All of the images in the training set necessarily have a low amount of distortions, and are also all large images. This is with the intent to provide the model with a baseline of decent vision. The distortions applied can include rotations of the optotype, blurring, or shrinkage, which is shown in Figure 4. Figure 5 showcases some examples of optotypes in the training set.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/low_med_high.png" alt="" />
  <figcaption>
  Figure 4: Different levels of distortion. From left to right: large optotypes, medium optotypes, small optotypes. While the dimensions of the image remain the same regardless of size, optotype remains approximately centered and progressively shrinks.
  </figcaption>
</figure>

<p>In the training dataset, there are 1500 images total, each belonging to one of 64 different optotypes.  All of the images in the training set necessarily have a low amount of distortions, and are also all large images. This is with the intent to provide the model with a baseline of decent vision. The distortions applied can include rotations of the optotype, blurring, or shrinkage, which is shown in Figure 4. Figure 5 showcases some examples of optotypes in the training set.</p>

<p>In general, there are the same amount of images in each of the 64 classes of the training set, as seen in Figure 3. All images are size \(400 \times 400\) with 3 channels.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_5.png" alt="" />
  <figcaption>Figure 5: Sample images from training set (low distortion and large images only) and testing set (large images with high distortion, small and medium images).</figcaption>
</figure>

<p>As for the testing dataset, there are 3223 images total, each belonging to one of 64 different classes, all of which have either medium or high amounts of distortions (Figure 1). Figure 2 showcases some examples of optotypes in the testing set.</p>

<p>Like the training set, there are about the same amount of images in each of the 64 classes of the testing set, as seen in Figure 4. However, there are more of each optotype, compared to the training set. All images are size \(400 \times 400\) with 3 channels and were separated into training/testing batches of size 32.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/training_count.png" alt="" />
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/testing_count.png" alt="" />
  <figcaption>Figures 6/7: Count of number of training and testing images in each of the 64 classes.</figcaption>
</figure>

<h1 id="architecture-and-design">Architecture and Design</h1>
<h2 id="transfer-learning">Transfer Learning</h2>
<p>At its core, the project mainly uses transfer learning to solve the visual acuity testing problem. When a dataset is relatively small, such as in this project, it is not enough for a model to capture the patterns of each of the images. Thus, it is usually beneficial to use a pre-trained model as a starting point.</p>

<h2 id="vgg-16">VGG-16</h2>
<p>The main model that is used in this project is the VGG-16 network, which is a convolutional neural network that was trained on a dataset called ImageNet, which has over 14 million images belonging to 1000 classes [1]. To fit this model to a different set of images, the output layers of the transfer learning model are removed and replaced with layers that are compatible with the new dataset. Then, while training, a different number of layers can be frozen to retain the pre-trained weights that the model has. The rest of the weights in the model are trained to cater the new dataset. These ideas are utilized in the project, with the hopes that the pre-trained model is able to expedite the training process.</p>

<h2 id="vgg-19">VGG-19</h2>
<p>Another version of the VGG-16 network, called the VGG-19 network, is another network that we experimented with. The VGG-19 network has 3 more layers than the VGG-16 network, making it a deeper, and a potential competitor model to the VGG-16 network.</p>

<h1 id="methods">Methods</h1>
<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>After we built and performed hyperparameter tuning, we obtained two kinds of models that potentially works best with the given dataset. To reiterate, the baseline model consisted of a VGG-19 network, with a “GlobalAveragePooling2D” layer near the output layer. For the loss, we used “Sparse Categorical Crossentropy”, which is congruent with the goal of multi-class classification without the use of one-hot encoding. We also used Adam Optimizer for training, since it worked efficiently with the models. None of the VGG-19 layers were froze and 70 epochs were used to train.</p>

<p>For training, with the given data, the loss of both models were able to converge after a few epochs and were able to reach close to minimum values, as shown in Figure. Both losses were only graphed to 90 epochs for comparison purposes. Both used a different number of epochs, as mentioned earlier.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/machine_visual_acuity_thumb.png" alt="" />
  <figcaption>
Figure 8: Training loss for baseline model and mixed data model.
  </figcaption>
</figure>

<p>As for the model with the mixed data, it consisted of a VGG-19 network, with a “GlobalAveragePooling2D” layer, a Dropout of 0.2 probability layer, a Dense layer with 128 neurons, another Dropout of 0.5 probability layer, and a BatchNormalization layer. As with the basemodel, “Sparse Categorical Crossentropy” and the Adam Optimizer were used. About \(\frac{1}{4}\) of the VGG-19 layers were froze and 160 epochs were used to train.</p>

<p>With these models, we tested them with their respected testing sets. With a training accuracy of 70%, the baseline model was able to obtain an accuracy of 34%.  Figure 9 displays some of the optotypes that the baseline model was able to predict correctly (with predicted likelihood for that label higher than 80%) , as well as incorrectly. A majority of the images that the baseline model was able to predict correctly were the images that didn’t have too much distortion. It was rare to find very distorted images. There was a variety of distortions found in the images that the model predicted incorrectly.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_9.png" alt="" />
  <figcaption>
Figure 9: Optotypes that the baseline model predicted correctly (left) and incorrectly (right).
  </figcaption>
</figure>

<h2 id="mixed-data-model">Mixed Data Model</h2>

<p>For the mixed data model, it had a training accuracy of 65.26% and a testing accuracy of 62.88%. Figure 10 displays some of the optotypes the model predicted correctly (with predicted likelihood for that label higher than 80%) and incorrectly. From the images that the mixed model predicted correctly, there were more high-distorted images found than in the images that the baseline mode predicted correctly. There were still some medium-distorted images that the mixed model predicted incorrectly, but a majority of the images were distorted beyond recognition, as seen in the figure.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_10.png" alt="" />
  <figcaption>
Figure 10: Optotypes that the mixed data model predicted correctly (left) and incorrectly (right).
  </figcaption>
</figure>

<p>For some of the optotypes with high amounts of distortions, we tested to see how well the mixed model was able to predict the optotypes, compared to the baseline model. The figure below (Figure 11) displays a few examples in which the mixed model was able to predict the correct label, with both high and low likelihoods of that choice, along with the baseline model’s prediction.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_11.png" alt="" />
  <figcaption>
Figure 11: Optotype prediction comparison of the two models.  </figcaption>
</figure>

<p>There were some optotypes that both models predicted incorrectly, which are shown in Figure 11. Most of the images that both predicted incorrectly involved a variation of “x” or “+”. The main optotypes that the models get confused with are: [“+blank”, “+circle”, “+diamond”, “+square”] and [“x-blank”, “x-circle”, “x-diamond”, “x-square”]. With high distortions, the shape in the middle of the “x” or “+” become non-recognizable, causing the model to be confused. It predicts the correct shape, but not the details of that shape. Other points of confusion included high distortions of the rotated “C” variations and variations of “O”.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_12.png" alt="" />
  <figcaption>
Figure 12: Optotypes both model predicted incorrectly.  </figcaption>
</figure>

<h2 id="data-augmentation">Data Augmentation</h2>
<p>Data Augmentation was additionally leveraged as is a standard technique in the literature [2]. Figure 13 shows the original discrepancy between the training and testing dataset, which was extremely skewed toward the testing side.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/Visual%20Acuity%20Dataset.png" alt="" />
  <figcaption>
Figure 13: Training/Testing data split before and after applying data augmentation. Training dataset gained an additional 9,000+ images.
</figcaption>
</figure>

<p>The specific technique chosen for augmentation must be carefully selected, as traditional data augmentation techniques such as rotating, cropping, etc. would alter the integrity of the data and skew the original distortions provided, rendering the provided labels now inaccurate. The techniques chosen to bolster the training data were increasing the brightness and contrast 20%, decreasing the brightness and contrast by 20%, and then converting the image to grayscale. An array of images that were augmented compared to images that were not augmented can be seen in Figure 14.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_15.png" alt="" />
  <figcaption>
Figure 14: Array of training items bolstered by data augmentation, and classified by multiple labels. Techniques applied include brightness adjustment, contrast adjustment, and converting to grayscale.
</figcaption>
</figure>

<h2 id="data-wrangling">Data Wrangling</h2>
<p>Real-world data, such as the dataset obtained for this project, needed to undergo intensive preprocessing to obtain several levels of labels that may be later used for analysis. For each image, the labels extracted included Acuity, Character Type (such as wingding, alpha, numeric, or symbol), the Optotype, Angle of Rotation, Level of Distortion, Image Size, and Augmentation.</p>

<p>While we did not have time to explore the correlations between the results and all of these labels, we did explore the relationship between the optotypes and angles.</p>

<p>The original dataset contained optotypes C-0, E-0, C-45, E-45, etc. One question we sought to explore was if we separated the angles from these optotypes would this increase the efficacy of the system?  This decreased the optotypes from 64 to 59. We plan on creating a multi-output system where it must additionally classify the angle of rotation. The goal with this is to create a system that parallels human categorization.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/figure_14.png" alt="" />
  <figcaption>
Figure 15: Validation loss, comparing variations of VGG16, as well as variations in optotype classification, with and without additional data wrangling applied.
</figcaption>
</figure>

<h1 id="evaluation-of-results">Evaluation of Results</h1>
<p>As expected, the augmented training dataset increased the validation and training accuracy significantly. As shown in Table 1, accuracy for the network to classify low distortion, large images went up significantly—from the 60s to the low 90s. This is about as high as we could expect it to go without risking overfitting. Interestingly, though, the data augmentation/wrangling seemed to reduce the efficacy in classification of the test data. This could be for several reasons. Without the included angle in the optotypes, it is possible the network may have confused a backwards E for instance with a 3. This kind of example is demonstrated in figure 12.</p>

<figure>
  <img class="rounded border-brooke" src="/assets/images/visual_acuity_report/table_1.png" alt="" />
  <figcaption>
Table 1: Comparison of results amongst data augmentation and wrangling techniques applied.</figcaption>
</figure>

<h1 id="conclusion">Conclusion</h1>

<p>While this overall research project is still a work-in-progress, much has been gained this quarter through the opportunity to utilize in-class time, and particularly, to be able to apply concepts from this course.</p>

<h2 id="comparison-to-human-results">Comparison to Human Results</h2>

<p>While it was touted that one of the primary goals of the project is to compare the machine results to those of humans, those comparisons are notably missing from the report. This is because the human trials are being carried out by the Browne lab, and the app being used to conduct those trials was just released last week. To prepare for the comparison of these results, we still have some work to do, but we eagerly await the opportunity to compare the results.</p>

<h2 id="neural-networks-as-a-model-of-human-vision">Neural Networks as a Model of Human Vision</h2>
<p>For my personal research agenda, one of the areas I’m interested in exploring are the parallels between our own human intelligence and biology and that of machines–what can we learn from each other? What gaps in machine learning are present that we can leverage biology and cognitive science to inspire our algorithms? While we have not yet had the opportunity to compare between the human results, I am further convinced from this project that neural models are an effective model of the human nervous system, and they particularly excel in problems related to computer vision.</p>

<h2 id="future-directions">Future Directions</h2>
<p>One area I would like to explore in the future, which will probably be over the summer, in this project is how might an individual’s level of literacy affect the results of the acuity test? Would a literate individual, who has effectively been “trained” on significantly more alphanumeric data than an illiterate person, perform better on alphanumeric optotypes even if those two people had equally poor eyesight? I focused much of my efforts this quarter, which would have surprised me in retrospect, on data wrangling, and provided a set of custom labels to images that are alphanumeric characters that can later be used for analysis.</p>

<h1 id="references">References</h1>
<p>[1] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015.</p>

<p>[2] Sebastien C. Wong, Adam Gatt, Victor Stamatescu, and Mark D. McDonnell. Understanding data augmentation for classification: when to warp?, 2016.13</p>

<h1 id="appendix">Appendix</h1>
<h2 id="code">Code</h2>
<p>The code can be viewed on GitHub here: <a href="https://github.com/brookekelseyryan/VisualAcuity">GitHub Visual Acuity</a>.</p>

<p>I wrote quite a bit of code for this project, so I’ll summarize here the module structure that delineates what to find in the .zip file. All of the code included was written by me, and I did of course leverage Deep Learning libraries like tensorflow, keras, etc.</p>
<ul>
  <li><strong>config</strong>: contains the YAML files with the training parameters</li>
  <li><strong>data</strong>: contains a helper class Dataset that I wrote which loads in the pickled processed image data</li>
  <li><strong>model</strong>: contains code I wrote for the VGG16 transfer model</li>
  <li><strong>preprocessing</strong>: this is all of the code used to preprocess and categorize all of the image data from the original dataset</li>
  <li><strong>Teller</strong>: sub-module of preprocessing, special processing that was applied to Teller acuities.</li>
  <li><strong>util</strong>: contains a utility method to connect my runs to Weights and Biases to produce visualizations.</li>
  <li><strong>main.py</strong>: entry point to the program</li>
</ul>

<h2 id="data-visualization">Data Visualization</h2>

<p>Weights and Biases was used to produce some of the graphs and store data from the runs. The project can be viewed here: <a href="https://wandb.ai/brookeryan/Visual_Acuity">WandB Visual Acuity</a>.</p>

<h2 id="link-to-dataset">Link to Dataset</h2>

<p>The original dataset, aggregated by the Browne Lab, can be viewed and downloaded here: <a href="https://urldefense.com/v3/__https://www.dropbox.com/s/di5fkadcv1esp1a/VisAcuity*20-*20For*20Baldi*20group.zip?dl=0__;JSUlJQ!!OLgoXmg!HLI3nw7BFw55NZsuv2vKK_bKd9LXT-3I6oh4HSCPLH1SWR8AZcB1xax257UTZxJ43A\$">DropBox</a>.</p>

			</article>




			<!-- Mailchimp Subscribe Form -->
			
			<!--  Don't edit anything here. Set your disqus id in _config.yml -->

<div id="comments" class="mt-5">
    <div id="giscus_thread">
    </div>
    <script src="https://giscus.app/client.js"
            data-repo="brookekelseyryan/brooke-blog"
            data-repo-id="R_kgDOGTYEWg"
            data-category="Show and tell"
            data-category-id="DIC_kwDOGTYEWs4CPZcI"
            data-mapping="title"
            data-reactions-enabled="1"
            data-emit-metadata="0"
            data-input-position="top"
            data-lang="en"
            data-theme="https://brookekryan.com/assets/css/giscus.css"
            crossorigin="anonymous"
            async>
    </script>
</div>
			

			<!--			Cite this -->

			<div class="row mt-3 mb-6" >
				<div class="col-12">

					
					<br>
					
					<h4>Cite this Article</h4>
					<div id="cite-this2">
						<div class="language-bibtex highlighter-rouge"> <div class="highlight border-brooke"> <pre class="highlight mb-0">@misc{ryan-2021-machine-visual-acuity,
  author = { Ryan, Brooke and Vu, Anderson and Chua, Andreana and Browne, Andrew and Baldi, Pierre },
  title = { Machine Visual Acuity },
  url = {https://brookekryan.com/research/machine-visual-acuity.html},
  year = { 2021 },
}</pre></div></div>
					</div>

				</div>
			</div>
<!--			-->

			<!-- Comments -->



		</div>


	</div>
</div>



    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    <script id="dsq-count-scr" src="//brookekryan.disqus.com/count.js" async></script>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


    <!-- Footer -->
    <footer class="bg-white p-3  small " >
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><a href="/index.html">Brooke K. Ryan</a></span>
                <span style="font-family: 'Space Mono', monospace"><a href="https://github.com/brookekelseyryan/brooke-blog">Copyright © <script>document.write(new Date().getFullYear())</script></a>.</span>

                <!--  Github Repo Star Btn-->
<!--                <a class="text-dark ml-1" target="_blank" href="https://github.com/brookekelseyryan/brooke-blog"><i class="fab fa-github"></i> View on Github</a>-->

            </div>
            <div>
<!--                <a href="https://github.com/brookekelseyryan" class="btn btn-faicon"><i class="fab fa-github "></i></a>-->
<!--                <a href="https://linkedin.com/in/bkryan" class="btn btn-faicon"><i class="fab fa-linkedin-in "></i></a>-->
<!--&lt;!&ndash;                <a href="https://brookekryan.com/Brooke_Ryan_CV.pdf" class="btn btn-faicon"><i class="ai ai-cv ai-1x"></i></a>&ndash;&gt;-->
<!--                <a href="mailto:brooke.ryan@uci.edu" class="btn btn-faicon"><i class="far fa-envelope "></i></a>-->
                <a href="https://github.com/brookekelseyryan" class=" btn btn-faicon  fun-icon"><i class="fab fa-github "></i></a>
                <a href="https://scholar.google.com/citations?&user=E5NlNS0AAAAJ" class=" btn btn-faicon "><i class="ai ai-google-scholar "></i></a>
                <a href="https://www.linkedin.com/in/brookekryan" class=" btn btn-faicon "><i class="fab fa-linkedin-in "></i></a>
                <a href="/assets/docs/Brooke_Ryan_CV.pdf" class=" btn btn-faicon "><i class="ai ai-cv-square "></i></a>
                <a href="mailto:brooke.ryan@uci.edu" class=" btn btn-faicon "><i class="fa-solid fa-envelope "></i></a>
                <a href="https://medium.com/@brookekryan" class=" btn btn-faicon "><i class="fa-brands fa-medium "></i></a>
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
