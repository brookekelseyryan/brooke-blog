
@online{21stCenturyArtEducationRiseGaskins2021,
  title = {21st {{Century Art Education}}: {{The Rise}} of {{AI}} and {{Machine Learning}}},
  shorttitle = {21st {{Century Art Education}}},
  author = {Gaskins, Nettrice},
  date = {2021-01-11T02:02:23},
  url = {https://nettricegaskins.medium.com/21st-century-art-education-the-rise-of-ai-and-machine-learning-1a1e2ca47107},
  urldate = {2022-08-29},
  abstract = {According to Autodesk, artificial intelligence or AI refers to a broad field of science encompassing a range of subjects from computer…},
  langid = {english},
  organization = {{Medium}},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/GUGW8YLV/21st-century-art-education-the-rise-of-ai-and-machine-learning-1a1e2ca47107.html}
}

@article{AccelerationInformationProcessingRoutePerceptualHochmann2022,
  title = {Acceleration of Information Processing En Route to Perceptual Awareness in Infancy},
  author = {Hochmann, Jean-Rémy and Kouider, Sid},
  date = {2022-03-14},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {32},
  number = {5},
  pages = {1206-1210.e3},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2022.01.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982222000409},
  urldate = {2022-05-18},
  abstract = {Electrophysiological studies1, 2, 3, 4, 5, 6 have suggested an acceleration in information processing in the first years of life, probably largely caused by the progressive myelination of the cortex.7,8 Here, we ask whether and how this acceleration affects information processes that contribute to perceptual awareness. We addressed this issue leveraging on the attentional blink phenomenon9,10 in infants,11 children, and adult participants. When two visual targets (T1 and T2) are to be detected, the observer often misses T2, if it appears shortly after T1, as if the observer’s attention blinked. This phenomenon is explained by the two-stage model of perception, where an early unconscious sensory stage is followed by a late and central stage that relies on limited attentional resources.9, 10, 11, 12, 13, 14 Although both T1 and T2 are processed in the earlier sensory stage, the capacity limits of the second stage are such that T2 cannot be processed as long as attention is occupied by T1.9, 10, 11, 12, 13 The duration of the attentional blink, thus, indexes the speed of the late processing stage of visual stimuli, which is associated with perceptual awareness.12, 13, 14 Indeed, in adults, the blink only occurs if T1 is consciously perceived but not when it is missed or processed subliminally.15 Accordingly, neuroimaging studies16, 17, 18 have shown that late processes blocked by T1 involve frontoparietal areas, thought to be responsible for global cognitive availability, conscious access, and reportability.19 Here, we show that the attentional blink is present in young infants, suggesting that the two-stage organization of perception is in place at 5 and 8~months of age. In addition, we show that the duration of the attentional blink shrinks with development, suggesting that a fundamental aspect of cognitive development is the fast acceleration of the late processing stage of perception.},
  langid = {english},
  keywords = {attentional blink,COGS 269,consciousness,infancy,notion,perceptual awareness},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/XB3DLSQX/S0960982222000409.html}
}

@article{AcrosstrialDynamicsHumanEEGRevealCollins2018,
  title = {Within- and across-Trial Dynamics of Human {{EEG}} Reveal Cooperative Interplay between Reinforcement Learning and Working Memory},
  author = {Collins, Anne G. E. and Frank, Michael J.},
  date = {2018-03-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {10},
  pages = {2502--2507},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1720963115},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1720963115},
  urldate = {2022-08-13},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/QNDZELPG/Collins and Frank - 2018 - Within- and across-trial dynamics of human EEG rev.pdf}
}

@inproceedings{ActiveLearningSoftwareEngineeringCambronero2019,
  title = {Active Learning for Software Engineering},
  booktitle = {Proceedings of the 2019 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Cambronero, José P. and Dang, Thurston H. Y. and Vasilakis, Nikos and Shen, Jiasi and Wu, Jerry and Rinard, Martin C.},
  date = {2019-10-23},
  series = {Onward! 2019},
  pages = {62--78},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3359591.3359732},
  url = {https://doi.org/10.1145/3359591.3359732},
  urldate = {2021-10-21},
  abstract = {Software applications have grown increasingly complex to deliver the features desired by users. Software modularity has been used as a way to mitigate the costs of developing such complex software. Active learning-based program inference provides an elegant framework that exploits this modularity to tackle development correctness, performance and cost in large applications. Inferred programs can be used for many purposes, including generation of secure code, code re-use through automatic encapsulation, adaptation to new platforms or languages, and optimization. We show through detailed examples how our approach can infer three modules in a representative application. Finally, we outline the broader paradigm and open research questions.},
  isbn = {978-1-4503-6995-4},
  keywords = {active learning,program inference,program modeling},
  annotation = {3 citations (Crossref) [2022-06-09]}
}

@inproceedings{AdoptingIndustryAgilePracticesLargescaleSchneider2020,
  title = {Adopting {{Industry Agile Practices}} in {{Large-scale Capstone Education}}},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  author = {Schneider, Jean-Guy and Eklund, Peter W. and Lee, Kevin and Chen, Feifei and Cain, Andrew and Abdelrazek, Mohamed},
  date = {2020-10},
  pages = {119--129},
  abstract = {This paper presents the practice and experience in adopting an agile organizational model for a final-year capstone program in Software Engineering. The model developed is motivated by having real (and developing) software artifacts with incrementally changing team members working on a product-line. This in turn results in more sophisticated capstone student-project outcomes. The model proposed supports student mentoring and promotes, through its internal organization, leadership and personal responsibility. The students are supported by professional software engineers, up-skilling workshops, and academic supervisors who act as a personalized reporting and grading point for the team. The academic supervisors are themselves supported by a tribe leader, a faculty member who assumes overall responsibility for a product-line, and who acts as a report to an external industry client/ sponsor. This paper describes the motivation for the capstone model, its adoption, and some preliminary observations.},
  eventtitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  keywords = {agile software development,capstone education,Education,Employment,Industries,Scalability,SEET 2020,Software,Software engineering,Software engineering education,Stakeholders}
}

@inproceedings{AdoptionOpenSourceProjectsEngineeringNascimento2018,
  title = {The {{Adoption}} of {{Open Source Projects}} in {{Engineering Education}}: {{A Real Software Development Experience}}},
  shorttitle = {The {{Adoption}} of {{Open Source Projects}} in {{Engineering Education}}},
  booktitle = {2018 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  author = {Nascimento, Debora M. C. and Chavez, Christina F. G. and Bittencourt, Roberto A.},
  date = {2018-10},
  pages = {1--9},
  issn = {2377-634X},
  doi = {10.1109/FIE.2018.8658908},
  abstract = {This research to practice full paper investigates software engineering students' perceptions of their contact with open source projects as a real-world experience. Working with open source projects (OSPs) has been shown as an interesting option in software engineering courses to bringing students closer to more realistic environments. However, when instructors use this approach, it is not clear whether students perceive the OSP as a real industrial software project, or whether they perceive the tasks they perform over OSPs as typical or close to industrial software project activities. The goal of this work was to identify students' perceptions of their interaction with an open source project as a real world experience. To do so, we performed three mixed-methods case studies with three different undergraduate classes. Each class had a different focus: i) software maintenance and evolution, ii) software testing, and iii) reverse engineering of software requirements. Results show that students perceived features that make OSPs close to industrial projects, realized that their OSP tasks are close to the ones in industrial projects, and also faced difficulties typical of working with real world software. Moreover, students forged a view of the skills needed for their future professional success. We conclude that students realized that performing tasks in OSPs was a real world experience they took part, contributing to their background both for the competencies they acquired and the difficulties they had to overcome.},
  eventtitle = {2018 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  keywords = {Computer aided software engineering,Industries,Interviews,Software,Software testing,Task analysis},
  annotation = {6 citations (Crossref) [2022-06-09]}
}

@article{AdversarialVulnerabilitiesHumanDecisionmakingDezfouli2020,
  title = {Adversarial Vulnerabilities of Human Decision-Making},
  author = {Dezfouli, Amir and Nock, Richard and Dayan, Peter},
  date = {2020-11-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {46},
  pages = {29221--29228},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2016921117},
  url = {https://www.pnas.org/doi/10.1073/pnas.2016921117},
  urldate = {2022-09-06},
  keywords = {hello,MattarLab RNN},
  file = {/Users/brookeryan/Zotero/storage/X72SFQAJ/Dezfouli et al. - 2020 - Adversarial vulnerabilities of human decision-maki.pdf}
}

@report{AestheticPreferenceArtEmergesWeightedIigaya2020,
  type = {preprint},
  title = {Aesthetic Preference for Art Emerges from a Weighted Integration over Hierarchically Structured Visual Features in the Brain},
  author = {Iigaya, Kiyohito and Yi, Sanghyun and Wahle, Iman A. and Tanwisuth, Koranis and O’Doherty, John P.},
  date = {2020-02-10},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.02.09.940353},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.02.09.940353},
  urldate = {2022-07-22},
  abstract = {It is an open question whether preferences for visual art can be lawfully predicted from the basic constituent elements of a visual image. Moreover, little is known about how such preferences are actually constructed in the brain. Here we developed and tested a computational framework to gain an understanding of how the human brain constructs aesthetic value. We show that it is possible to explain human preferences for a piece of art based on an analysis of features present in the image. This was achieved by analyzing the visual properties of drawings and photographs by multiple means, ranging from image statistics extracted by computer vision tools, subjective human ratings about attributes, to a deep convolutional neural network. Crucially, it is possible to predict subjective value ratings not only within but also across individuals, speaking to the possibility that much of the variance in human visual preference is shared across individuals. Neuroimaging data revealed that preference computations occur in the brain by means of a graded hierarchical representation of lower and higher level features in the visual system. These features are in turn integrated to compute an overall subjective preference in the parietal and prefrontal cortex. Our findings suggest that rather than being idiosyncratic, human preferences for art can be explained at least in part as a product of a systematic neural integration over underlying visual features of an image. This work not only advances our understanding of the brain-wide computations underlying value construction but also brings new mechanistic insights to the study of visual aesthetics and art appreciation.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/9ZUGJ38E/Iigaya et al. - 2020 - Aesthetic preference for art emerges from a weight.pdf}
}

@article{AgeGenderDependentDevelopmentTheoryCalero2013a,
  title = {Age and Gender Dependent Development of {{Theory}} of {{Mind}} in 6- to 8-Years Old Children},
  author = {Calero, Cecilia I. and Salles, Alejo and Semelman, Mariano and Sigman, Mariano},
  date = {2013-06-17},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front Hum Neurosci},
  volume = {7},
  eprint = {23785326},
  eprinttype = {pmid},
  pages = {281},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00281},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3683618/},
  urldate = {2022-04-20},
  abstract = {The ability to attribute different mental states to distinct individuals, or Theory of Mind (ToM), is widely believed to be developed mostly during preschool years. How different factors such as gender, number of siblings, or coarse personality traits affect this development is not entirely agreed upon. Here, we introduce a computerized version of the scaled ToM suite of tasks introduced by Wellman and Liu (), which allows us to meaningfully test ToM development on children 6 to 8-years old. We find that kids this age are still not entirely proficient in all ToM tasks, and continue to show a progression of performance with age. By testing this new age range, too, we are able to observe a significant advantage of girls over boys in ToM performance. Other factors such as number of siblings, birth order, and coarse personality traits show no significant relation with the ToM task results. Finally, we introduce a novel way to quantify the scaling property of the suite involving a sequence of set inclusions on one hand and a comparison between specially tailored sets of logistic models on the other. These measures confirm the validity of the scale in the 6- to 8-years old range.},
  pmcid = {PMC3683618},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/AgeGenderDependentDevelopmentTheoryCalero2013a.pdf}
}

@article{AIgeneratedArtIllustratesAnotherProblemNaughton2022,
  title = {{{AI-generated}} Art Illustrates Another Problem with Technology},
  author = {Naughton, John},
  date = {2022-08-20T15:00:42},
  journaltitle = {The Observer},
  issn = {0029-7712},
  url = {https://www.theguardian.com/commentisfree/2022/aug/20/ai-art-artificial-intelligence-midjourney-dall-e-replacing-artists},
  urldate = {2022-08-29},
  abstract = {Artificial intelligence is being used to design magazine covers and provide pictures for internet newsletters. What could possibly go wrong?},
  entrysubtype = {newspaper},
  journalsubtitle = {Opinion},
  langid = {british},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/A2V7V5C8/ai-art-artificial-intelligence-midjourney-dall-e-replacing-artists.html}
}

@inproceedings{AISocialGlueUncoveringRolesSuh2021,
  title = {{{AI}} as {{Social Glue}}: {{Uncovering}} the {{Roles}} of {{Deep Generative AI}} during {{Social Music Composition}}},
  shorttitle = {{{AI}} as {{Social Glue}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Suh, Minhyang (Mia) and Youngblom, Emily and Terry, Michael and Cai, Carrie J},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445219},
  url = {https://doi.org/10.1145/3411764.3445219},
  urldate = {2022-06-10},
  abstract = {Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with human beings in co-creating novel content (e.g. music, art). While substantial research focuses on (individual) human-AI collaborations, comparatively less research examines how AI can play a role in human-human collaborations during co-creation. In a qualitative lab study, we observed 30 participants (15 pairs) compose a musical phrase in pairs, both with and without AI. Our findings reveal that AI may play important roles in influencing human social dynamics during creativity, including: 1) implicitly seeding a common ground at the start of collaboration, 2) acting as a psychological safety net in creative risk-taking, 3) providing a force for group progress, 4) mitigating interpersonal stalling and friction, and 5) altering users’ collaborative and creative roles. This work contributes to the future of generative AI in social creativity by providing implications for how AI could enrich, impede, or alter creative social dynamics in the years to come.},
  isbn = {978-1-4503-8096-6},
  keywords = {human-AI co-creation,machine learning,music composition},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/AISocialGlueUncoveringRolesSuh2021.pdf}
}

@online{AlienDreamsEmergingArtScene,
  title = {Alien {{Dreams}}: {{An Emerging Art Scene}} - {{ML}}@{{B Blog}}},
  shorttitle = {Alien {{Dreams}}},
  url = {https://ml.berkeley.edu/blog/posts/clip-art/},
  urldate = {2022-05-31},
  abstract = {In January 2021, OpenAI released the weights and code for their CLIP model, and since then various hackers, artists, researchers, and deep learning enthusiasts have figured out novel methods for combining CLIP with various generative models to create beautiful visual art from just a text prompt. In this blog post I document the evolution of this new art scene and share a bunch of cool artwork along the way.},
  langid = {english},
  organization = {{Alien Dreams: An Emerging Art Scene - ML@B Blog}},
  keywords = {ObsCite},
  file = {/Users/brookeryan/Zotero/storage/6NLVUZC6/clip-art.html}
}

@online{AllUCANRBlogsResources,
  title = {All {{UC ANR}} Blogs},
  author = {Resources, University of California Agriculture {and} Natural},
  url = {https://ucanr.edu/sites/news/All_UC_ANR_blogs/index.cfm?blogtag=dragon%20fruit&blogasset=75643},
  urldate = {2022-07-24},
  abstract = {News \& Information Outreach - All UC ANR blogs},
  langid = {american},
  keywords = {Dragon Fruit,notion},
  file = {/Users/brookeryan/Zotero/storage/H4GKQ5LN/index.html}
}

@online{AnalogiesBiologyDeepLearningRough,
  title = {Analogies between {{Biology}} and {{Deep Learning}} [Rough Note]},
  url = {https://colah.github.io/notes/bio-analogies/},
  urldate = {2022-08-23},
  abstract = {A list of advantages that make understanding artificial nerural networks much easier than biological ones.},
  langid = {english},
  keywords = {Neural Networks Research Project,Neural Networks to Humans},
  file = {/Users/brookeryan/Zotero/storage/XEL84NNR/bio-analogies.html}
}

@inproceedings{AnalysisCodeReadingGainMoreBusjahn2011,
  title = {Analysis of Code Reading to Gain More Insight in Program Comprehension},
  booktitle = {Proceedings of the 11th {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Busjahn, Teresa and Schulte, Carsten and Busjahn, Andreas},
  date = {2011-11},
  series = {Koli {{Calling}} '11},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2094131.2094133},
  url = {https://doi.org/10.1145/2094131.2094133},
  urldate = {2021-10-22},
  abstract = {Code reading, although an integral part of program comprehension, is rarely reflected. In this paper, we want to argue for a research approach and direction exploiting the potential that lies in the analysis of reading processes. Based on the vast experience compiled in psychology and some studies in computing, eye tracking and think aloud were elaborated as a viable research instrument for code reading studies. We conducted a feasibility study, designed to examine the actual process of code reading as the sensory starting point of comprehension. Computational and statistical tools were developed to facilitate data capture and analysis for eye tracking experiments. Results do not just provide proof of concept but already emphasize differences between reading natural language text and source code, as well as a distinct attention allocation within different code elements like keywords and operators. In conclusion we suggest a combination of theory-driven selected stimuli material, a carefully designed procedure of eye tracking, complemented with suitable post-tests on comprehension as well as retrospective think aloud in order to obtain additional information on the linking process between perception and comprehension. As an addition to other research approaches this should most certainly help us to improve our knowledge of comprehension within an educational research framework.},
  isbn = {978-1-4503-1052-9},
  keywords = {code comprehension,code reading,CS Ed research,educational research,eye tracking,program comprehension},
  annotation = {36 citations (Crossref) [2022-06-09]}
}

@inproceedings{AnyCubesChildrenToyLearningAIScheidt2019a,
  title = {Any-{{Cubes}}: {{A Children}}'s {{Toy}} for {{Learning AI}}: {{Enhanced Play}} with {{Deep Learning}} and {{MQTT}}},
  shorttitle = {Any-{{Cubes}}},
  booktitle = {Proceedings of {{Mensch}} Und {{Computer}} 2019},
  author = {Scheidt, Alexander and Pulver, Tim},
  date = {2019-09-08},
  series = {{{MuC}}'19},
  pages = {893--895},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3340764.3345375},
  url = {https://doi.org/10.1145/3340764.3345375},
  urldate = {2022-04-25},
  abstract = {Here we present Any-Cubes, a prototype toy with which children can intuitively and playfully explore and understand machine learning as well as Internet of Things technology. Our prototype is a combination of deep learning-based image classification [12] and machine-to-machine (m2m) communication via MQTT. The system consists of two physical and tangible wooden cubes. Cube 1 ("sensor cube") is inspired by Google's teachable machine [14,15]. The sensor cube can be trained on any object or scenery. The machine learning functionality is directly implemented on the microcontroller (Raspberry Pi) by a Google Edge TPU Stick. Via MQTT protocol, the microcontroller sends its current status to Cube 2, the actuator cube. The actuator cube provides three switches (relays controlled by an Arduino board) to which peripheral devices can be connected. This allows simple if-then functions to be executed in real time, regardless of location. We envision our system as an intuitive didactic tool for schools and maker spaces.},
  isbn = {978-1-4503-7198-8},
  keywords = {Edge AI,Education,IoT,Machine Learning,Transfer Learning},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/AnyCubesChildrenToyLearningAIScheidt2019a.pdf}
}

@misc{APADictionaryPsychology2020b,
  title = {{{APA Dictionary}} of {{Psychology}}},
  date = {2020},
  journaltitle = {dictionary.app.org},
  url = {https://dictionary.apa.org/social-context},
  urldate = {2021-10-21},
  abstract = {social context the specific circumstance or general environment that serves as a social framework for individual or interpersonal behavior. This context frequently influences, at least to some degree, the actions and feelings that occur within it.},
  langid = {english}
}

@online{APADictionaryPsychology2020c,
  title = {{{APA Dictionary}} of {{Psychology}}},
  date = {2020},
  url = {https://dictionary.apa.org/social-context},
  urldate = {2021-10-21},
  abstract = {social context the specific circumstance or general environment that serves as a social framework for individual or interpersonal behavior. This context frequently influences, at least to some degree, the actions and feelings that occur within it.},
  langid = {english},
  organization = {{dictionary.app.org}}
}

@article{ApplicationComputationalModelsSocialNeuroscienceCharpentier2018,
  title = {The {{Application}} of {{Computational Models}} to {{Social Neuroscience}}: {{Promises}} and {{Pitfalls}}},
  shorttitle = {The {{Application}} of {{Computational Models}} to {{Social Neuroscience}}},
  author = {Charpentier, Caroline and O’Doherty, John},
  date = {2018-09-01},
  journaltitle = {Social Neuroscience},
  shortjournal = {Social Neuroscience},
  volume = {13},
  doi = {10.1080/17470919.2018.1518834},
  abstract = {Interactions with conspecifics are key to any social species. In order to navigate this social world, it is crucial for individuals to learn from and about others. Whether it is learning a new skill by observing a parent perform it, avoiding negative outcomes, or making complex collective decisions, understanding the mechanisms underlying such social cognitive processes has been of considerable interest to psychologists and neuroscientists, particularly to studies of learning and decision-making. Here, we review studies that have used computational modelling techniques, combined with neuroimaging, to shed light on how people learn and make decisions in social contexts. As opposed to previous methods used in social neuroscience studies, the computational approach allows one to directly examine where in the brain particular computations, as estimated by models of behavior, are implemented. Similar to studies of experiential learning, findings suggest that learning from others can be implemented using several strategies: vicarious reward learning, where one learns from observing the reward outcomes of another agent; action imitation, which relies on encoding a prediction error between the expected and actual actions of the other agent; and social inference, where one learns by inferring the goals and intentions of others. These strategies rely on distinct neural networks, which may be recruited adaptively depending on task demands, the environment and other social factors.},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/5IDPESTU/Charpentier and O’Doherty - 2018 - The Application of Computational Models to Social .pdf}
}

@online{AreYouSpectatorRealityAre,
  title = {Are You a Spectator to Reality? {{Or}} Are You Its Creator?},
  shorttitle = {Are You a Spectator to Reality?},
  url = {https://bigthink.com/the-well/your-brain-creates-reality/},
  urldate = {2022-07-22},
  abstract = {Signals from the environment, such as those detected by your sense organs, have no inherent psychological meaning. Your brain creates it.},
  langid = {american},
  organization = {{Big Think}},
  keywords = {Literature Review,notion},
  file = {/Users/brookeryan/Zotero/storage/56JHS6FV/your-brain-creates-reality.html}
}

@online{Artbreedera,
  title = {Artbreeder},
  url = {https://www.artbreeder.com/},
  urldate = {2022-06-09},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Zotero/storage/MTAJ2PWF/www.artbreeder.com.html}
}

@online{ArtificialNeuralNetworksAccuratelyPredict,
  title = {Artificial {{Neural Networks Accurately Predict Language Processing}} in the {{Brain}} | {{bioRxiv}}},
  url = {https://www.biorxiv.org/content/10.1101/2020.06.26.174482v1},
  urldate = {2022-08-13},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/52SVKZKX/2020.06.26.html}
}

@book{ArtistMachineWorldAIPoweredCreativityMiller2019,
  title = {The {{Artist}} in the {{Machine}}: {{The World}} of {{AI-Powered Creativity}}},
  shorttitle = {The {{Artist}} in the {{Machine}}},
  author = {Miller, Arthur I.},
  date = {2019-10-01},
  eprint = {YGquDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{MIT Press}},
  abstract = {An authority on creativity introduces us to AI-powered computers that are creating art, literature, and music that may well surpass the creations of humans.Today's computers are composing music that sounds “more Bach than Bach,” turning photographs into paintings in the style of Van Gogh's Starry Night, and even writing screenplays. But are computers truly creative—or are they merely tools to be used by musicians, artists, and writers? In this book, Arthur I. Miller takes us on a tour of creativity in the age of machines.~Miller, an authority on creativity, identifies the key factors essential to the creative process, from “the need for introspection” to “the ability to discover the key problem.” He talks to people on the cutting edge of artificial intelligence, encountering computers that mimic the brain and machines that have defeated champions in chess, Jeopardy!, and Go. In the central part of the book, Miller explores the riches of computer-created art, introducing us to artists and computer scientists who have, among much else, unleashed an artificial neural network to create a nightmarish, multi-eyed dog-cat; taught AI to imagine; developed a robot that paints; created algorithms for poetry; and produced the world's first computer-composed musical, Beyond the Fence, staged by Android Lloyd Webber and friends.But, Miller writes, in order to be truly creative, machines will need to step into the world. He probes the nature of consciousness and speaks to researchers trying to develop emotions and consciousness in computers. Miller argues that computers can already be as creative as humans—and someday will surpass us. But this is not a dystopian account; Miller celebrates the creative possibilities of artificial intelligence in art, music, and literature.},
  isbn = {978-0-262-35460-8},
  langid = {english},
  pagetotal = {429},
  keywords = {Computers / Artificial Intelligence / General,Social Science / Media Studies}
}

@online{ArtReproductionLegalANOUSKA2019,
  title = {Is Art Reproduction Legal?},
  author = {{ANOUSKA}},
  date = {2019-09-24T13:28:49+00:00},
  url = {https://fineartreproductionstudio.com/art-reproduction-articles/is-art-reproduction-legal/},
  urldate = {2022-07-21},
  abstract = {Is art reproduction legal? Yes art reproduction is perfectly legal as long as you adhere to copyright law. If an artist has been dead for over 70 years then},
  langid = {american},
  organization = {{The Fine Art Reproduction Studio}},
  keywords = {Etsy,notion}
}

@misc{AskingAnsweringQuestionsProgrammingChangeSillito,
  title = {‪{{Asking}} and Answering Questions during a Programming Change Task‬},
  author = {Sillito, J and Murphy, GC and De Volder, K},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=IgGxGN8AAAAJ&citation_for_view=IgGxGN8AAAAJ:u5HHmVD_uO8C},
  urldate = {2021-10-22},
  abstract = {‪J Sillito, GC Murphy, K De Volder‬, ‪IEEE Transactions on Software Engineering, 2008‬ - ‪Cited by 361‬}
}

@article{AskingAnsweringQuestionsProgrammingChangeSillito2008,
  title = {Asking and {{Answering Questions}} during a {{Programming Change Task}}},
  author = {Sillito, Jonathan and Murphy, Gail C. and De Volder, Kris},
  date = {2008-07},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {34},
  number = {4},
  pages = {434--451},
  issn = {1939-3520},
  doi = {10.1109/TSE.2008.26},
  abstract = {Little is known about the specific kinds of questions programmers ask when evolving a code base and how well existing tools support those questions. To better support the activity of programming, answers are needed to three broad research questions: 1) What does a programmer need to know about a code base when evolving a software system? 2) How does a programmer go about finding that information? 3) How well do existing tools support programmers in answering those questions? We undertook two qualitative studies of programmers performing change tasks to provide answers to these questions. In this paper, we report on an analysis of the data from these two user studies. This paper makes three key contributions. The first contribution is a catalog of 44 types of questions programmers ask during software evolution tasks. The second contribution is a description of the observed behavior around answering those questions. The third contribution is a description of how existing deployed and proposed tools do, and do not, support answering programmers' questions.},
  keywords = {Computer science,Data analysis,Enhancement,Genetic programming,IEEE activities,Lab-on-a-chip,Programming Environments/Construction Tools,Programming profession,Software psychology,Software systems,Software tools},
  annotation = {180 citations (Crossref) [2022-06-09]}
}

@online{AskingAnsweringQuestionsProgrammingChangeSillitoa,
  title = {‪{{Asking}} and Answering Questions during a Programming Change Task‬},
  author = {Sillito, J and Murphy, GC and De Volder, K},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=IgGxGN8AAAAJ&citation_for_view=IgGxGN8AAAAJ:u5HHmVD_uO8C},
  urldate = {2021-10-22},
  abstract = {‪J Sillito, GC Murphy, K De Volder‬, ‪IEEE Transactions on Software Engineering, 2008‬ - ‪Cited by 361‬}
}

@article{BAKER20053,
  title = {An Experimental Card Game for Teaching Software Engineering Processes},
  author = {Baker, Alex and Oh Navarro, Emily and van der Hoek, André},
  options = {useprefix=true},
  date = {2005},
  journaltitle = {Journal of Systems and Software},
  volume = {75},
  number = {1},
  pages = {3--16},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2004.02.033},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121204000378},
  abstract = {The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small “toy” software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in real-world software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.},
  keywords = {Educational games,Simulation games,Software engineering education,Software engineering simulation},
  annotation = {110 citations (Crossref) [2022-06-09]}
}

@inproceedings{BarriersFacedNewcomersOpenSourceSteinmacher2014,
  title = {Barriers {{Faced}} by {{Newcomers}} to {{Open Source Projects}}: {{A Systematic Review}}},
  shorttitle = {Barriers {{Faced}} by {{Newcomers}} to {{Open Source Projects}}},
  author = {Steinmacher, Igor and Graciotto Silva, Marco Aurélio and Gerosa, Marco Aurelio},
  date = {2014-05},
  volume = {427},
  doi = {10.1007/978-3-642-55128-4_21},
  abstract = {To remain sustainable, some open source projects need a constant influx of new volunteers, or newcomers. However, the newcom-ers face different kinds of problems when onboarding to a project. In this paper we present the results of a systematic literature review aiming at identifying the barriers that a newcomer can face when contributing to an Open Source Software project. We identified and analyzed 21 studies that evidence this kind of problem. As a result we provide a hierarchical model that relies on five categories of barriers: finding a way to start, social interactions, code issues, documentation problems and newcom-ers' knowledge. The most evidenced barriers are newcomers' previous technical skills, receiving response from community, centrality of social contacts, and finding the appropriate way to start contributing. This classification provides a baseline for further researches related to new-comers onboarding.},
  isbn = {978-3-642-55127-7},
  annotation = {15 citations (Crossref) [2022-06-09]}
}

@online{BCSAAPMentorInformation,
  title = {{{BCS AAP Mentor Information}}},
  url = {https://docs.google.com/document/d/1yJR4yIGlGLjMivzZQZn7JQcVMADzotBwD5udT58hNFE/edit?usp=embed_facebook},
  urldate = {2022-07-14},
  abstract = {BCS AAP Mentor Information  Thank you for your interest in mentoring for the BCS Application Assistance Program!  Outline for this resource: BCS AAP Overview Your Role as a Mentor General Application Review Guidelines   I. Overview  We chose to model our program after the Biological Engineering d...},
  langid = {english},
  organization = {{Google Docs}},
  keywords = {notion,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/DM7GGU3B/edit.html}
}

@book{BeautifulCodeOram2007,
  title = {Beautiful Code},
  editor = {Oram, Andrew and Wilson, Greg},
  date = {2007},
  series = {Theory in Practice Series},
  edition = {1st. ed},
  publisher = {{O'Reilly}},
  location = {{Beijing ; Sebastapol, Calif}},
  isbn = {978-0-596-51004-6},
  langid = {english},
  pagetotal = {593},
  keywords = {Software engineering},
  annotation = {OCLC: ocn163289538}
}

@article{BehavioralpatternExplorationDevelopmentInstructionalToolHsu2021b,
  title = {Behavioral-Pattern Exploration and Development of an Instructional Tool for Young Children to Learn {{AI}}},
  author = {Hsu, Ting-Chia and Abelson, Hal and Lao, Natalie and Tseng, Yu-Han and Lin, Yi-Ting},
  date = {2021},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {2},
  pages = {100012},
  issn = {2666920X},
  doi = {10.1016/j.caeai.2021.100012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000060},
  urldate = {2022-06-09},
  langid = {english}
}

@article{BiologicalIntelligenceBehaviorLearningTheoryZhang,
  title = {Biological {{Intelligence}}: {{From Behavior}} to {{Learning Theory}}},
  author = {Zhang, Tony},
  pages = {180},
  abstract = {Knowing how to learn, think, and act is not just a hallmark of intelligence, but a necessity of survival for many organisms. Behavior, the complete set of actions of species, allows us to glimpse into the minds of humans and animals, and by extension, intelligence itself. Biological intelligence is characterized by fast adaptation to changes and challenges, which is what allows species to survive in natural environments from starvation and predation. To study learning in a controlled setting, we can observe the behavior evoked through decision-making tasks that make it possible to quantify and analyze learning. By modeling the extracted behavioral features, we could start to understand the possible underlying mechanisms by proposing neural theory models, and look for those signals in the brain. Understanding the neural mechanisms of learning also strengthens the basis for building intelligent machines that are flexible and adaptive to the nonstationary world we live in. In this thesis, I present works in (1) automating behavioral setups and modeling suboptimal behavior in a traditional decision-making task [5], (2) using an ethological navigation task to characterize fast-sequence learning [6], and (3) how neural theory can explain some core behavioral phenomena in (2), and be used to solve a central problem in graph search [8].},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/7P649T93/Zhang - Biological Intelligence From Behavior to Learning.pdf}
}

@inproceedings{BlockchainBasedTrustedInstantMessagingModelWang2021,
  title = {Blockchain-{{Based Trusted Instant Messaging Model Research}}},
  booktitle = {2021 4th {{International Conference}} on {{Hot Information-Centric Networking}} ({{HotICN}})},
  author = {Wang, Huiyuan and Yu, Yimin and Zhao, Jinyi and Wang, Jingyi},
  date = {2021-11},
  pages = {32--37},
  doi = {10.1109/HotICN53262.2021.9680848},
  abstract = {Instant messaging (IM) provides real-time communication between two or more participants over the Internet. Today's instant messaging products are proliferating and playing an increasingly important role in life. However, the problem of user identity and communication data leakage is gradually revealed. To address the data privacy and security problems brought by current IM systems, this paper proposes a trusted IM model based on blockchain technology, applying IPFS technology, blockchain technology combined with data encryption technology and distributed identity (DID) to design a decentralized instant messaging model, where user identity and data cannot be unilaterally tampered with or destroyed, ensuring the privacy of communication data. The overall structure of the system and the design of each module function are also introduced to provide new ideas for the study of a secure decentralized communication model.},
  eventtitle = {2021 4th {{International Conference}} on {{Hot Information-Centric Networking}} ({{HotICN}})},
  keywords = {Blockchain,Blockchains,Data models,Data privacy,Decentralized Identity (DID),Encryption,Information-centric networking,Instant messaging,Instant Messaging (IM),IPFS,Real-time systems,Smart Contract},
  annotation = {0 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/YBCSZCIP/Wang et al. - 2021 - Blockchain-Based Trusted Instant Messaging Model R.pdf}
}

@unpublished{BrainMixtureExpertsO'Doherty2021,
  title = {The {{Brain}} as a {{Mixture}} of {{Experts}}},
  author = {O'Doherty, John P},
  date = {2021},
  url = {https://www.youtube.com/watch?v=jYd271w1bLk},
  keywords = {Advisor Literature,notion}
}

@misc{CallGraphIntelliJIDEsPlugin2022,
  title = {Call {{Graph}} - {{IntelliJ IDEs Plugin}} \textbackslash textbar {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/12304-call-graph},
  urldate = {2022-01-31}
}

@online{CallGraphIntelliJIDEsPlugin2022a,
  title = {Call {{Graph}} - {{IntelliJ IDEs Plugin}} | {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/12304-call-graph},
  urldate = {2022-01-31}
}

@online{CalNewport,
  title = {Cal {{Newport}}},
  url = {https://www.timeblockplanner.com/},
  urldate = {2022-08-16},
  abstract = {A Daily Method for Deep Work in a Distracted World. Available now where books are sold.},
  langid = {american},
  organization = {{Cal Newport}},
  keywords = {notion,time management,wiki},
  file = {/Users/brookeryan/Zotero/storage/QAYAT8QQ/www.timeblockplanner.com.html}
}

@online{CalNewportDeepWorkTime,
  title = {Cal {{Newport}}'s {{Deep Work Time Blocking Method}}},
  url = {https://www.neuyear.net/blogs/productivity/cal-newports-deep-work-time-blocking-method},
  urldate = {2022-08-16},
  abstract = {Cal Newport is a professor of Computer Science at Georgetown University, New York Times bestselling author and a~highly productive individual. In his book, Deep Work, he explains his key to daily productivity: schedule out every minute of your day. This may sound extreme, but he argues it’s the best way to keep yourself focused on “deep work” and fight off the temptation to engage in shallow tasks (like email and social media). And it’s easier than you think. He calls it “Time Blocking” and here’s his method. Time Blocking Steps 1. Every morning before work, grab a lined sheet of paper and, on the left side of the page, write the hours of the day on every other line (just the hours you typically work: 8am-5pm say). Thus, each line represents 30 minutes of work. 2. Divide the hours of the day into blocks and assign activities to the blocks. For example, you might block out 9am to 11am to write a blog post. So you draw a box on the page from 9am to 11am and write “blog post” in the box. Do this for the rest of your time in the day.  3. To keep things clean, the minimum time block should be 30 minutes. So for small tasks, batch them into a “task block” - just draw a line to the right side of the page where you list the tasks you’ll accomplish in that block. That’s it. When you’re done, every minute of your day should be blocked. As Cal says, “You have, in effect, given every minute of your day a job. Now as you go through your day, use this schedule to guide you.” Some Caveats... You probably have some objections at this point. But first read these caveats:1. Not every block needs to be a work task! Make sure to make blocks for lunch or relaxation breaks. In Deep Work, Cal argues extensively for the need to give our brains a rest.2. Your schedule will likely break at some point every day Either because your time estimates were off, or you were interrupted with some new urgent tasks. This is to be expected. But don’t throw away your schedule, just take a few minutes when you can to re-evaluate your priorities and redraw your schedule (either on another page, or cross out the old schedule and write next to it). “Your goal is not to stick to a given schedule at all costs; it’s instead to maintain a thoughtful say in what you’re doing with your time going forward.”3. Don't be too rigidA common concern is that following a rigid schedule will keep you from being able to follow creative ideas or great opportunities when they strike. The solution is simple: if something more important comes up--like following a creative inspiration--then it’s fine to ignore the schedule until it is played out. At which time, simply re-build your schedule for the remaining time in your day. The schedule is not meant to be a prison, but it is meant to protect you from the creep of less important--but more enticing--tasks stealing time from your important work. Tactics to keep your schedule intact Be conservative with your time estimates At first, you will very likely underestimate the time your tasks take. By overestimating, you are less likely to break your schedule.Use “overflow conditional” blocks That is, if you’re not sure how long a task will take, follow it with an overflow block that has a non-urgent task. If you complete that first task on time, then do the non-urgent task, but if not, you can use-up that overflow block to finish up your first task and save the non-urgent task for another time (or day). Have multiple “task blocks” in the day which you can use to catch-up on unexpected tasks that may arise. Perhaps end each day with a block to catch unfinished things.Incidentally, the Action Plan Pad and Week Dominator are well suited to Cal Newport’s time blocking method. They give you a more structured template than a simple lined sheet of paper to block out your day. Download a free PDF version of the Action Plan Pad here.Newport concludes: “It’s natural to resist this idea [to schedule every minute of your day], as it’s undoubtedly easier to continue to allow the twin forces of internal whim and external requests to drive your schedule. But you must overcome this distrust of structure if you want to approach your true potential as someone who creates things that matter.”His book, Deep Work, is full of helpful insights pulled from well respected studies on human psychology and other experts of productivity. If you want to improve your capacity for},
  langid = {english},
  organization = {{NeuYear.net}},
  keywords = {notion,time management,wiki}
}

@online{CalypsoCozmoTakeYourCozmoa,
  title = {Calypso for {{Cozmo}}: {{Take}} Your {{Cozmo}} Programming to the next Level.},
  shorttitle = {Calypso for {{Cozmo}}},
  url = {https://calypso.software/},
  urldate = {2022-04-20},
  abstract = {Calypso is a robot intelligence framework for the Cozmo robot by Anki.},
  organization = {{Calypso Software}},
  file = {/Users/brookeryan/Zotero/storage/TQ5H6HSL/calypso.software.html}
}

@inproceedings{CaseStudyOnboardingSoftwareTeamsJu2021,
  title = {A Case Study of Onboarding in Software Teams: {{Tasks}} and Strategies},
  shorttitle = {A Case Study of Onboarding in Software Teams},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Ju, An and Sajnani, Hitesh and Kelly, Scot and Herzig, Kim},
  date = {2021},
  pages = {613--623},
  publisher = {{IEEE}}
}

@article{CausalAccountBrainNetworkComputationsHill2017,
  title = {A Causal Account of the Brain Network Computations Underlying Strategic Social Behavior},
  author = {Hill, Christopher A and Suzuki, Shinsuke and Polania, Rafael and Moisa, Marius and O'Doherty, John P and Ruff, Christian C},
  date = {2017-08},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {20},
  number = {8},
  pages = {1142--1149},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4602},
  url = {http://www.nature.com/articles/nn.4602},
  urldate = {2022-07-22},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/DQMBEY6C/Hill et al. - 2017 - A causal account of the brain network computations.pdf}
}

@article{CautionaryNotePredictingSocialJudgmentsKeles2021,
  title = {A {{Cautionary Note}} on {{Predicting Social Judgments}} from {{Faces}} with {{Deep Neural Networks}}},
  author = {Keles, Umit and Lin, Chujun and Adolphs, Ralph},
  date = {2021-12},
  journaltitle = {Affective Science},
  shortjournal = {Affec Sci},
  volume = {2},
  number = {4},
  pages = {438--454},
  issn = {2662-2041, 2662-205X},
  doi = {10.1007/s42761-021-00075-5},
  url = {https://link.springer.com/10.1007/s42761-021-00075-5},
  urldate = {2022-07-22},
  abstract = {People spontaneously infer other people’s psychology from faces, encompassing inferences of their affective states, cognitive states, and stable traits such as personality. These judgments are known to be often invalid, but nonetheless bias many social decisions. Their importance and ubiquity have made them popular targets for automated prediction using deep convolutional neural networks (DCNNs). Here, we investigated the applicability of this approach: how well does it generalize, and what biases does it introduce? We compared three distinct sets of features (from a face identification DCNN, an object recognition DCNN, and using facial geometry), and tested their prediction across multiple out-of-sample datasets. Across judgments and datasets, features from both pre-trained DCNNs provided better predictions than did facial geometry. However, predictions using object recognition DCNN features were not robust to superficial cues (e.g., color and hair style). Importantly, predictions using face identification DCNN features were not specific: models trained to predict one social judgment (e.g., trustworthiness) also significantly predicted other social judgments (e.g., femininity and criminal), and at an even higher accuracy in some cases than predicting the judgment of interest (e.g., trustworthiness). Models trained to predict affective states (e.g., happy) also significantly predicted judgments of stable traits (e.g., sociable), and vice versa. Our analysis pipeline not only provides a flexible and efficient framework for predicting affective and social judgments from faces but also highlights the dangers of such automated predictions: correlated but unintended judgments can drive the predictions of the intended judgments.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/3P6DZZYE/Keles et al. - 2021 - A Cautionary Note on Predicting Social Judgments f.pdf}
}

@incollection{CHAPTERNeuroscienceEmotionHumansAdolphs2018,
  title = {{{CHAPTER}} 9. {{The Neuroscience}} of {{Emotion}} in {{Humans}}},
  booktitle = {{{CHAPTER}} 9. {{The Neuroscience}} of {{Emotion}} in {{Humans}}},
  author = {Adolphs, Ralph and Anderson, David},
  date = {2018-06-05},
  pages = {251--278},
  publisher = {{Princeton University Press}},
  doi = {10.23943/9781400889914-012},
  url = {https://www.degruyter.com/document/doi/10.23943/9781400889914-012/html?lang=de},
  urldate = {2022-07-22},
  abstract = {Das Kapitel CHAPTER 9. The Neuroscience of Emotion in Humans erschien in The Neuroscience of Emotion auf Seite 251.},
  isbn = {978-1-4008-8991-4},
  langid = {english},
  keywords = {Advisor Literature,notion}
}

@misc{CharacterizingCanonicalComputationsGeneratingPhenomenalPeters2021,
  title = {Towards {{Characterizing}} the {{Canonical Computations Generating Phenomenal Experience}}},
  author = {Peters, Megan A. K.},
  date = {2021-04-27T01:56:18},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/bqfr6},
  url = {https://psyarxiv.com/bqfr6/},
  urldate = {2022-07-19},
  abstract = {Few people tackle the neural or computational basis of qualitative experience (Frith, 2019). Why? One major reason is that science and philosophy have struggled to propose how we might even begin to start studying it. Here I propose that metacognitive computations, and the subjective feelings that accompany them, possess multiple unique properties that provide a powerful opportunity for studying the neural and computational correlates of subjective experience. These properties fall into five categories: (1) The metacognitive process leads to subjective experiences: there is something it is like to feel confident; (2) Metacognition is “about” internal representations, formalizing introspective access; (3) Metacognitive computations are “recursive” (i.e., can apply to meta-cognition and meta-meta-cognition), offering the opportunity to discover “canonical computations” that are preserved across processing levels and neurobiological implementations; (4) Metacognitive processes are objectively characterizable and anchored to empirically observable behavior; and (5) Metacognitive computations are unobservable yet hierarchically dependent, presenting a unique computational opportunity to develop sensitive, specific models. I define the Metacognition as a Step Toward Explaining Phenomenology (M-STEP) argument to state that, given these properties, computational models of metacognition represent an empirically-tractable early step in characterizing the generative process that constructs qualitative experience. I also present practical ways to engage in the M-STEP process. By applying decades of developments in computational cognitive science and formal computational model comparisons to the specific properties of perceptual metacognition, we may reveal new and exciting insights about how the brain constructs subjective conscious experiences and the nature of those experiences themselves.},
  langid = {american},
  keywords = {Neuroscience of Consciousness,notion},
  file = {/Users/brookeryan/Zotero/storage/4B5XKM2U/Peters - 2021 - Towards Characterizing the Canonical Computations .pdf}
}

@misc{CharacterizingCanonicalComputationsGeneratingPhenomenalPeters2021a,
  title = {Towards {{Characterizing}} the {{Canonical Computations Generating Phenomenal Experience}}},
  author = {Peters, Megan A. K.},
  date = {2021-04-27T01:56:18},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/bqfr6},
  url = {https://psyarxiv.com/bqfr6/},
  urldate = {2022-07-22},
  abstract = {Few people tackle the neural or computational basis of qualitative experience (Frith, 2019). Why? One major reason is that science and philosophy have struggled to propose how we might even begin to start studying it. Here I propose that metacognitive computations, and the subjective feelings that accompany them, possess multiple unique properties that provide a powerful opportunity for studying the neural and computational correlates of subjective experience. These properties fall into five categories: (1) The metacognitive process leads to subjective experiences: there is something it is like to feel confident; (2) Metacognition is “about” internal representations, formalizing introspective access; (3) Metacognitive computations are “recursive” (i.e., can apply to meta-cognition and meta-meta-cognition), offering the opportunity to discover “canonical computations” that are preserved across processing levels and neurobiological implementations; (4) Metacognitive processes are objectively characterizable and anchored to empirically observable behavior; and (5) Metacognitive computations are unobservable yet hierarchically dependent, presenting a unique computational opportunity to develop sensitive, specific models. I define the Metacognition as a Step Toward Explaining Phenomenology (M-STEP) argument to state that, given these properties, computational models of metacognition represent an empirically-tractable early step in characterizing the generative process that constructs qualitative experience. I also present practical ways to engage in the M-STEP process. By applying decades of developments in computational cognitive science and formal computational model comparisons to the specific properties of perceptual metacognition, we may reveal new and exciting insights about how the brain constructs subjective conscious experiences and the nature of those experiences themselves.},
  langid = {american},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/MNUAF4CC/Peters - 2021 - Towards Characterizing the Canonical Computations .pdf}
}

@online{CHI2020FreeProceedings,
  title = {{{CHI}} 2020 {{Free Proceedings}}},
  url = {https://chi2020.acm.org/chi-2020-free-proceedings/},
  urldate = {2022-06-10},
  langid = {british},
  organization = {{CHI 2020}},
  keywords = {AI Education Project,notion,systematic literature review},
  file = {/Users/brookeryan/Zotero/storage/SBK9GZ2S/chi-2020-free-proceedings.html}
}

@article{ChildHackerBuildingMoreHumanlikeRule,
  title = {The Child as Hacker: {{Building}} More Human-like Models of Learning},
  author = {Rule, Joshua S},
  pages = {258},
  abstract = {Cognitive science faces a radical challenge in explaining the richness of human learning and cognitive development. This thesis proposes that developmental theories can address the challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that learning from childhood onward is analogous to a style of programming called hacking—making code better along many dimensions through an open-ended and internally-motivated set of diverse values and activities. This thesis also develops a first attempt to formalize and assess the child as hacker view through an in-depth empirical study of human and machine concept learning. It introduces list functions as a domain for psychological investigation, demonstrating how they subsume many classic concept learning tasks while opening new avenues for exploring algorithmic thinking over complex structures. It also presents HL, a computational learning model whose representations, objectives, and mechanisms reflect core principles of hacking. Existing work on concept learning shows that learners both prefer simple explanations of data and find them easier to learn than complex ones. The child as hacker, by contrast, suggests that learners use mechanisms that dissociate hypothesis complexity and learning difficulty for certain problem classes. We thus conduct a large-scale experiment exploring list functions that vary widely in difficulty and algorithmic content to help identify structural sources of learning difficulty. We find that while description length alone predicts learning, predictions are much better when accounting for concepts’ semantic features. These include the use of internal arguments, counting knowledge, case-based and recursive reasoning, and visibility—a measure we introduce to modify description length based on the complexity of inferring each symbol in a description. We further show that HL’s hacker-like design uses these semantic features to better predict human performance than several alternative models of learning as programming. These results lay groundwork for a new generation of computational models and demonstrate how the child as hacker hypothesis can productively contribute to our understanding of learning.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/TWZHTGBV/Rule - The child as hacker Building more human-like mode.pdf}
}

@article{ChildrenLearningNumberWordsIndigenousPiantadosi2014,
  title = {Children's Learning of Number Words in an Indigenous Farming-Foraging Group},
  author = {Piantadosi, Steven T. and Jara-Ettinger, Julian and Gibson, Edward},
  date = {2014-07},
  journaltitle = {Developmental Science},
  shortjournal = {Dev Sci},
  volume = {17},
  number = {4},
  pages = {553--563},
  issn = {1363755X},
  doi = {10.1111/desc.12078},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/desc.12078},
  urldate = {2022-08-10},
  abstract = {We show that children in the Tsimane’, a farming-foraging group in the Bolivian rain-forest, learn number words along a similar developmental trajectory to children from industrialized countries. Tsimane’ children successively acquire the first three or four number words before fully learning how counting works. However, their learning is substantially delayed relative to children from the United States, Russia, and Japan. The presence of a similar developmental trajectory likely indicates that the incremental stages of numerical knowledge – but not their timing — reflect a fundamental property of number concept acquisition which is relatively independent of language, culture, age, and early education.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/Q5N6BM9P/Piantadosi et al. - 2014 - Children's learning of number words in an indigeno.pdf}
}

@unpublished{CLIPCLOPCLIPGuidedCollagePhotomontageMirowski2022,
  title = {{{CLIP-CLOP}}: {{CLIP-Guided Collage}} and {{Photomontage}}},
  shorttitle = {{{CLIP-CLOP}}},
  author = {Mirowski, Piotr and Banarse, Dylan and Malinowski, Mateusz and Osindero, Simon and Fernando, Chrisantha},
  date = {2022-05-19},
  number = {arXiv:2205.03146},
  eprint = {2205.03146},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.03146},
  url = {http://arxiv.org/abs/2205.03146},
  urldate = {2022-06-10},
  abstract = {The unabated mystique of large-scale neural networks, such as the CLIP dual image-and-text encoder, popularized automatically generated art. Increasingly more sophisticated generators enhanced the artworks' realism and visual appearance, and creative prompt engineering enabled stylistic expression. Guided by an artist-in-the-loop ideal, we design a gradient-based generator to produce collages. It requires the human artist to curate libraries of image patches and to describe (with prompts) the whole image composition, with the option to manually adjust the patches' positions during generation, thereby allowing humans to reclaim some control of the process and achieve greater creative freedom. We explore the aesthetic potentials of high-resolution collages, and provide an open-source Google Colab as an artistic tool.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/CLIPCLOPCLIPGuidedCollagePhotomontageMirowski2022.pdf;/Users/brookeryan/Zotero/storage/ULFVCDM2/2205.html}
}

@inproceedings{CodeCity3DVisualizationLargescaleSoftwareWettel2008,
  title = {{{CodeCity}}: {{3D}} Visualization of Large-Scale Software},
  shorttitle = {{{CodeCity}}},
  booktitle = {Companion of the 30th International Conference on {{Software}} Engineering},
  author = {Wettel, Richard and Lanza, Michele},
  date = {2008-05},
  series = {{{ICSE Companion}} '08},
  pages = {921--922},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1370175.1370188},
  url = {https://doi.org/10.1145/1370175.1370188},
  urldate = {2021-10-22},
  abstract = {CodeCity is a language-independent interactive 3D visualization tool for the analysis of large software systems. Based on a city metaphor, it depicts classes as buildings and packages as districts of a "software city". By offering consistent locality and solid orientation points we keep the viewer oriented during the exploration of a city. We applied our tool on several large-scale industrial systems.},
  isbn = {978-1-60558-079-1},
  keywords = {visualization},
  annotation = {50 citations (Crossref) [2022-06-09]}
}

@book{CodeReadingOpenSourcePerspectiveSpinellis2003,
  title = {Code {{Reading}}: {{The Open Source Perspective}}},
  shorttitle = {Code {{Reading}}},
  author = {Spinellis, Diomidis},
  date = {2003-05-27},
  eprint = {8lYbNfsAVT4C},
  eprinttype = {googlebooks},
  publisher = {{Addison-Wesley Professional}},
  abstract = {If you are a programmer, you need this book.  You've got a day to add a new feature in a 34,000-line program: Where do you start? Page 333  How can you understand and simplify an inscrutable piece of code? Page 39  Where do you start when disentangling a complicated build process? Page 167  How do you comprehend code that appears to be doing five things in parallel? Page 132   You may read code because you have to--to fix it, inspect it, or improve it. You may read code the way an engineer examines a machine--to discover what makes it tick. Or you may read code because you are scavenging--looking for material to reuse.  Code-reading requires its own set of skills, and the ability to determine which technique you use when is crucial. In this indispensable book, Diomidis Spinellis uses more than 600 real-world examples to show you how to identify good (and bad) code: how to read it, what to look for, and how to use this knowledge to improve your own code.   Fact: If you make a habit of reading good code, you will write better code yourself.},
  isbn = {978-0-672-33370-5},
  langid = {english},
  pagetotal = {529},
  keywords = {Computers / Software Development & Engineering / General}
}

@online{CollaborativeApproachTeachingSoftwareArchitecture,
  title = {A {{Collaborative Approach}} to {{Teaching Software Architecture}} | {{Proceedings}} of the 2017 {{ACM SIGCSE Technical Symposium}} on {{Computer Science Education}}},
  url = {https://dl.acm.org/doi/10.1145/3017680.3017737},
  urldate = {2021-10-22}
}

@inproceedings{CollaborativeApproachTeachingSoftwareArchitectureVanDeursen2017,
  title = {A {{Collaborative Approach}} to {{Teaching Software Architecture}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGCSE Technical Symposium}} on {{Computer Science Education}}},
  author = {Van Deursen, Arie and Aniche, Maurício and Aué, Joop and Slag, Rogier and De Jong, Michael and Nederlof, Alex and Bouwers, Eric},
  date = {2017-03},
  series = {{{SIGCSE}} '17},
  pages = {591--596},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3017680.3017737},
  url = {https://doi.org/10.1145/3017680.3017737},
  urldate = {2021-10-22},
  abstract = {Teaching software architecture is hard. The topic is abstract and is best understood by experiencing it, which requires proper scale to fully grasp its complexity. Furthermore, students need to practice both technical and social skills to become good software architects. To overcome these teaching challenges, we developed the Collaborative Software Architecture Course. In this course, participants work together to study and document a large, open source software system of their own choice. In the process, all communication is transparent in order to foster an open learning environment, and the end-result is published as an online book to benefit the larger open source community. We have taught this course during the past four years to classes of 50-100 students each. Our experience suggests that: (1) open source systems can be successfully used to let students gain experience with key software architecture concepts, (2) students are capable of making code contributions to the open source projects, (3) integrators (architects) from open source systems are willing to interact with students about their contributions, (4) working together on a joint book helps teams to look beyond their own work, and study the architectural descriptions produced by the other teams.},
  isbn = {978-1-4503-4698-6},
  keywords = {collaborative book writing,open learning,software architecture,software engineering education},
  annotation = {12 citations (Crossref) [2022-06-09]}
}

@incollection{CollaborativeTechnologiesChildrenSpecialNeedsBaykal2020,
  title = {Collaborative {{Technologies}} for {{Children}} with {{Special Needs}}: {{A Systematic Literature Review}}},
  shorttitle = {Collaborative {{Technologies}} for {{Children}} with {{Special Needs}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Baykal, Gökçe Elif and Van Mechelen, Maarten and Eriksson, Eva},
  date = {2020-04-21},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3313831.3376291},
  urldate = {2022-06-09},
  abstract = {This paper presents a systematic literature review on collaborative technologies for children with special needs in ACM Digital Library. The aim of the review is to (1) reveal the current state of the art, (2) identify the types of technologies and contexts of use, the demographics and special needs of the target group, and the methodological approaches and theoretical groundings, and (3) define a future research agenda. The results of the systematic literature review show that collaborative technologies for children with special needs are increasingly gaining attention, mostly involve tangible and/or embodied interaction, and are often developed for use in the classroom. The target group that is most represented are boys between 6 to 12 years with Autism Spectrum Disorder. The results further show a wide range of evaluation criteria for measuring collaboration, an interchanging use of theoretical concepts and a lack of definitions for the concept collaboration, and a need for more demographically diverse studies.},
  isbn = {978-1-4503-6708-0},
  keywords = {AI Education Project,cci,collaboration,collaborative learning,collaborative technologies,notion,special need,systematic literature review}
}

@article{ColorNamingLanguagesReflectsColorGibson2017,
  title = {Color Naming across Languages Reflects Color Use},
  author = {Gibson, Edward and Futrell, Richard and Jara-Ettinger, Julian and Mahowald, Kyle and Bergen, Leon and Ratnasingam, Sivalogeswaran and Gibson, Mitchell and Piantadosi, Steven T. and Conway, Bevil R.},
  date = {2017-10-03},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {40},
  pages = {10785--10790},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1619666114},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1619666114},
  urldate = {2022-08-13},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/T3USWIZN/Gibson et al. - 2017 - Color naming across languages reflects color use.pdf}
}

@book{ComplexPTSDSurvivingThrivingGUIDEWalker2013,
  title = {Complex {{PTSD}}: {{From Surviving}} to {{Thriving}}: {{A GUIDE AND MAP FOR RECOVERING FROM CHILDHOOD TRAUMA}}},
  shorttitle = {Complex {{PTSD}}},
  author = {Walker, Pete},
  date = {2013-12-18},
  publisher = {{Azure Coyote Publishing}},
  langid = {english},
  pagetotal = {376},
  keywords = {Kindle}
}

@article{ComprehensionComputerCodeReliesPrimarilyIvanova2020,
  title = {Comprehension of Computer Code Relies Primarily on Domain-General Executive Brain Regions},
  author = {Ivanova, Anna A and Srikant, Shashank and Sueoka, Yotaro and Kean, Hope H and Dhamala, Riva and O'Reilly, Una-May and Bers, Marina U and Fedorenko, Evelina},
  editor = {Martin, Andrea E and Behrens, Timothy E and Matchin, William and Bornkessel-Schlesewsky, Ina},
  date = {2020-12-15},
  journaltitle = {eLife},
  volume = {9},
  pages = {e58906},
  issn = {2050-084X},
  doi = {10.7554/eLife.58906},
  url = {https://doi.org/10.7554/eLife.58906},
  urldate = {2021-10-22},
  abstract = {Computer programming is a novel cognitive tool that has transformed modern society. What cognitive and neural mechanisms support this skill? Here, we used functional magnetic resonance imaging to investigate two candidate brain systems: the multiple demand (MD) system, typically recruited during math, logic, problem solving, and executive tasks, and the language system, typically recruited during linguistic processing. We examined MD and language system responses to code written in Python, a text-based programming language (Experiment 1) and in ScratchJr, a graphical programming language (Experiment 2); for both, we contrasted responses to code problems with responses to content-matched sentence problems. We found that the MD system exhibited strong bilateral responses to code in both experiments, whereas the language system responded strongly to sentence problems, but weakly or not at all to code problems. Thus, the MD system supports the use of novel cognitive tools even when the input is structurally similar to natural language.},
  keywords = {computer code,fMRI,language,Literature Review,multiple demand,notion,programming},
  annotation = {23 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/Q39IBJ6X/Ivanova et al. - 2020 - Comprehension of computer code relies primarily on.pdf}
}

@article{ComprehensiveModelCodeReadabilityScalabrino2018,
  title = {A Comprehensive Model for Code Readability},
  author = {Scalabrino, Simone and Linares-Vásquez, Mario and Oliveto, Rocco and Poshyvanyk, Denys},
  date = {2018},
  journaltitle = {Journal of Software: Evolution and Process},
  volume = {30},
  number = {6},
  pages = {e1958},
  issn = {2047-7481},
  doi = {10.1002/smr.1958},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1958},
  urldate = {2021-10-05},
  abstract = {Unreadable code could compromise program comprehension, and it could cause the introduction of bugs. Code consists of mostly natural language text, both in identifiers and comments, and it is a particular form of text. Nevertheless, the models proposed to estimate code readability take into account only structural aspects and visual nuances of source code, such as line length and alignment of characters. In this paper, we extend our previous work in which we use textual features to improve code readability models. We introduce 2 new textual features, and we reassess the readability prediction power of readability models on more than 600 code snippets manually evaluated, in terms of readability, by 5K+ people. We also replicate a study by Buse and Weimer on the correlation between readability and FindBugs warnings, evaluating different models on 20 software systems, for a total of 3M lines of code. The results demonstrate that (1) textual features complement other features and (2) a model containing all the features achieves a significantly higher accuracy as compared with all the other state-of-the-art models. Also, readability estimation resulting from a more accurate model, ie, the combined model, is able to predict more accurately FindBugs warnings.},
  langid = {english},
  keywords = {code readability,quality warning prediction,textual analysis},
  annotation = {18 citations (Crossref) [2022-06-09] \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.1958}
}

@article{ComputationalNeuroethologyCallActionDatta2019,
  title = {Computational {{Neuroethology}}: {{A Call}} to {{Action}}},
  shorttitle = {Computational {{Neuroethology}}},
  author = {Datta, Sandeep Robert and Anderson, David J. and Branson, Kristin and Perona, Pietro and Leifer, Andrew},
  date = {2019-10-09},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {104},
  number = {1},
  pages = {11--24},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.09.038},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627319308414},
  urldate = {2022-07-25},
  abstract = {The brain is worthy of study because it is in charge of behavior. A flurry of recent technical advances in measuring and quantifying naturalistic behaviors provide an important opportunity for advancing brain science. However, the problem of understanding unrestrained behavior in the context of neural recordings and manipulations remains unsolved, and developing approaches to addressing this challenge is critical. Here we discuss considerations in computational neuroethology—the science of quantifying naturalistic behaviors for understanding the brain—and propose strategies to evaluate progress. We point to open questions that require resolution and call upon the broader systems neuroscience community to further develop and leverage measures of naturalistic, unrestrained behavior, which will enable us to more effectively probe the richness and complexity of the brain.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/8XGFTKKS/Datta et al. - 2019 - Computational Neuroethology A Call to Action.pdf}
}

@article{ConnectedArtsLearningCultivatingEquityPeppler2022,
  title = {Connected {{Arts Learning}}: {{Cultivating Equity Through Connected}} and {{Creative Educational Experiences}}},
  shorttitle = {Connected {{Arts Learning}}},
  author = {Peppler, Kylie and Dahn, Maggie and Ito, Mizuko},
  date = {2022-03-01},
  journaltitle = {Review of Research in Education},
  shortjournal = {Review of Research in Education},
  volume = {46},
  number = {1},
  pages = {264--287},
  publisher = {{American Educational Research Association}},
  issn = {0091-732X},
  doi = {10.3102/0091732X221084322},
  url = {https://doi.org/10.3102/0091732X221084322},
  urldate = {2022-06-10},
  abstract = {This review brings together scholarship from creative educational experiences (CEE) and connected learning to describe a connected arts learning framework, reframing arts education in the 21st century with a focus on connecting youths? interest-driven art making to opportunities through supportive relationships. Such a framework pushes the arts education field to consider outcomes beyond artistic skill acquisition and academic achievement to include a broader range of opportunities, including those civic- and career-related; promote interest development through targeted exposure to new forms of art making; create and implement professional development and programming to emphasize networks and connections; and draw from culturally sustaining practices to bridge connections between spaces for learning. A connected learning lens applied to what we know about high quality arts education sharpens our focus on how CEE can cultivate equity and social/cultural connection for youth.},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ConnectedArtsLearningCultivatingEquityPeppler2022.pdf}
}

@inproceedings{ConnectingDotsRethinkingRelationshipCodeKaras2021,
  title = {Connecting the Dots: Rethinking the Relationship between Code and Prose Writing with Functional Connectivity},
  shorttitle = {Connecting the Dots},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Karas, Zachary and Jahn, Andrew and Weimer, Westley and Huang, Yu},
  date = {2021-08},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {767--779},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3468264.3468579},
  url = {https://doi.org/10.1145/3468264.3468579},
  urldate = {2021-10-21},
  abstract = {Medical imaging studies of software engineering have risen in popularity and may reveal the neural underpinnings of coding activities. To date, however, all studies in computer science venues have treated brain regions independently and in isolation. Since most complex neural activity involves coordination among multiple regions, previous analyses may overlook neural behavior. We propose to apply functional connectivity analysis to medical imaging data from software engineering tasks. Informally, this analysis treats the brain as a graph, rather than a series of independent modules, and statistically infers relevant edges. We present a functional connectivity analysis of existing data, which elucidates the interconnections between code writing and prose writing, especially regarding higher mathematics and semantic processing. First, we found a significant link between Broca’s Area (language) and the Number Form Area (higher mathematics) for coding. This both refines previous interpretations that code writing and natural language are distinct from each other, and may also contribute to the understanding of the Number Form Area in the Psychology literature. Second, we identify an area with important functional connectivity for both prose writing and coding, unlike previous analyses that associated it with coding. This advances our neural understanding of coding and prose writing, and was only exposed by using functional connectivity analysis. Third, for coding, we find a strong functional connectivity result for a brain region involved in semantic processing for language, with implications for CS training. Finally, we find a neural relationship between coding and expertise, including a more grounded explanation than prior work.},
  isbn = {978-1-4503-8562-6},
  keywords = {code writing,expertise,fMRI,functional connectivity},
  annotation = {0 citations (Crossref) [2022-06-09]}
}

@article{ConstructionismAIHistoryPossibleFuturesKahn2021,
  title = {Constructionism and {{AI}}: {{A}} History and Possible Futures},
  shorttitle = {Constructionism and {{AI}}},
  author = {Kahn, Ken and Winters, Niall},
  date = {2021},
  journaltitle = {British Journal of Educational Technology},
  volume = {52},
  number = {3},
  pages = {1130--1142},
  issn = {1467-8535},
  doi = {10.1111/bjet.13088},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13088},
  urldate = {2022-06-10},
  abstract = {Constructionism, long before it had a name, was intimately tied to the field of Artificial Intelligence. Soon after the birth of Logo at BBN, Seymour Papert set up the Logo Group as part of the MIT AI Lab. Logo was based upon Lisp, the first prominent AI programming language. Many early Logo activities involved natural language processing, robotics, artificial game players, and generating poetry, art, and music. In the 1970s researchers explored enhancements to Logo to support AI programming by children. In the 1980s the Prolog community, inspired by Logo's successes, began exploring how to adapt logic programming for use by school children. While there have been over 40 years of active AI research in creating intelligent tutoring systems, there was little AI-flavoured constructionism after the 1980s until about 2017 when suddenly a great deal of activity started. Amongst those activities were attempts to enhance Scratch, Snap!, and MIT App Inventor with new blocks for speech synthesis, speech recognition, image recognition, and the use of pre-trained deep learning models. The Snap! enhancements also include support for word embeddings, as well as blocks to enable learners to create, train, and use deep neural networks. Student and teacher project-oriented resources highlighting these new AI programming components appeared at the same time. In this paper, we review this history, providing a unique perspective on AI developments—both social and technical—from a constructionist perspective. Reflecting on these, we close with speculations about possible futures for AI and constructionism. Practitioner notes What is already known about this topic There exist excellent broad surveys of the current status of teaching machine learning in schools, for example Marques et al. (2020). There are historical collections of AI and education research papers that include descriptions of constructionist activities, for example Yazdani (1984). What this paper adds This paper adds an in-depth focus on historical and current efforts on AI and education that support constructionist teaching. This focus enables us to delve deeper than a broad survey. Uniquely, we provide a 50-year historical perspective on constructionist AI tools, trials, and research. Grounded in this history and our survey of current tools and projects, we provide speculations about future directions. Implications for practice and/or policy We hope our descriptions of current AI programming tools for non-experts placed in a broad historical context will be of use to teachers wishing to introduce AI to their students in a constructionist manner, as well as to developers and researchers aiming to support such teaching.},
  langid = {english},
  keywords = {artificial intelligence,constructionism,deep neural networks,logic programming,Logo,machine learning,ObsCite,project-based learning,Snap!,TensorFlow.js},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13088},
  file = {/Users/brookeryan/Documents/Obsidian/reference/ConstructionismAIHistoryPossibleFuturesKahn2021.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/ConstructionismAIHistoryPossibleFuturesKahn2021.pdf;/Users/brookeryan/Zotero/storage/GWVR5PJC/bjet.html}
}

@article{ConstructionismEthicsCreativityDevelopingPrimaryAli,
  title = {Constructionism, {{Ethics}}, and {{Creativity}}: {{Developing Primary}} and {{Middle School Artiﬁcial Intelligence Education}}},
  author = {Ali, Safinah and Payne, Blakeley H and Williams, Randi and Park, Hae Won and Breazeal, Cynthia},
  pages = {4},
  abstract = {Children growing up in the era of artificial intelligence (AI) will have a fundamentally different relationship with technology than those before them. As AI changes how we live, work, and play this raises the critical question, ”How do we best prepare students to flourish in the era of AI?” In order to create a future where a diverse and inclusive citizenry can participate in the development of the future of AI, we are developing powerful K12 AI education curricula that emphasize constructionist learning, designing with ethics in mind, and developing a creative mindset. Children will need all of these skills to thrive in the AI era. Here, we describe the tools we created and studies we conducted to build curricula that embody these core principles.},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ConstructionismEthicsCreativityDevelopingPrimaryAlia.pdf}
}

@article{CorpusInvestigationSyntacticEmbeddingPirahãFutrell2016a,
  title = {A {{Corpus Investigation}} of {{Syntactic Embedding}} in {{Pirahã}}},
  author = {Futrell, Richard and Stearns, Laura and Everett, Daniel L. and Piantadosi, Steven T. and Gibson, Edward},
  date = {2016-03-02},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {3},
  pages = {e0145289},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0145289},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0145289},
  urldate = {2022-08-10},
  abstract = {The Pirahã language has been at the center of recent debates in linguistics, in large part because it is claimed not to exhibit recursion, a purported universal of human language. Here, we present an analysis of a novel corpus of natural Pirahã speech that was originally collected by Dan Everett and Steve Sheldon. We make the corpus freely available for further research. In the corpus, Pirahã sentences have been shallowly parsed and given morpheme-aligned English translations. We use the corpus to investigate the formal complexity of Pirahã syntax by searching for evidence of syntactic embedding. In particular, we search for sentences which could be analyzed as containing center-embedding, sentential complements, adverbials, complementizers, embedded possessors, conjunction or disjunction. We do not find unambiguous evidence for recursive embedding of sentences or noun phrases in the corpus. We find that the corpus is plausibly consistent with an analysis of Pirahã as a regular language, although this is not the only plausible analysis.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/VI7PZBEE/Futrell et al. - 2016 - A Corpus Investigation of Syntactic Embedding in P.pdf;/Users/brookeryan/Zotero/storage/S76GEPU9/article.html}
}

@article{CreativeCodingProgrammingPersonalExpressionPeppler2005,
  title = {Creative Coding: {{Programming}} for Personal Expression},
  shorttitle = {Creative Coding},
  author = {Peppler, Kylie and Kafai, Yasmin},
  date = {2005},
  journaltitle = {Retrieved August},
  volume = {30},
  number = {2008},
  pages = {314},
  publisher = {{Citeseer}}
}

@unpublished{CreativityTextbasedGenerativeArtOppenlaender2022,
  title = {The {{Creativity}} of {{Text-based Generative Art}}},
  author = {Oppenlaender, Jonas},
  date = {2022-05-13},
  number = {arXiv:2206.02904},
  eprint = {2206.02904},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.02904},
  urldate = {2022-06-10},
  abstract = {Text-based generation of digital images has made a giant leap towards becoming a mainstream phenomenon. With text-based generative systems, anybody can create digital images and artworks. This raises the question of whether text-based generative art is creative. This paper expounds on the nature of human creativity involved in text-based generative art with a specific focus on the practice of prompt engineering, drawing on Rhodes's conceptual model of creativity. The paper critiques the current product-centered view of creativity which may fall short in the context of text-based generative art. An case exemplifying this shortcoming is provided and future opportunities for research on text-based generative art are outlined.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics,Computer Science - Human-Computer Interaction,H.5,H.m},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/CreativityTextbasedGenerativeArtOppenlaender2022.pdf;/Users/brookeryan/Zotero/storage/HQKAXWN5/2206.html}
}

@inproceedings{CurrentPracticesChallengesDesignImplicationsKrauß2021,
  title = {Current {{Practices}}, {{Challenges}}, and {{Design Implications}} for {{Collaborative AR}}/{{VR Application Development}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Krauß, Veronika and Boden, Alexander and Oppermann, Leif and Reiners, René},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445335},
  url = {https://doi.org/10.1145/3411764.3445335},
  urldate = {2022-06-09},
  abstract = {Augmented/Virtual Reality (AR/VR) is still a fragmented space to design for due to the rapidly evolving hardware, the interdisciplinarity of teams, and a lack of standards and best practices. We interviewed 26 professional AR/VR designers and developers to shed light on their tasks, approaches, tools, and challenges. Based on their work and the artifacts they generated, we found that AR/VR application creators fulfill four roles: concept developers, interaction designers, content authors, and technical developers. One person often incorporates multiple roles and faces a variety of challenges during the design process from the initial contextual analysis to the deployment. From analysis of their tool sets, methods, and artifacts, we describe critical key challenges. Finally, we discuss the importance of prototyping for the communication in AR/VR development teams and highlight design implications for future tools to create a more usable AR/VR tool chain.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,AR design,AR development,AR/VR,Augmented Reality,authoring tools,MR,notion,ObsCite,practitioners,systematic literature review,XR},
  file = {/Users/brookeryan/Zotero/storage/Z58NLGDA/Krauß et al_2021_Current Practices, Challenges, and Design Implications for Collaborative AR-VR.pdf}
}

@online{DALL,
  title = {{{DALL}}·{{E}} 2},
  url = {https://openai.com/dall-e-2/},
  urldate = {2022-05-12},
  abstract = {DALL·E 2 is a new AI system that can create realistic images and art from a description in natural language.},
  langid = {english},
  organization = {{OpenAI}},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Zotero/storage/QKSAUZIT/dall-e-2.html}
}

@article{DamageFrontoPolarCortexAssociatedImpairedDreher2008,
  title = {Damage to the {{Fronto-Polar Cortex Is Associated}} with {{Impaired Multitasking}}},
  author = {Dreher, Jean-Claude and Koechlin, Etienne and Tierney, Michael and Grafman, Jordan},
  date = {2008-09-16},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {3},
  number = {9},
  eprint = {18795100},
  eprinttype = {pmid},
  pages = {e3227},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003227},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2528949/},
  urldate = {2022-08-09},
  abstract = {Background A major question in understanding the functional organization of the brain is to delineate the functional divisions of the prefrontal cortex. Of particular importance to the cognitive capacities that are uniquely human is the fronto-polar cortex (Brodmann's area 10), which is disproportionally larger in humans relative to the rest of the brain than it is in the ape's brain. The specific function of this brain region remains poorly understood, but recent neuroimaging studies have proposed that it may hold goals in mind while exploring and processing secondary goals. Principal Findings Here we show that the extent of damage to the fronto-polar cortex predicts impairment in the management of multiple goals. This result reveals that the integrity of the fronto-polar cortex is necessary to perform tasks that require subjects to maintain a primary goal in mind while processing secondary goals, an ability which is crucial for complex human cognitive abilities. Conclusion/Significance These results provide important new insights concerning the cerebral basis of complex human cognition such as planning and multitasking.},
  pmcid = {PMC2528949},
  keywords = {Advisor Literature},
  file = {/Users/brookeryan/Zotero/storage/Y7VVHJ4U/Dreher et al. - 2008 - Damage to the Fronto-Polar Cortex Is Associated wi.pdf}
}

@incollection{DebuggingDatabaseQueriesSurveyToolsGathani2020a,
  title = {Debugging {{Database Queries}}: {{A Survey}} of {{Tools}}, {{Techniques}}, and {{Users}}},
  shorttitle = {Debugging {{Database Queries}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gathani, Sneha and Lim, Peter and Battle, Leilani},
  date = {2020-04-21},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3313831.3376485},
  urldate = {2022-06-09},
  abstract = {Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.},
  isbn = {978-1-4503-6708-0},
  keywords = {AI Education Project,debugging databases,empirical study,literature review,notion,survey,systematic literature review,visualization}
}

@online{DeepDreama,
  title = {{{DeepDream}}},
  keywords = {AI Generated Art,notion}
}

@article{DeepElseCriticalFrameworkAIGrba2022,
  title = {Deep {{Else}}: {{A Critical Framework}} for {{AI Art}}},
  shorttitle = {Deep {{Else}}},
  author = {Grba, Dejan},
  date = {2022-03},
  journaltitle = {Digital},
  volume = {2},
  number = {1},
  pages = {1--32},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2673-6470},
  doi = {10.3390/digital2010001},
  url = {https://www.mdpi.com/2673-6470/2/1/1},
  urldate = {2022-06-10},
  abstract = {From a small community of pioneering artists who experimented with artificial intelligence (AI) in the 1970s, AI art has expanded, gained visibility, and attained socio-cultural relevance since the second half of the 2010s. Its topics, methodologies, presentational formats, and implications are closely related to a range of disciplines engaged in the research and application of AI. In this paper, I present a comprehensive framework for the critical exploration of AI art. It comprises the context of AI art, its prominent poetic features, major issues, and possible directions. I address the poetic, expressive, and ethical layers of AI art practices within the context of contemporary art, AI research, and related disciplines. I focus on the works that exemplify poetic complexity and manifest the epistemic or political ambiguities indicative of a broader milieu of contemporary culture, AI science/technology, economy, and society. By comparing, acknowledging, and contextualizing both their accomplishments and shortcomings, I outline the prospective strategies to advance the field. The aim of this framework is to expand the existing critical discourse of AI art with new perspectives which can be used to examine the creative attributes of emerging practices and to assess their cultural significance and socio-political impact. It contributes to rethinking and redefining the art/science/technology critique in the age when the arts, together with science and technology, are becoming increasingly responsible for changing ecologies, shaping cultural values, and political normalization.},
  issue = {1},
  langid = {english},
  keywords = {AI art,anthropomorphism,artificial intelligence,creativity,deep learning,digital art,generative art,machine learning,mainstream contemporary art,new media art},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/DeepElseCriticalFrameworkAIGrba2022.pdf}
}

@online{DeepstyleNewDeepLearningAlgorithm5ives2015,
  type = {Reddit Post},
  title = {Deepstyle? {{New}} Deep Learning Algorithm Will Stylize Images Based on Another (Ex: {{Van Gogh}})},
  shorttitle = {Deepstyle?},
  author = {5ives},
  date = {2015-08-30T04:22:45},
  url = {www.reddit.com/r/deepdream/comments/3ix9ow/deepstyle_new_deep_learning_algorithm_will/},
  urldate = {2022-05-12},
  organization = {{r/deepdream}},
  file = {/Users/brookeryan/Zotero/storage/VXIWGHUE/deepstyle_new_deep_learning_algorithm_will.html}
}

@article{DependencyLocalityExplanatoryPrincipleWordFutrell2020,
  title = {Dependency Locality as an Explanatory Principle for Word Order},
  author = {Futrell, Richard and Levy, Roger P. and Gibson, Edward},
  date = {2020},
  journaltitle = {Language},
  volume = {96},
  number = {2},
  pages = {371--412},
  publisher = {{Linguistic Society of America}},
  issn = {1535-0665},
  doi = {10.1353/lan.2020.0024},
  url = {https://muse.jhu.edu/article/757632},
  urldate = {2022-08-13},
  abstract = {This work focuses on explaining both grammatical universals of word order and quantitative word-order preferences in usage by means of a simple efficiency principle: dependency locality. In its simplest form, dependency locality holds that words linked in a syntactic dependency (any head–dependent relationship) should be close in linear order. We give large-scale corpus evidence that dependency locality predicts word order in both grammar and usage, beyond what would be expected from independently motivated principles, and demonstrate a means for dissociating grammar and usage in corpus studies. Finally, we discuss previously undocumented variation in dependency length and how it correlates with other linguistic features such as head direction, providing a rich set of explananda for future linguistic theories.},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/P542TN5W/Futrell et al. - 2020 - Dependency locality as an explanatory principle fo.pdf}
}

@inproceedings{DesignGuidelinesPromptEngineeringTexttoImageLiu2022,
  title = {Design {{Guidelines}} for {{Prompt Engineering Text-to-Image Generative Models}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Liu, Vivian and Chilton, Lydia B},
  date = {2022-04-29},
  pages = {1--23},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3501825},
  url = {https://dl.acm.org/doi/10.1145/3491102.3501825},
  urldate = {2022-06-10},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  keywords = {AI co-creation,computational creativity,design guidelines,multimodal generative models,ObsCite,prompt engineering.,text-to-image},
  file = {/Users/brookeryan/Documents/Obsidian/reference/DesignGuidelinesPromptEngineeringTexttoImageLiu2022a.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/DesignGuidelinesPromptEngineeringTexttoImageLiu2022a.pdf;/Users/brookeryan/Zotero/storage/BJRRUCQY/Liu and Chilton - 2022 - Design Guidelines for Prompt Engineering Text-to-I.pdf}
}

@inproceedings{DesigningEffectiveInterviewChatbotsAutomaticHan2021,
  title = {Designing {{Effective Interview Chatbots}}: {{Automatic Chatbot Profiling}} and {{Design Suggestion Generation}} for {{Chatbot Debugging}}},
  shorttitle = {Designing {{Effective Interview Chatbots}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Han, Xu and Zhou, Michelle and Turner, Matthew J. and Yeh, Tom},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445569},
  url = {https://doi.org/10.1145/3411764.3445569},
  urldate = {2022-06-09},
  abstract = {Recent studies show the effectiveness of interview chatbots for information elicitation. However, designing an effective interview chatbot is non-trivial. Few tools exist to help designers design, evaluate, and improve an interview chatbot iteratively. Based on a formative study and literature reviews, we propose a computational framework for quantifying the performance of interview chatbots. Incorporating the framework, we have developed iChatProfile, an assistive chatbot design tool that can automatically generate a profile of an interview chatbot with quantified performance metrics and offer design suggestions for improving the chatbot based on such metrics. To validate the effectiveness of iChatProfile, we designed and conducted a between-subject study that compared the performance of 10 interview chatbots designed with or without using iChatProfile. Based on the live chats between the 10 chatbots and 1349 users, our results show that iChatProfile helped the designers build significantly more effective interview chatbots, improving both interview quality and user experience.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,Automatic Chatbot Evaluation,Automatic Chatbot Profiling,Chatbot Debugging,Chatbot Design Suggestion,Chatbot Evaluation Framework,Conversational AI Agents,Interview Chatbot,notion,systematic literature review},
  file = {/Users/brookeryan/Documents/Obsidian/reference/DesigningEffectiveInterviewChatbotsAutomaticHan2021b.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/DesigningEffectiveInterviewChatbotsAutomaticHan2021b.pdf}
}

@inproceedings{DesigningInteractiveTransferLearningToolsMishra2021,
  title = {Designing {{Interactive Transfer Learning Tools}} for {{ML Non-Experts}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mishra, Swati and Rzeszotarski, Jeffrey M},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445096},
  url = {https://doi.org/10.1145/3411764.3445096},
  urldate = {2022-06-09},
  abstract = {Interactive machine learning (iML) tools help to make ML accessible to users with limited ML expertise. However, gathering necessary training data and expertise for model-building remains challenging. Transfer learning, a process where learned representations from a model trained on potentially terabytes of data can be transferred to a new, related task, offers the possibility of providing ”building blocks” for non-expert users to quickly and effectively apply ML in their work. However, transfer learning largely remains an expert tool due to its high complexity. In this paper, we design a prototype to understand non-expert user behavior in an interactive environment that supports transfer learning. Our findings reveal a series of data- and perception-driven decision-making strategies non-expert users employ, to (in)effectively transfer elements using their domain expertise. Finally, we synthesize design implications which might inform future interactive transfer learning environments.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,Interactive Machine Learning,notion,Prototyping,systematic literature review,Transfer Learning,User Study}
}

@incollection{DesigningVoiceInterfacesBackCurriculumMurad2020,
  title = {Designing {{Voice Interfaces}}: {{Back}} to the ({{Curriculum}}) {{Basics}}},
  shorttitle = {Designing {{Voice Interfaces}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Murad, Christine and Munteanu, Cosmin},
  date = {2020-04-21},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3313831.3376522},
  urldate = {2022-06-09},
  abstract = {Voice user interfaces (VUIs) are rapidly increasing in popularity in the consumer space. This leads to a concurrent explosion of available applications for such devices, with many industries rushing to offer voice interactions for their products. This pressure is then transferred to interface designers; however, a large majority of designers have been only trained to handle the usability challenges specific to Graphical User Interfaces (GUIs). Since VUIs differ significantly in design and usability from GUIs, we investigate in this paper the extent to which current educational resources prepare designers to handle the specific challenges of VUI design. For this, we conducted a preliminary scoping scan and syllabi meta review of HCI curricula at more than twenty top international HCI departments, revealing that the current offering of VUI design training within HCI education is rather limited. Based on this, we advocate for the updating of HCI curricula to incorporate VUI design, and for the development of VUI-specific pedagogical artifacts to be included in new curricula.},
  isbn = {978-1-4503-6708-0},
  keywords = {AI Education Project,conversational interface,hci curriculum,hci education,notion,ObsCite,speech,systematic literature review,voice user interface,vui design},
  file = {/Users/brookeryan/Documents/Obsidian/reference/DesigningVoiceInterfacesBackCurriculumMurad2020.md}
}

@book{DesignPatternsElementsReusableObjectOrientedGamma1995,
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  shorttitle = {Design Patterns},
  author = {Gamma, Erich and Johnson, Ralph and Helm, Richard and Johnson, Ralph E. and Vlissides, John},
  date = {1995},
  eprint = {tmNNfSkfTlcC},
  eprinttype = {googlebooks},
  publisher = {{Pearson Deutschland GmbH}},
  abstract = {Capturing a wealth of experience about the design of object-oriented software, four top-notch designers present a catalog of simple and succinct solutions to commonly occurring design problems. Previously undocumented, these 23 patterns allow designers to create more flexible, elegant, and ultimately reusable designs without having to rediscover the design solutions themselves. The authors begin by describing what patterns are and how they can help you design object-oriented software. They then go on to systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. With Design Patterns as your guide, you will learn how these important patterns fit into the software development process, and how you can leverage them to solve your own design problems most efficiently. Each pattern describes the circumstances in which it is applicable, when it can be applied in view of other design constraints, and the consequences and trade-offs of using the pattern within a larger design. All patterns are compiled from real systems and are based on real-world examples. Each pattern also includes code that demonstrates how it may be implemented in object-oriented programming languages like C++ or Smalltalk. 0201633612B07092001},
  isbn = {978-3-8273-3043-7},
  langid = {ngerman},
  pagetotal = {512},
  keywords = {Computers / Programming / Object Oriented}
}

@inproceedings{DevelopingMiddleSchoolStudentsAILee2021a,
  title = {Developing {{Middle School Students}}' {{AI Literacy}}},
  booktitle = {Proceedings of the 52nd {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Lee, Irene and Ali, Safinah and Zhang, Helen and DiPaola, Daniella and Breazeal, Cynthia},
  date = {2021-03-03},
  pages = {191--197},
  publisher = {{ACM}},
  location = {{Virtual Event USA}},
  doi = {10.1145/3408877.3432513},
  url = {https://dl.acm.org/doi/10.1145/3408877.3432513},
  urldate = {2022-04-25},
  abstract = {In this experience report, we describe an AI summer workshop designed to prepare middle school students to become informed citizens and critical consumers of AI technology and to develop their foundational knowledge and skills to support future endeavors as AI-empowered workers. The workshop featured the 30-hour “Developing AI Literacy” or DAILy curriculum that is grounded in literature on child development, ethics education, and career development. The participants in the workshop were students between the ages of 10 and 14; 87\% were from underrepresented groups in STEM and Computing. In this paper we describe the online curriculum, its implementation during synchronous online workshop sessions in summer of 2020, and preliminary findings on student outcomes. We reflect on the successes and lessons we learned in terms of supporting students’ engagement and conceptual learning of AI, shifting attitudes toward AI, and fostering conceptions of future selves as AI-enabled workers. We conclude with discussions of the affordances and barriers to bringing AI education to students from underrepresented groups in STEM and Computing.},
  eventtitle = {{{SIGCSE}} '21: {{The}} 52nd {{ACM Technical Symposium}} on {{Computer Science Education}}},
  isbn = {978-1-4503-8062-1},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/DevelopingMiddleSchoolStudentsAILee2021a.pdf}
}

@online{DiscoDiffusiona,
  title = {{{DiscoDiffusion}}},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/DiscoDiffusiona.md}
}

@online{DiscoDiffusionCheatsheetElisoGenerative,
  title = {Disco {{Diffusion Cheatsheet}} – {{Eliso}}'s {{Generative Art Guides}}},
  url = {https://botbox.dev/disco-diffusion-cheatsheet/},
  urldate = {2022-06-10},
  langid = {american},
  keywords = {AI Generated Art,notion}
}

@article{DislikeJustItHowDevelopersMasood2022,
  title = {Like, Dislike, or Just Do It? {{How}} Developers Approach Software Development Tasks},
  shorttitle = {Like, Dislike, or Just Do It?},
  author = {Masood, Zainab and Hoda, Rashina and Blincoe, Kelly and Damian, Daniela},
  date = {2022-10-01},
  journaltitle = {Information and Software Technology},
  shortjournal = {Information and Software Technology},
  volume = {150},
  pages = {106963},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.106963},
  url = {https://www.sciencedirect.com/science/article/pii/S0950584922001045},
  urldate = {2022-06-10},
  abstract = {Context: Software developers work on various tasks and activities that contribute towards creating and maintaining software applications, frameworks, or other software components. These include technical (e.g., writing code and fixing bugs) and non-technical activities (e.g., communicating within or outside teams to understand, clarify, and resolve issues) as part of their day-to-day responsibilities. Interestingly, there is an aspect of desirability associated with these tasks and activities. Objective: However, not all of these tasks are desirable to developers, and yet they still need to be done. This study explores desirability and undesirability of developers for software development tasks. Method: Based on semi-structured interviews from 32 software developers and applying a grounded theory research approach, the study investigates what tasks are desirable and undesirable for developers, what makes tasks desirable and undesirable for them, what are the perceived consequences of working on these tasks, and how do they deal with such tasks. Results: We identified a set of underlying factors that make tasks (un)desirable for developers, categorised as personal, social, organisational, technical, and operational factors. We also found that working on desirable tasks has positive consequences while working on undesirable tasks has negative consequences. We reported different standard, assisted, and mitigation strategies that aid software practitioners manage developers’ likes and dislikes. Conclusion: Understanding these likes and dislikes, contributing factors, and strategies can help the managers and teams ensure balanced work distribution, developers’ happiness, and productivity, ultimately increasing the value developers add to software products.},
  langid = {english},
  keywords = {Contributing factors,Desirability,Software tasks,Undesirability},
  file = {/Users/brookeryan/Zotero/storage/DJ9LIUPE/S0950584922001045.html}
}

@article{DragonFruitStrawberryPearZee2004,
  title = {({{Dragon Fruit}}, {{Strawberry Pear}})},
  author = {Zee, Francis and Yen, Chung-Ruey and Nishina, Melvin},
  date = {2004},
  number = {9},
  pages = {3},
  langid = {english},
  keywords = {Dragon Fruit,notion},
  file = {/Users/brookeryan/Zotero/storage/QFR4DACL/Zee et al. - 2004 - (Dragon Fruit, Strawberry Pear).pdf}
}

@online{DreamingNeuralNets,
  title = {Dreaming {{Neural Nets}}},
  url = {https://www.reddit.com/r/deepdream/},
  urldate = {2022-05-12},
  file = {/Users/brookeryan/Zotero/storage/DH47AE5Z/deepdream.html}
}

@article{EfficientlyAddingOurSensoryEvidencePeters2020,
  title = {Efficiently Adding up Our Sensory Evidence},
  author = {Peters, Megan A. K.},
  date = {2020-08},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {4},
  number = {8},
  pages = {778--779},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0857-2},
  url = {http://www.nature.com/articles/s41562-020-0857-2},
  urldate = {2022-07-22},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/MSMXN7PK/Peters - 2020 - Efficiently adding up our sensory evidence.pdf}
}

@article{EfﬁcientPredictionTraitJudgmentsFacesKeles,
  title = {Efficient Prediction of Trait Judgments from Faces Using Deep Neural Networks},
  author = {Keles, Umit and Lin, Chujun and Adolphs, Ralph},
  pages = {17},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/LKWZJR9H/Keles et al. - Efﬁcient prediction of trait judgments from faces .pdf}
}

@article{EmergenceHumanConsciousnessFetalNeonatalLagercrantz2009,
  title = {The {{Emergence}} of {{Human Consciousness}}: {{From Fetal}} to {{Neonatal Life}}},
  shorttitle = {The {{Emergence}} of {{Human Consciousness}}},
  author = {Lagercrantz, Hugo and Changeux, Jean-Pierre},
  date = {2009-03},
  journaltitle = {Pediatric Research},
  shortjournal = {Pediatr Res},
  volume = {65},
  number = {3},
  pages = {255--260},
  publisher = {{Nature Publishing Group}},
  issn = {1530-0447},
  doi = {10.1203/PDR.0b013e3181973b0d},
  url = {https://www.nature.com/articles/pr200950},
  urldate = {2022-05-18},
  abstract = {A simple definition of consciousness is sensory awareness of the body, the self, and the world. The fetus may be aware of the body, for example by perceiving pain. It reacts to touch, smell, and sound, and shows facial expressions responding to external stimuli. However, these reactions are probably preprogrammed and have a subcortical nonconscious origin. Furthermore, the fetus is almost continuously asleep and unconscious partially due to endogenous sedation. Conversely, the newborn infant can be awake, exhibit sensory awareness, and process memorized mental representations. It is also able to differentiate between self and nonself touch, express emotions, and show signs of shared feelings. Yet, it is unreflective, present oriented, and makes little reference to concept of him/herself. Newborn infants display features characteristic of what may be referred to as basic consciousness and they still have to undergo considerable maturation to reach the level of adult consciousness. The preterm infant, ex utero, may open its eyes and establish minimal eye contact with its mother. It also shows avoidance reactions to harmful stimuli. However, the thalamocortical connections are not yet fully established, which is why it can only reach a minimal level of consciousness.},
  issue = {3},
  langid = {english},
  keywords = {COGS 269,general,Medicine/Public Health,notion,Pediatric Surgery,Pediatrics},
  annotation = {120 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/PT4HDDPR/Lagercrantz and Changeux - 2009 - The Emergence of Human Consciousness From Fetal t.pdf;/Users/brookeryan/Zotero/storage/2HYPKDAQ/pr200950.html}
}

@article{EmergingArtificialIntelligenceArtPedagogyLeonard2021,
  title = {Emerging {{Artificial Intelligence}}, {{Art}} and {{Pedagogy}}: {{Exploring Discussions}} of {{Creative Algorithms}} and {{Machines}} for {{Art Education}}},
  shorttitle = {Emerging {{Artificial Intelligence}}, {{Art}} and {{Pedagogy}}},
  author = {Leonard, Nicholas},
  date = {2021-02-19},
  journaltitle = {Digital Culture \& Education},
  shortjournal = {Digital Culture \& Education},
  volume = {13},
  abstract = {The continued development and emergence of creative machines and computational creativity provokes certain questions that audit ontological and epistemological assumptions. Creative artificial intelligence challenges computer scientists, digital artists, and art educators to clarify or reconceptualize their notions of cognition and creativity. The article starts by addressing the increase in AI algorithms in both daily life and formal education settings to begin highlighting the shared investment across domains. The focus is then narrowed down to highlight creative machines and digital artmaking. By exploring the statements and artworks from computer scientists and digital artists, correlations to art education pedagogical approaches are then constructed. This will then lead into a recognition for a need to challenge and examine the ontological and epistemological assumptions present in art education. Finally, a new material theoretical framework for digital art education pedagogy is proposed to reorient discussions to ask new questions regarding increasingly creative machines and the experiences and education of students in the visual arts},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/JCDDTYQ8/Leonard - 2021 - Emerging Artificial Intelligence, Art and Pedagogy.pdf}
}

@inproceedings{EmpiricalEvaluationTarantulaAutomaticFaultlocalizationJones2005,
  title = {Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM}} International {{Conference}} on {{Automated}} Software Engineering},
  author = {Jones, James A. and Harrold, Mary Jean},
  date = {2005-11},
  series = {{{ASE}} '05},
  pages = {273--282},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1101908.1101949},
  url = {https://doi.org/10.1145/1101908.1101949},
  urldate = {2021-10-22},
  abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study—it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
  isbn = {978-1-58113-993-8},
  keywords = {automated debugging,empirical study,fault localization,program analysis},
  annotation = {602 citations (Crossref) [2022-06-09]}
}

@misc{EndotaxisUniversalAlgorithmMappingGoalLearningZhang2021,
  title = {Endotaxis: {{A Universal Algorithm}} for {{Mapping}}, {{Goal-Learning}}, and {{Navigation}}},
  shorttitle = {Endotaxis},
  author = {Zhang, Tony and Rosenberg, Matthew and Perona, Pietro and Meister, Markus},
  date = {2021-09-25},
  pages = {2021.09.24.461751},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.09.24.461751},
  url = {https://www.biorxiv.org/content/10.1101/2021.09.24.461751v1},
  urldate = {2022-07-25},
  abstract = {An animal entering a new environment typically faces three challenges: explore the space for resources, memorize their locations, and navigate towards those targets as needed. Experimental work on exploration, mapping, and navigation has mostly focused on simple environments – such as an open arena, a pond [1], or a desert [2] – and much has been learned about neural signals in diverse brain areas under these conditions [3, 4]. However, many natural environments are highly constrained, such as a system of burrows, or of paths through the underbrush. More generally, many cognitive tasks are equally constrained, allowing only a small set of actions at any given stage in the process. Here we propose an algorithm that learns the structure of an arbitrary environment, discovers useful targets during exploration, and navigates back to those targets by the shortest path. It makes use of a behavioral module common to all motile animals, namely the ability to follow an odor to its source [5]. We show how the brain can learn to generate internal “virtual odors” that guide the animal to any location of interest. This endotaxis algorithm can be implemented with a simple 3-layer neural circuit using only biologically realistic structures and learning rules. Several neural components of this scheme are found in brains from insects to humans. Nature may have evolved a general mechanism for search and navigation on the ancient backbone of chemotaxis.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/ZD89NV9Q/Zhang et al. - 2021 - Endotaxis A Universal Algorithm for Mapping, Goal.pdf;/Users/brookeryan/Zotero/storage/CCURQ3X2/2021.09.24.html}
}

@inproceedings{EngagingComputerScienceWhenSolvingCharlton2019,
  title = {Engaging with Computer Science When Solving Tangible Problems},
  booktitle = {Proceedings of the 3rd {{Conference}} on {{Computing Education Practice}}},
  author = {Charlton, Patricia and Poslad, Stefan},
  date = {2019-01-09},
  series = {{{CEP}} '19},
  pages = {1--4},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3294016.3294026},
  url = {https://doi.org/10.1145/3294016.3294026},
  urldate = {2022-06-09},
  abstract = {This research investigates part of the challenge of widening participation and inclusion for teaching and learning about CS that the Institute of Coding plans to address. This research reports on working with a large number of schools, researchers and academics both formally and informally and across a wide age range and ability. The findings from a number of studies reflects important pedagogical theory, design and practice of teaching and learning about the computer science and engineering through tangible learning context. These findings and observations are examined in the light of these teaching and learning experiences and especially the observation of development of resilience in students learning and engagement in challenging areas of study.},
  isbn = {978-1-4503-6631-1},
  keywords = {Computer Science,Education,Learning about Internet of Things,Pedagogy,Resilience,Tangible Problem Solving},
  file = {/Users/brookeryan/Zotero/storage/UETNR8L4/Charlton and Poslad - 2019 - Engaging with computer science when solving tangib.pdf}
}

@inproceedings{EngagingTeachersCoDesignIntegratedAILin2021,
  title = {Engaging {{Teachers}} to {{Co-Design Integrated AI Curriculum}} for {{K-12 Classrooms}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lin, Phoebe and Van Brummelen, Jessica},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445377},
  url = {https://doi.org/10.1145/3411764.3445377},
  urldate = {2022-08-29},
  abstract = {Artificial Intelligence (AI) education is an increasingly popular topic area for K-12 teachers. However, little research has investigated how AI curriculum and tools can be designed to be more accessible to all teachers and learners. In this study, we take a Value-Sensitive Design approach to understanding the role of teacher values in the design of AI curriculum and tools, and identifying opportunities to integrate AI into core curriculum to leverage learners’ interests. We organized co-design workshops with 15 K-12 teachers, where teachers and researchers co-created lesson plans using AI tools and embedding AI concepts into various core subjects. We found that K-12 teachers need additional scaffolding in AI tools and curriculum to facilitate ethics and data discussions, and value supports for learner evaluation and engagement, peer-to-peer collaboration, and critical reflection. We present an exemplar lesson plan that shows entry points for teaching AI in non-computing subjects and reflect on co-designing with K-12 teachers in a remote setting.},
  isbn = {978-1-4503-8096-6},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/P9INK72B/Lin and Van Brummelen - 2021 - Engaging Teachers to Co-Design Integrated AI Curri.pdf}
}

@article{EnvisioningAIK12WhatShouldTouretzky2019b,
  title = {Envisioning {{AI}} for {{K-12}}: {{What Should Every Child Know}} about {{AI}}?},
  shorttitle = {Envisioning {{AI}} for {{K-12}}},
  author = {Touretzky, David and Gardner-McCune, Christina and Martin, Fred and Seehorn, Deborah},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {9795--9799},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33019795},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/5053},
  urldate = {2022-06-09},
  abstract = {The ubiquity of AI in society means the time is ripe to consider what educated 21st century digital citizens should know about this subject. In May 2018, the Association for the Advancement of Artificial Intelligence (AAAI) and the Computer Science Teachers Association (CSTA) formed a joint working group to develop national guidelines for teaching AI to K-12 students. Inspired by CSTA's national standards for K-12 computing education, the AI for K-12 guidelines will define what students in each grade band should know about artificial intelligence, machine learning, and robotics. The AI for K-12 working group is also creating an online resource directory where teachers can find AI- related videos, demos, software, and activity descriptions they can incorporate into their lesson plans. This blue sky talk invites the AI research community to reflect on the big ideas in AI that every K-12 student should know, and how we should communicate with the public about advances in AI and their future impact on society. It is a call to action for more AI researchers to become AI educators, creating resources that help teachers and students understand our work.}
}

@online{EpiphyllumOxypetalumCareGrowingNightBloomingGoldwyn2020,
  title = {Epiphyllum {{Oxypetalum Care}}: {{Growing Night-Blooming Cereus}}},
  shorttitle = {Epiphyllum {{Oxypetalum Care}}},
  author = {Goldwyn, Brittany},
  date = {2020-07-09T06:00:00+00:00},
  url = {https://www.bybrittanygoldwyn.com/epiphyllum-oxypetalum-care/},
  urldate = {2022-07-25},
  abstract = {I'm sharing epiphyllum oxypetalum care tips, including how to help your night-blooming cereus thrive and maybe even flower!},
  langid = {american},
  organization = {{By Brittany Goldwyn | Live Creatively}},
  keywords = {Epiphyllum oxypetalum,notion}
}

@online{EpiphyllumOxypetalumCareSecretsQueenRaffaele2019,
  title = {Epiphyllum {{Oxypetalum Care}} - {{Secrets}} of {{Queen}} of the {{Night}}},
  author = {Raffaele, Author},
  date = {2019-02-02T20:34:45+00:00},
  url = {https://www.ohiotropics.com/2019/02/02/epiphyllum-oxypetalum-secrets-of-queen-of-the-night/},
  urldate = {2022-07-25},
  abstract = {Detailed How-To care guide for Epiphyllum oxypetalum. Learn the proper care and soil mix, as well as how to make it flower, and how to easily propagate.},
  langid = {american},
  organization = {{Houseplant Care Tips}},
  keywords = {Epiphyllum oxypetalum,notion},
  file = {/Users/brookeryan/Zotero/storage/WVNEWVJR/epiphyllum-oxypetalum-secrets-of-queen-of-the-night.html}
}

@online{EpiphyllumOxypetalumQueenNightCareSlim2019,
  title = {Epiphyllum {{Oxypetalum}} '{{Queen Of The Night}}' {{Care Guide}}},
  author = {Slim, Jenn},
  date = {2019-05-24T20:48:59+00:00},
  url = {https://succulentplantcare.com/epiphyllum-oxypetalum-queen-of-the-night-care-guide/},
  urldate = {2022-07-25},
  abstract = {Epiphyllum Oxypetalum ‘Queen Of The Night’ is one of the most largely cultivated epiphyllum species and is known for its fragrant, white flowers that bloom only at night.},
  langid = {american},
  organization = {{Succulent Plant Care}},
  keywords = {Epiphyllum oxypetalum,notion},
  file = {/Users/brookeryan/Zotero/storage/AAQJH8HR/epiphyllum-oxypetalum-queen-of-the-night-care-guide.html}
}

@online{Epiphyllumoxypetalumsexualreproductionintroduction,
  title = {Epiphyllum-Oxypetalum-Sexual-Reproduction-Introduction},
  url = {https://cactus-epiphytes.eu/z_page_epiphyllum_oxypetalum_reproduction_00.htm},
  urldate = {2022-07-25},
  keywords = {Epiphyllum oxypetalum,notion},
  file = {/Users/brookeryan/Zotero/storage/KFL8NC36/z_page_epiphyllum_oxypetalum_reproduction_00.html}
}

@article{EstimatingSuccessReidentificationsIncompleteDatasetsRocher2019,
  title = {Estimating the Success of Re-Identifications in Incomplete Datasets Using Generative Models},
  author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
  options = {useprefix=true},
  date = {2019-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {3069},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10933-3},
  url = {http://www.nature.com/articles/s41467-019-10933-3},
  urldate = {2022-04-26},
  langid = {english},
  annotation = {220 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/STCVBAA7/Rocher et al. - 2019 - Estimating the success of re-identifications in in.pdf}
}

@article{EventSegmentationRevealsWorkingMemoryJafarpour2022,
  title = {Event Segmentation Reveals Working Memory Forgetting Rate},
  author = {Jafarpour, Anna and Buffalo, Elizabeth A. and Knight, Robert T. and Collins, Anne G. E.},
  date = {2022-03-18},
  journaltitle = {iScience},
  shortjournal = {iScience},
  volume = {25},
  number = {3},
  pages = {103902},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2022.103902},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004222001729},
  urldate = {2022-08-13},
  abstract = {We encounter the world as a continuous flow and effortlessly segment sequences of events into episodes. This process of event segmentation engages working memory (WM) for tracking the flow of events and impacts subsequent memory accuracy. WM is limited in how much information (i.e., WM capacity) and for how long the information is retained (i.e., forgetting rate). In this study, across multiple tasks, we estimated participants’ WM capacity and forgetting rate in a dynamic context and evaluated their relationship to event segmentation. A U-shaped relationship across tasks shows that individuals who segmented the movie more finely or coarsely than the average have a faster WM forgetting rate. A separate task assessing long-term memory retrieval revealed that the coarse-segmenters have better recognition of temporal order of events compared to the fine-segmenters. These findings show that event segmentation employs dissociable memory strategies and correlates with how long information is retained in WM},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/63JASIZZ/Jafarpour et al. - 2022 - Event segmentation reveals working memory forgetti.pdf}
}

@article{ExploratoryStudyProgramComprehensionStrategiesCorritore2001,
  title = {An Exploratory Study of Program Comprehension Strategies of Procedural and Object-Oriented Programmers},
  author = {Corritore, Cynthia L. and Wiedenbeck, Susan},
  date = {2001-01},
  journaltitle = {International Journal of Human-Computer Studies},
  shortjournal = {International Journal of Human-Computer Studies},
  volume = {54},
  number = {1},
  pages = {1--23},
  issn = {10715819},
  doi = {10.1006/ijhc.2000.0423},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581900904233},
  urldate = {2021-10-21},
  langid = {english},
  annotation = {25 citations (Crossref) [2022-06-09]}
}

@inproceedings{ExploringMachineTeachingChildrenDwivedi2021a,
  title = {Exploring {{Machine Teaching}} with {{Children}}},
  booktitle = {2021 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Dwivedi, Utkarsh and Gandhi, Jaina and Parikh, Raj and Coenraad, Merijke and Bonsignore, Elizabeth and Kacorri, Hernisa},
  date = {2021-10},
  pages = {1--11},
  issn = {1943-6106},
  doi = {10.1109/VL/HCC51201.2021.9576171},
  abstract = {Iteratively building and testing machine learning models can help children develop creativity, flexibility, and comfort with machine learning and artificial intelligence. We explore how children use machine teaching interfaces with a team of 14 children (aged 7–13 years) and adult co-designers. Children trained image classifiers and tested each other's models for robustness. Our study illuminates how children reason about ML concepts, offering these insights for designing machine teaching experiences for children: (i) ML metrics (e.g. confidence scores) should be visible for experimentation; (ii) ML activities should enable children to exchange models for promoting reflection and pattern recognition; and (iii) the interface should allow quick data inspection (e.g. images vs. gestures).},
  eventtitle = {2021 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  keywords = {AI education,child-computer interaction,Education,informal learning,Inspection,machine learning,Machine learning,machine teaching,Measurement,Reflection,Robustness,Visualization},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ExploringMachineTeachingChildrenDwivedi2021a.pdf;/Users/brookeryan/Zotero/storage/SFJ38TSW/9576171.html}
}

@incollection{ExploringWhyUnderrepresentedStudentsAreBarretto2021a,
  title = {Exploring {{Why Underrepresented Students Are Less Likely}} to {{Study Machine Learning}} and {{Artificial Intelligence}}},
  booktitle = {Proceedings of the 26th {{ACM Conference}} on {{Innovation}} and {{Technology}} in {{Computer Science Education V}}. 1},
  author = {Barretto, Daphne and LaChance, Julienne and Burton, Emanuelle and Liao, Soohyun Nam},
  date = {2021-06-26},
  pages = {457--463},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3430665.3456332},
  urldate = {2022-04-25},
  abstract = {There is little research on why underrepresented minorities are less likely to specifically study Machine Learning and Artificial Intelligence (ML/AI). We surveyed 159 undergraduate students about their interest in, exposure to, and personal views on ML/AI in order to explore variations in responses by self-reported gender and race/ethnicity groups. We found that students underrepresented by race/ethnicity are \textasciitilde 6 times less likely to take a traditional ML/AI course than those not underrepresented by race/ethnicity, but no significant difference was found between gender representation. Additionally, students underrepresented by race/ethnicity are more likely to report interest in social, cultural, and political impacts of ML/AI rather than the more technical aspects of ML/AI itself, which is a prevalent interest of students not underrepresented by race/ethnicity. We explore potential reasoning for this difference through further analysis of their survey responses. Encouragingly, we find that regardless of representational status 72.0\% of students who report lack of interest in a traditional introductory course are interested in a ML/AI course that focuses more on the political, philosophical, and ethical issues raised by ML/AI and its impacts on society. Our findings suggest that a 'CS Principles" style introductory ML/AI course, emphasizing social and political impacts, could be an effective way to promote diversity in ML/AI.},
  isbn = {978-1-4503-8214-4},
  keywords = {artificial intelligence,diversity,machine learning,race and ethnicity,underrepresented students},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ExploringWhyUnderrepresentedStudentsAreBarretto2021a.pdf}
}

@article{ExpressiveAIHybridArtScienceMateas2001,
  title = {Expressive {{AI}}: {{A Hybrid Art}} and {{Science Practice}}},
  shorttitle = {Expressive {{AI}}},
  author = {Mateas, Michael},
  date = {2001-04},
  journaltitle = {Leonardo},
  shortjournal = {Leonardo},
  volume = {34},
  number = {2},
  pages = {147--153},
  issn = {0024-094X, 1530-9282},
  doi = {10.1162/002409401750184717},
  url = {https://direct.mit.edu/leon/article/34/2/147-153/44007},
  urldate = {2022-06-10},
  abstract = {Expressive AI is a new interdiscipline of AI-based cultural production combining art practice and AI research practice. This paper explores the notion of expressive AI by comparing it with other AI discourses, describing how it borrows notions of interpretation and authorship from both art and AI research practice, and by providing preliminary desiderata for the practice.},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/brookeryan/Documents/Obsidian/reference/ExpressiveAIHybridArtScienceMateas2001.md;/Users/brookeryan/Zotero/storage/JAZMFNZ2/Mateas - 2001 - Expressive AI A Hybrid Art and Science Practice.pdf}
}

@inproceedings{ExpressivityInteractionFrameworkDesignBruns2021,
  title = {Expressivity in {{Interaction}}: A {{Framework}} for {{Design}}},
  shorttitle = {Expressivity in {{Interaction}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Bruns, Miguel and Ossevoort, Stijn and Petersen, Marianne Graves},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445231},
  url = {https://doi.org/10.1145/3411764.3445231},
  urldate = {2022-06-09},
  abstract = {Expressivity is frequently recurring as a term in HCI, but it is often approached from different perspectives. Affective computing prompted research into emotional expressivity, and with technology becoming more ubiquitous and tangible, the opportunities for expressive behaviors towards systems as well as the expressivity of systems increases. By analyzing exemplar research-through-design cases and a literature survey on the use of expressivity in interaction, we discuss how different perspectives and concepts contribute to understand expressivity in interaction. We integrate these perspectives and make them operational for interaction design by creating a framework including design considerations such as freedom of interaction, action-perception loops, multimodality, subtlety, ambiguity, skill development and temporal form. The framework is a result of a mixed-method approach including a review of existing definitions and scholarly artefacts, and a systematic literature review to identify design cases including an analysis of these design cases. We finally illustrate how the framework has been used to inform the design of a shape-changing soft-robotic interface. As a result, we contribute an integrated framework on how to design for expressivity in interaction.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,Design Case,Expressivity in Interaction,Framework,notion,ObsCite,Research-through-Design,systematic literature review},
  file = {/Users/brookeryan/Zotero/storage/C7HUQASF/Bruns et al_2021_Expressivity in Interaction.pdf}
}

@article{ExtractionSubjectsDifferencesAcceptabilityDependAbeillé2020,
  title = {Extraction from Subjects: {{Differences}} in Acceptability Depend on the Discourse Function of the Construction},
  shorttitle = {Extraction from Subjects},
  author = {Abeillé, Anne and Hemforth, Barbara and Winckel, Elodie and Gibson, Edward},
  date = {2020-11-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {204},
  pages = {104293},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104293},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027720301128},
  urldate = {2022-08-13},
  abstract = {In order to explain the unacceptability of certain long-distance dependencies – termed syntactic islands by Ross (1967) – syntacticians proposed constraints on long-distance dependencies which are universal and purely syntactic and thus not dependent on the meaning of the construction (Chomsky, 1977; Chomsky, 1995 a.o.). This predicts that these constraints should hold across constructions and languages. In this paper, we investigate the “subject island” constraint across constructions in English and French, a constraint that blocks extraction out of subjects. In particular, we compare extraction out of nominal subjects with extraction out of nominal objects, in relative clauses and wh-questions, using similar materials across constructions and languages. Contrary to the syntactic accounts, we find that unacceptable extractions from subjects involve (a) extraction in wh-questions (in both languages); or (b) preposition stranding (in English). But the extraction of a whole prepositional phrase from subjects in a relative clause, in both languages, is as good or better than a similar extraction from objects. Following Erteschik-Shir (1973) and Kuno (1987) among others, we propose a theory that takes into account the discourse status of the extracted element in the construction at hand: the extracted element is a focus (corresponding to new information) in wh-questions, but not in relative clauses. The focus status conflicts with the non-focal status of a subject (usually given or discourse-old). These results suggest that most previous discussions of islands may rely on the wrong premise that all extraction types behave alike. Once different extraction types are recognized as different constructions (Croft, 2001; Ginzburg \& Sag, 2000; Goldberg, 2006; Sag, 2010), with their own discourse functions, one can explain different extraction patterns depending on the construction.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/B8CKDEQG/Abeillé et al. - 2020 - Extraction from subjects Differences in acceptabi.pdf}
}

@inproceedings{FamilyLearningTalkAILiteracyLong2022,
  title = {Family {{Learning Talk}} in {{AI Literacy Learning Activities}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Teachey, Anthony and Magerko, Brian},
  date = {2022-04-29},
  pages = {1--20},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3502091},
  url = {https://dl.acm.org/doi/10.1145/3491102.3502091},
  urldate = {2022-08-30},
  abstract = {The unique role that AI plays in making decisions that afect humans creates a need for public understanding of AI. Informal learning spaces are important contexts for fostering AI literacy, as they can reach a broader audience and provide spaces for children and parents to learn together. This paper explores 1) what types of dialogue familes engage in when learning about AI in an at-home learning environment to inform our understanding of 2) how to design AI literacy activities for informal learning contexts. We present an analysis of family dialogue surrounding three AI education activities and use our fndings to update existing principles for designing AI literacy educational interventions. Our fndings indicate that embodied interaction, collaboration, and lowering barriers to entry were efective at fostering learning talk. Our results also reveal emergent areas for future research on how to support parents and design visualizations and datasets for AI learning.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/YPPFJYHY/Long et al. - 2022 - Family Learning Talk in AI Literacy Learning Activ.pdf}
}

@inproceedings{FashionQAIDrivenCreativitySupportToolJeon2021,
  title = {{{FashionQ}}: {{An AI-Driven Creativity Support Tool}} for {{Facilitating Ideation}} in {{Fashion Design}}},
  shorttitle = {{{FashionQ}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jeon, Youngseung and Jin, Seungwan and Shih, Patrick C. and Han, Kyungsik},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445093},
  url = {https://doi.org/10.1145/3411764.3445093},
  urldate = {2022-06-10},
  abstract = {Recent research on creativity support tools (CST) adopts artificial intelligence (AI) that leverages big data and computational capabilities to facilitate creative work. Our work aims to articulate the role of AI in supporting creativity with a case study of an AI-based CST tool in fashion design based on theoretical groundings. We developed AI models by externalizing three cognitive operations (extending, constraining, and blending) that are associated with divergent and convergent thinking. We present FashionQ, an AI-based CST that has three interactive visualization tools (StyleQ, TrendQ, and MergeQ). Through interviews and a user study with 20 fashion design professionals (10 participants for the interviews and 10 for the user study), we demonstrate the effectiveness of FashionQ on facilitating divergent and convergent thinking and identify opportunities and challenges of incorporating AI in the ideation process. Our findings highlight the role and use of AI in each cognitive operation based on professionals’ expertise and suggest future implications of AI-based CST development.},
  isbn = {978-1-4503-8096-6},
  keywords = {artificial intelligence utilization,cognitive operation,creativity support tool,fashion design,ideation process},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/FashionQAIDrivenCreativitySupportToolJeon2021.pdf}
}

@online{FemaleScienceProfessorWritingMeProfessor2007,
  title = {{{FemaleScienceProfessor}}: {{Writing To Me}}},
  shorttitle = {{{FemaleScienceProfessor}}},
  author = {Professor, Female Science},
  date = {2007-12-12},
  url = {https://science-professor.blogspot.com/2007/12/writing-to-me.html},
  urldate = {2022-08-05},
  organization = {{FemaleScienceProfessor}},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/RALQ8BIR/writing-to-me.html}
}

@misc{ForwardEntrainmentEvidenceControversiesConstraintsSaberi2021,
  title = {Forward {{Entrainment}}: {{Evidence}}, {{Controversies}}, {{Constraints}}, and {{Mechanisms}}},
  shorttitle = {Forward {{Entrainment}}},
  author = {Saberi, Kourosh and Hickok, Gregory},
  date = {2021-07-09},
  pages = {2021.07.06.451373},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.07.06.451373},
  url = {https://www.biorxiv.org/content/10.1101/2021.07.06.451373v1},
  urldate = {2022-07-13},
  abstract = {We define forward entrainment as that part of the entrainment process that outlasts the entraining stimulus. In this study, we examine conditions under which one may or may not observe forward entrainment. In part 1, we review and evaluate studies that have observed forward entrainment using a variety of psychophysical methods (detection, discrimination and reaction times), different target stimuli (tones, noise, gaps), different entraining sequences (sinusoidal, rectangular or sawtooth waveforms), a variety of physiological measures (MEG, EEG, ECoG, CSD), in different modalities (auditory and visual), across modalities (audiovisual and auditory- motor), and in different species. In part 2, we review those studies that have failed to observe forward entrainment, with emphasis on evaluating the methodological and stimulus design differences that may clarify the contrasting findings across these two classes of studies. In part 3, we describe those experimental conditions under which we ourselves have failed to observe forward entrainment, and provide new data on use of complex envelope patterns as entraining stimuli, show data on intersubject variability, and provide new findings on psychometric functions that characterize the strength of forward entrainment at different SNRs. In part 4 we theorize on potential mechanisms, describe how neurophysiological and psychophysical studies approach the study of entrainment, and caution against drawing direct causal inferences between the two without compelling evidence beyond correlative measures.},
  langid = {english},
  keywords = {2022 Advisor Search,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ForwardEntrainmentEvidenceControversiesConstraintsSaberi2021.pdf;/Users/brookeryan/Zotero/storage/2SZL7W3Q/2021.07.06.451373v1.html}
}

@online{FrequentlyAskedQuestionsDragonFruits,
  title = {Frequently {{Asked Questions Dragon Fruits}} - {{GrowDF}}.Com {{Pitaya Pitahaya}}},
  url = {https://growdf.com/frequently-asked-questions/},
  urldate = {2022-07-24},
  abstract = {Frequently Asked Questions about Dragon Fruits. Is it self fertile or self sterile? Do I cross pollinate my Dragon Fruits?},
  langid = {american},
  organization = {{GrowDF.com}},
  keywords = {Dragon Fruit,notion},
  file = {/Users/brookeryan/Zotero/storage/9CZHXG3Z/frequently-asked-questions.html}
}

@online{GANGoghCreatingArtGANsJones2017,
  title = {{{GANGogh}}: {{Creating Art}} with {{GANs}}},
  shorttitle = {{{GANGogh}}},
  author = {Jones, Kenny},
  date = {2017-06-19T00:19:58},
  url = {https://towardsdatascience.com/gangogh-creating-art-with-gans-8d087d8f74a1},
  urldate = {2022-05-12},
  abstract = {Introduction:},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/brookeryan/Zotero/storage/BPDXPLA2/gangogh-creating-art-with-gans-8d087d8f74a1.html}
}

@online{GaryCottrellGradSchoolAdvice,
  title = {Gary {{Cottrell}}: {{Grad}} School Advice},
  url = {https://cseweb.ucsd.edu//~gary/Advice.html},
  urldate = {2021-10-28},
  keywords = {Advice,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/CD79ZFFX/Advice.html}
}

@inproceedings{GenAICHIGenerativeAIHCIMuller2022,
  title = {{{GenAICHI}}: {{Generative AI}} and {{HCI}}},
  shorttitle = {{{GenAICHI}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems Extended Abstracts}}},
  author = {Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and Martin, Charles Patrick and Walsh, Greg},
  date = {2022-04-27},
  series = {{{CHI EA}} '22},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3491101.3503719},
  url = {https://doi.org/10.1145/3491101.3503719},
  urldate = {2022-06-10},
  abstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. It is time to convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain.},
  isbn = {978-1-4503-9156-6},
  keywords = {Bias,Design,Generative AI,ObsCite,Uncertainty.},
  file = {/Users/brookeryan/Documents/Obsidian/reference/GenAICHIGenerativeAIHCIMuller2022.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/GenAICHIGenerativeAIHCIMuller2022.pdf}
}

@article{GeneralizingPhysicalPredictionComposingForcesWang,
  title = {Generalizing Physical Prediction by Composing Forces and Objects},
  author = {Wang, Haoliang and Allen, Kelsey and Vul, Edward and Fan, Judith E},
  pages = {8},
  langid = {english},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/R4DEHQDD/Wang et al. - Generalizing physical prediction by composing forc.pdf}
}

@online{GettingGraduateSchool,
  title = {Getting into {{Graduate School}}},
  url = {https://grad.berkeley.edu/graduate-diversity/berkeley-undergrads/getting-into-graduate-school/},
  urldate = {2022-08-29},
  langid = {american},
  organization = {{Berkeley Graduate Division}},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/Q8BNCEDC/getting-into-graduate-school.html}
}

@misc{Gitb,
  title = {Git},
  url = {https://git-scm.com/},
  urldate = {2022-01-31}
}

@online{Gitc,
  title = {Git},
  url = {https://git-scm.com/},
  urldate = {2022-01-31}
}

@article{GoalNeglectSpearmanCompetingPartsDuncan20080204,
  title = {Goal Neglect and {{Spearman}}'s {$<$}em{$>$}g{$<$}/Em{$>$}: {{Competing}} Parts of a Complex Task.},
  shorttitle = {Goal Neglect and {{Spearman}}'s {$<$}em{$>$}g{$<$}/Em{$>$}},
  author = {Duncan, John and Parr, Alice and Woolgar, Alexandra and Thompson, Russell and Bright, Peter and Cox, Sally and Bishop, Sonia and Nimmo-Smith, Ian},
  date = {20080204},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {137},
  number = {1},
  pages = {131},
  publisher = {{US: American Psychological Association}},
  issn = {1939-2222},
  doi = {10.1037/0096-3445.137.1.131},
  url = {https://psycnet.apa.org/fulltext/2008-01081-009.pdf},
  urldate = {2022-08-09},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/JD9U8BFK/2008-01081-009.html}
}

@online{GradApplicationMaterialsGoogleDrive,
  title = {Grad {{Application Materials}} - {{Google Drive}}},
  url = {https://drive.google.com/drive/folders/1eZ1CMG-bZQlPtB6OkduFy0Qq2-uS-mjH},
  urldate = {2022-08-07},
  keywords = {Advice,PhD Application Assistance},
  file = {/Users/brookeryan/Zotero/storage/EHSITUUH/1eZ1CMG-bZQlPtB6OkduFy0Qq2-uS-mjH.html}
}

@online{GradSchoolHacksUCLAGraduate,
  title = {Grad {{School Hacks}} – {{UCLA Graduate Programs}} in {{Bioscience}} ({{GPB}})},
  url = {https://bioscience.ucla.edu/grad-school-hacks/},
  urldate = {2022-07-19},
  langid = {american},
  keywords = {notion,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/F3VLL7L4/grad-school-hacks.html}
}

@online{GraduateDesignatedEmphasisCognitiveScience,
  title = {Graduate {{Designated Emphasis}} | {{Cognitive Science}}},
  url = {https://cogsci.berkeley.edu/cogsci.berkeley.edu/graduatedesignatedemphasis},
  urldate = {2022-08-09},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/CWER4LRW/graduatedesignatedemphasis.html}
}

@online{GraduateSchoolStatementPurposeEECS,
  title = {Graduate {{School Statement}} of {{Purpose}} : {{EECS Communication Lab}}},
  url = {https://mitcommlab.mit.edu/eecs/commkit/graduate-school-personal-statement/},
  urldate = {2022-07-14},
  keywords = {notion,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/Y4IW42UP/graduate-school-personal-statement.html}
}

@online{GuideGettingGradSchoolStanford,
  title = {Guide on {{Getting Into Grad School}} | {{Stanford Humanities}} and {{Sciences}}},
  url = {https://humsci.stanford.edu/prospective-students/guide-getting-grad-school},
  urldate = {2022-08-09},
  langid = {english},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/4TVNESUM/guide-getting-grad-school.html}
}

@online{GuideStableDiffusion,
  title = {(1) Guide - {{StableDiffusion}}},
  url = {https://www.reddit.com/r/StableDiffusion/wiki/guide/},
  urldate = {2022-08-30},
  abstract = {r/StableDiffusion: Welcome to the unofficial Stable Diffusion AI subreddit! Here you can post art, discuss about the software, share something cool …},
  langid = {american},
  organization = {{reddit}},
  keywords = {AI Generated Art Educational},
  file = {/Users/brookeryan/Zotero/storage/9Y9KLB4V/guide.html}
}

@inproceedings{HeadWornDisplaysEmergencyMedicalServicesSchlosser2021,
  title = {Head-{{Worn Displays}} for {{Emergency Medical Services Staff}}: {{Properties}} of {{Prehospital Work}}, {{Use Cases}}, and {{Design Considerations}}},
  shorttitle = {Head-{{Worn Displays}} for {{Emergency Medical Services Staff}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schlosser, Paul and Matthews, Ben and Salisbury, Isaac and Sanderson, Penelope and Hayes, Sass},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445614},
  url = {https://doi.org/10.1145/3411764.3445614},
  urldate = {2022-06-09},
  abstract = {Head-worn displays (HWDs) offer their users high mobility, hands-free operation, and “see-what-I-see” features. In the prehospital environment, emergency medical services (EMS) staff could benefit from the unique characteristics of HWDs. We conducted a field study to analyze work practices of EMS staff and the potential of HWDs to support their activities. Based on our observations and the comments of EMS staff, we propose three use cases for HWDs in the prehospital environment. They are (1) enhanced communication between different care providers, (2) hands-free access to clinical monitoring and imaging, (3) and improved realism of training scenarios. We conclude with a set of design considerations and suggest that for the successful implementation of HWDs in EMS environments, researchers, designers, and clinical stakeholders should consider the harsh outdoor environment in which HWDs will be used, the extensive workload of staff, the complex collaboration performed, privacy requirements, and the high variability of work.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,Emergency medical services,Head-worn display,notion,Smart glasses,systematic literature review},
  file = {/Users/brookeryan/Zotero/storage/VW9L5ADA/Schlosser et al_2021_Head-Worn Displays for Emergency Medical Services Staff.pdf}
}

@article{HierarchicalConstructionValueO’Doherty,
  title = {The Hierarchical Construction of Value},
  author = {O’Doherty, John P and Rutishauser, Ueli and Iigaya, Kiyohito},
  pages = {15},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/GXMFEX52/O’Doherty et al. - The hierarchical construction of value.pdf}
}

@misc{HighResolutionImageSynthesisLatentDiffusionRombach2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04-13},
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10752},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2022-08-30},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {AI Generated Art Algorithms,notion},
  file = {/Users/brookeryan/Zotero/storage/U9TJZTIQ/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/Users/brookeryan/Zotero/storage/XZ5E3N4I/2112.html}
}

@online{HomeCognimatesb,
  title = {Home - {{Cognimates}}},
  url = {http://cognimates.me/home/},
  urldate = {2022-04-20},
  file = {/Users/brookeryan/Zotero/storage/GBCRBZWI/home.html}
}

@online{HotPota,
  title = {Hot {{Pot}}},
  url = {https://hotpot.ai/art-maker},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/HotPota.md}
}

@online{HowBecomeScientistPoor,
  title = {How to {{Become}} a {{Scientist While Poor}}},
  url = {http://www.christineliuart.com/writing/2019/2/2/how-to-become-a-scientist-while-poor},
  urldate = {2022-08-07},
  abstract = {Science, for hundreds of years, was a hobby. From Darwin to Da Vinci, exploration and experimentation was reserved for the wealthy who had time and money to ponder life’s greatest questions. Today, research is a global enterprise, with a pipeline from student to professional researcher. However, the},
  langid = {american},
  organization = {{Christine Liu}},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/SKAGAXCK/how-to-become-a-scientist-while-poor.html}
}

@article{HowBeGraduateAdviseeRaman2014,
  title = {How to {{Be}} a {{Graduate Advisee}}},
  author = {Raman, Indira M.},
  date = {2014-01-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {81},
  number = {1},
  eprint = {24411728},
  eprinttype = {pmid},
  pages = {9--11},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.12.030},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(13)01191-4},
  urldate = {2022-08-05},
  langid = {english},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/2GMMIP9U/Raman - 2014 - How to Be a Graduate Advisee.pdf}
}

@online{HowCanMyArtworkSteerGros-Dubois2020,
  title = {How Can My Artwork Steer Clear of Copyright Infringement?},
  author = {Gros-Dubois, Eric},
  date = {2020-07-21T13:15:25+00:00},
  url = {https://www.epgdlaw.com/how-can-my-artwork-steer-clear-of-copyright-infringement/},
  urldate = {2022-07-21},
  abstract = {How can my artwork steer clear of copyright infringement? - An article by EPGD Business Law on Entertainment Law.},
  langid = {american},
  organization = {{EPGD Business Law}},
  keywords = {Etsy,notion},
  file = {/Users/brookeryan/Zotero/storage/F9FGNIJK/how-can-my-artwork-steer-clear-of-copyright-infringement.html}
}

@online{HowCanPublishPapersNIPS,
  title = {How Can {{I}} Publish Papers in {{NIPS}}, {{ICML}}, {{AAAI}}, {{IJCAI}}? {{I}} Don't Know How to Get the Novel Ideas.},
  shorttitle = {How Can {{I}} Publish Papers in {{NIPS}}, {{ICML}}, {{AAAI}}, {{IJCAI}}?},
  url = {https://www.quora.com/How-can-I-publish-papers-in-NIPS-ICML-AAAI-IJCAI-I-dont-know-how-to-get-the-novel-ideas},
  urldate = {2021-10-01},
  abstract = {Answer (1 of 4): I am a firm believer of this notion of Andrew Ng. (paraphrasing his words) You have to put the following steps in loop: 1. Read as much as recent papers in your research area as possible. This gives you an understanding of the current research problems in your area. 2. Try to ...},
  langid = {english},
  organization = {{Quora}},
  keywords = {notion,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/RLBUX5DM/How-can-I-publish-papers-in-NIPS-ICML-AAAI-IJCAI-I-dont-know-how-to-get-the-novel-ideas.html}
}

@article{HowComputerDesignedThisWeek,
  title = {How a Computer Designed This Week’s Cover},
  journaltitle = {The Economist},
  issn = {0013-0613},
  url = {https://www.economist.com/news/2022/06/11/how-a-computer-designed-this-weeks-cover},
  urldate = {2022-08-29},
  entrysubtype = {magazine},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/KQPJYGJV/how-a-computer-designed-this-weeks-cover.html}
}

@article{HowDevelopersUseAPIDocumentationMeng2019,
  title = {How Developers Use {{API}} Documentation: An Observation Study},
  shorttitle = {How Developers Use {{API}} Documentation},
  author = {Meng, Michael and Steinhardt, Stephanie and Schubert, Andreas},
  date = {2019-08-26},
  journaltitle = {Communication Design Quarterly},
  shortjournal = {Commun. Des. Q. Rev},
  volume = {7},
  number = {2},
  pages = {40--49},
  doi = {10.1145/3358931.3358937},
  url = {https://doi.org/10.1145/3358931.3358937},
  urldate = {2021-10-21},
  abstract = {Application Programming Interfaces (APIs) play a crucial role in modern software engineering. However, learning to use a new API often is a challenge for developers. In order to support the learning process effectively, we need to understand how developers use documentation when starting to work with a new API. We report an exploratory study that observed developers while they solved programming tasks involving a simple API. The results reveal differences regarding developer activities and documentation usage that a successful design strategy for API documentation needs to accommodate. Several guidelines to optimize API documentation are discussed.},
  keywords = {API documentation,information design,observation method,usability},
  annotation = {6 citations (Crossref) [2022-06-09]}
}

@article{HowEffectiveDevelopersInvestigateSourceRobillard2005,
  title = {How Effective Developers Investigate Source Code: {{An}} Exploratory Study},
  shorttitle = {How Effective Developers Investigate Source Code},
  author = {Robillard, Martin and Coelho, Wesley and Murphy, Gail},
  date = {2005-01},
  journaltitle = {Software Engineering, IEEE Transactions on},
  volume = {30},
  pages = {889--903},
  doi = {10.1109/TSE.2004.101},
  abstract = {Prior to performing a software change task, developers must discover and understand the subset of the system relevant to the task. Since the behavior exhibited by individual developers when investigating a software system is influenced by intuition, experience, and skill, there is often significant variability in developer effectiveness. To understand the factors that contribute to effective program investigation behavior, we conducted a study of five developers performing a change task on a medium-size open source system. We isolated the factors related to effective program investigation behavior by performing a detailed qualitative analysis of the program investigation behavior of successful and unsuccessful developers. We report on these factors as a set of detailed observations, such as evidence of the phenomenon of inattention blindness by developers skimming source code. In general, our results support the intuitive notion that a methodical and structured approach to program investigation is the most effective.},
  annotation = {145 citations (Crossref) [2022-06-09]}
}

@article{HowEfficiencyShapesHumanLanguageGibson2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  date = {2019-05},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {389--407},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.02.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300580},
  urldate = {2022-08-10},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/VVNFA9T3/Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf}
}

@article{HowEfficiencyShapesHumanLanguageGibson2019a,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  date = {2019-05-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {389--407},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.02.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661319300580},
  urldate = {2022-08-13},
  abstract = {Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/QP6LB7D7/Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf;/Users/brookeryan/Zotero/storage/IYKFLV5W/S1364661319300580.html}
}

@online{HowEmailProfessorRegardingResearch,
  title = {How to {{Email}} a {{Professor Regarding Research}} | {{Undergraduate Research Opportunities}}},
  url = {https://ugr.ue.ucsc.edu/email},
  urldate = {2022-08-10},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/BGMNX7EZ/email.html}
}

@online{HowEmailProfessorRegardingResearcha,
  title = {How to {{Email}} a {{Professor Regarding Research}} | {{Undergraduate Research Opportunities}}},
  url = {https://ugr.ue.ucsc.edu/email},
  urldate = {2022-08-10},
  keywords = {Advice,notion}
}

@online{HowGrowQueenNightCactus,
  title = {How to {{Grow}} a {{Queen}} of the {{Night Cactus}}},
  url = {https://homeguides.sfgate.com/grow-queen-night-cactus-70224.html},
  urldate = {2022-07-25},
  abstract = {How to Grow a Queen of the Night Cactus. The Queen of the Night cactus (Epiphyllum oxypetalum) has frequently been the guest of honor at late-night parties, since it performs only during the dark hours. As it flaunts fragrant white flowers up to 1 foot long, however, it is worth waiting up for. Also called ...},
  langid = {english},
  organization = {{Home Guides | SF Gate}},
  keywords = {Epiphyllum oxypetalum,notion},
  file = {/Users/brookeryan/Zotero/storage/L5UPYDGV/grow-queen-night-cactus-70224.html}
}

@online{HowItGoodDALLEExplained,
  title = {How Is It so Good ? ({{DALL-E Explained Pt}}. 2) - {{ML}}@{{B Blog}}},
  shorttitle = {How Is It so Good ?},
  url = {https://ml.berkeley.edu/blog/posts/dalle2/},
  urldate = {2022-05-31},
  abstract = {DALL-E is an increibly powerful model from OpenAI capable of generating incredibly creative images from a text prompt. In this blog post we explore the transformer part of DALL-E, which is sort of like its brain; it's the component responsible for connecting the world of natural language with our visual world. Specificially this blog will look at questions like "how is it so good ?" and "why are transformers able to express and integrate so much information ?"},
  langid = {english},
  organization = {{How is it so good ? (DALL-E Explained Pt. 2) - ML@B Blog}},
  file = {/Users/brookeryan/Zotero/storage/389A5E4B/dalle2.html}
}

@online{HowMakeWaterproofStickersFull2021,
  title = {How {{To Make Waterproof Stickers}} | {{Full Page Cricut Hack}}},
  date = {2021-03-16T03:02+00:00},
  url = {https://mooneyesart.com/how-to-make-waterproof-stickers-full-page-cricut-hack/},
  urldate = {2022-07-21},
  abstract = {Here is an easy way to make your own waterproof stickers. You can follow along if you have a Cricut machine, or you can even cut them out by hand! The Cricut machine just makes the process easier.},
  langid = {american},
  organization = {{mooneyesart}},
  keywords = {notion,Waterproof stickers},
  file = {/Users/brookeryan/Zotero/storage/WH28RZ6B/how-to-make-waterproof-stickers-full-page-cricut-hack.html}
}

@article{HowPickGraduateAdvisorBarres2013,
  title = {How to {{Pick}} a {{Graduate Advisor}}},
  author = {Barres, Ben A.},
  date = {2013-10-16},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {80},
  number = {2},
  eprint = {24139033},
  eprinttype = {pmid},
  pages = {275--279},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.10.005},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(13)00907-0},
  urldate = {2022-08-05},
  langid = {english},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/I6UVYY65/Barres - 2013 - How to Pick a Graduate Advisor.pdf;/Users/brookeryan/Zotero/storage/ZTN4M4UI/S0896-6273(13)00907-0.html}
}

@inproceedings{HowProfessionalDevelopersComprehendSoftwareRoehm2012,
  title = {How Do Professional Developers Comprehend Software?},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Roehm, Tobias and Tiarks, Rebecca and Koschke, Rainer and Maalej, Walid},
  date = {2012-06},
  pages = {255--265},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227188},
  abstract = {Research in program comprehension has considerably evolved over the past two decades. However, only little is known about how developers practice program comprehension under time and project pressure, and which methods and tools proposed by researchers are used in industry. This paper reports on an observational study of 28 professional developers from seven companies, investigating how developers comprehend software. In particular we focus on the strategies followed, information needed, and tools used. We found that developers put themselves in the role of end users by inspecting user interfaces. They try to avoid program comprehension, and employ recurring, structured comprehension strategies depending on work context. Further, we found that standards and experience facilitate comprehension. Program comprehension was considered a subtask of other maintenance tasks rather than a task by itself. We also found that face-to-face communication is preferred to documentation. Overall, our results show a gap between program comprehension research and practice as we did not observe any use of state of the art comprehension tools and developers seem to be unaware of them. Our findings call for further careful analysis and for reconsidering research agendas.},
  eventtitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Companies,Content management,context awareness,Documentation,empirical studies,Interviews,Java,maintenance,program comprehension,Software,software documentation,Visualization},
  annotation = {92 citations (Crossref) [2022-06-09]}
}

@online{HowShareGoogleCalendarManage,
  title = {How to {{Share Google Calendar}} \& {{Manage Your Availability}} | {{Reclaim}}},
  url = {https://reclaim.ai/blog/how-to-share-google-calendar},
  urldate = {2022-08-23},
  abstract = {Learn how to share your Google Calendar with another Google Calendar, Outlook or iCloud Calendar, key privacy settings, and the best way to block your availability to avoid double-booking.},
  keywords = {Advisor Literature},
  file = {/Users/brookeryan/Zotero/storage/TECWQ6EH/how-to-share-google-calendar.html}
}

@online{HowTakeSmartNotesObsidianDuffney2021,
  type = {Substack newsletter},
  title = {How to {{Take Smart Notes}} in {{Obsidian}}},
  author = {Duffney, Josh},
  date = {2021-07-26},
  url = {https://theknowledgeworker.substack.com/p/how-to-take-smart-notes-in-obsidian},
  urldate = {2022-05-11},
  abstract = {A Zettelkasten Tutorial},
  organization = {{The Knowledge Worker}},
  keywords = {notion,Smart Notes},
  file = {/Users/brookeryan/Zotero/storage/R3JJEQWR/how-to-take-smart-notes-in-obsidian.html}
}

@book{HowTakeSmartNotesOneAhrens2022,
  title = {How to {{Take Smart Notes}}: {{One Simple Technique}} to {{Boost Writing}}, {{Learning}} and {{Thinking}}},
  shorttitle = {How to {{Take Smart Notes}}},
  author = {Ahrens, Sönke},
  date = {2022-03-11},
  eprint = {QmBjEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Sönke Ahrens}},
  abstract = {This is the second, revised and expanded edition. The first edition was published under the slightly longer title \&quot;How to Take Smart Notes. One Simple Technique to Boost Writing,~Learning and Thinking - for Students, Academics and Nonfiction Book Writers\&quot;.The key to good and efficient writing lies in the intelligent organisation of ideas and notes. This book helps students, academics and other knowledge workers to get more done, write intelligent texts and learn for the long run. It teaches you how to take smart notes and ensure they bring you and your projects forward.The Take Smart Notes principle is based on established psychological insight and draws from a tried and tested note-taking technique: the~Zettelkasten. This is the first comprehensive guide and description of this system in English, and not only does it explain how it works, but also why. It suits students and academics in the social sciences and humanities, nonfiction writers and others who are in the business of reading, thinking and writing.Instead of wasting your time searching for your notes, quotes or references, you can focus on what really counts: thinking, understanding and developing new ideas in writing.Dr. Sönke Ahrens is a writer and researcher in the field of education and social science. He is the author of the award-winning book “Experiment and Exploration: Forms of World Disclosure” (Springer).Since its first publication, How to Take Smart Notes has sold more than 100,000 copies and has been translated into seven languages.},
  isbn = {978-3-9824388-1-8},
  langid = {english},
  pagetotal = {212},
  keywords = {Kindle,notion,Self-Help / Personal Growth / Memory Improvement,Self-Help / Self-Management / Time Management,Study Aids / Book Notes}
}

@article{HowWorkingMemoryReinforcementLearningYoo2022,
  title = {How {{Working Memory}} and {{Reinforcement Learning Are Intertwined}}: {{A Cognitive}}, {{Neural}}, and {{Computational Perspective}}},
  shorttitle = {How {{Working Memory}} and {{Reinforcement Learning Are Intertwined}}},
  author = {Yoo, Aspen H. and Collins, Anne G. E.},
  date = {2022-03-05},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  volume = {34},
  number = {4},
  pages = {551--568},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01808},
  url = {https://doi.org/10.1162/jocn_a_01808},
  urldate = {2022-08-13},
  abstract = {Reinforcement learning and working memory are two core processes of human cognition and are often considered cognitively, neuroscientifically, and algorithmically distinct. Here, we show that the brain networks that support them actually overlap significantly and that they are less distinct cognitive processes than often assumed. We review literature demonstrating the benefits of considering each process to explain properties of the other and highlight recent work investigating their more complex interactions. We discuss how future research in both computational and cognitive sciences can benefit from one another, suggesting that a key missing piece for artificial agents to learn to behave with more human-like efficiency is taking working memory's role in learning seriously. This review highlights the risks of neglecting the interplay between different processes when studying human behavior (in particular when considering individual differences). We emphasize the importance of investigating these dynamics to build a comprehensive understanding of human cognition.},
  keywords = {Advisor Literature,notion}
}

@online{IllustratedVQGANMIRANDA2021,
  title = {The {{Illustrated VQGAN}}},
  author = {MIRANDA, LJ},
  date = {2021-08-08T00:00:00+08:00},
  url = {https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/},
  urldate = {2022-05-31},
  abstract = {VQGAN allows us to generate high-resolution images from text, and has now taken art Twitter by storm. Let me talk about how it works on a conceptual level in...},
  langid = {english},
  organization = {{Lj Miranda}},
  keywords = {ObsCite},
  file = {/Users/brookeryan/Zotero/storage/3A7R439Q/clip-vqgan.html}
}

@inproceedings{ImageExplorerMultiLayeredTouchExplorationEncourageLee2022,
  title = {{{ImageExplorer}}: {{Multi-Layered Touch Exploration}} to {{Encourage Skepticism Towards Imperfect AI-Generated Image Captions}}},
  shorttitle = {{{ImageExplorer}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lee, Jaewook and Herskovitz, Jaylin and Peng, Yi-Hao and Guo, Anhong},
  date = {2022-04-29},
  series = {{{CHI}} '22},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501966},
  url = {https://doi.org/10.1145/3491102.3501966},
  urldate = {2022-08-29},
  abstract = {Blind users rely on alternative text (alt-text) to understand an image; however, alt-text is often missing. AI-generated captions are a more scalable alternative, but they often miss crucial details or are completely incorrect, which users may still falsely trust. In this work, we sought to determine how additional information could help users better judge the correctness of AI-generated captions. We developed ImageExplorer, a touch-based multi-layered image exploration system that allows users to explore the spatial layout and information hierarchies of images, and compared it with popular text-based (Facebook) and touch-based (Seeing AI) image exploration systems in a study with 12 blind participants. We found that exploration was generally successful in encouraging skepticism towards imperfect captions. Moreover, many participants preferred ImageExplorer for its multi-layered and spatial information presentation, and Facebook for its summary and ease of use. Finally, we identify design improvements for effective and explainable image exploration systems for blind users.},
  isbn = {978-1-4503-9157-3},
  keywords = {Example Surveys}
}

@article{ImagesSymbolsDrawingWindowMindMukherjee,
  title = {From {{Images}} to {{Symbols}}: {{Drawing}} as a {{Window}} into the {{Mind}}},
  author = {Mukherjee, Kushin and Huey, Holly and Rogers, Timothy T and Fan, Judith E},
  pages = {4},
  langid = {english},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/Z3IIDSNN/Mukherjee et al. - From Images to Symbols Drawing as a Window into t.pdf}
}

@unpublished{ImaginationAugmentedNaturalLanguageUnderstandingLu2022,
  title = {Imagination-{{Augmented Natural Language Understanding}}},
  author = {Lu, Yujie and Zhu, Wanrong and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang},
  date = {2022-05-03},
  number = {arXiv:2204.08535},
  eprint = {2204.08535},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.08535},
  url = {http://arxiv.org/abs/2204.08535},
  urldate = {2022-06-10},
  abstract = {Human brains integrate linguistic and perceptual information simultaneously to understand natural language, and hold the critical ability to render imaginations. Such abilities enable us to construct new abstract concepts or concrete objects, and are essential in involving practical knowledge to solve problems in low-resource scenarios. However, most existing methods for Natural Language Understanding (NLU) are mainly focused on textual signals. They do not simulate human visual imagination ability, which hinders models from inferring and learning efficiently from limited data samples. Therefore, we introduce an Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language understanding tasks from a novel learning perspective -- imagination-augmented cross-modal understanding. iACE enables visual imagination with external knowledge transferred from the powerful generative and pre-trained vision-and-language models. Extensive experiments on GLUE and SWAG show that iACE achieves consistent improvement over visually-supervised pre-trained models. More importantly, results in extreme and normal few-shot settings validate the effectiveness of iACE in low-resource natural language understanding circumstances.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Literature Review,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ImaginationAugmentedNaturalLanguageUnderstandingLu2022.pdf;/Users/brookeryan/Zotero/storage/SZ2WVPK9/2204.html}
}

@inproceedings{ImprovingCodeReadabilityModelsTextualScalabrino2016,
  title = {Improving Code Readability Models with Textual Features},
  booktitle = {2016 {{IEEE}} 24th {{International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  author = {Scalabrino, Simone and Linares-Vásquez, Mario and Poshyvanyk, Denys and Oliveto, Rocco},
  date = {2016-05},
  pages = {1--10},
  doi = {10.1109/ICPC.2016.7503707},
  abstract = {Code reading is one of the most frequent activities in software maintenance; before implementing changes, it is necessary to fully understand source code often written by other developers. Thus, readability is a crucial aspect of source code that may significantly influence program comprehension effort. In general, models used to estimate software readability take into account only structural aspects of source code, e.g., line length and a number of comments. However, source code is a particular form of text; therefore, a code readability model should not ignore the textual aspects of source code encapsulated in identifiers and comments. In this paper, we propose a set of textual features aimed at measuring code readability. We evaluated the proposed textual features on 600 code snippets manually evaluated (in terms of readability) by 5K+ people. The results demonstrate that the proposed features complement classic structural features when predicting code readability judgments. Consequently, a code readability model based on a richer set of features, including the ones proposed in this paper, achieves a significantly higher accuracy as compared to all of the state-of-the-art readability models.},
  eventtitle = {2016 {{IEEE}} 24th {{International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  keywords = {Computational modeling,Feature extraction,Semantics,Software quality,Syntactics,Visualization},
  annotation = {45 citations (Crossref) [2022-06-09]}
}

@misc{IndividualsUseDifferentSpatialReferencePitt2021,
  title = {Individuals Use Different Spatial Reference Frames on Different Axes: {{Evidence}} from Indigenous {{Amazonians}}},
  shorttitle = {Individuals Use Different Spatial Reference Frames on Different Axes},
  author = {Pitt, Benjamin and Carstensen, Alexandra and Boni, Isabelle and Piantadosi, Steven T. and Gibson, Edward},
  date = {2021-10-01T20:02:31},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/wu2ep},
  url = {https://psyarxiv.com/wu2ep/},
  urldate = {2022-08-13},
  abstract = {The physical properties of space may be universal, but the way people conceptualize space is variable. In some groups, people tend to use egocentric space (e.g. left, right) to encode the locations of objects, while in other groups, people encode the same spatial scene using allocentric space (e.g. upriver, downriver). These different spatial frames of reference (FoRs) characterize the way people talk about spatial relations and the way they think about them, even when they are not using language. Although spatial language and spatial reasoning tend to covary, the root causes of this variation are unclear. Here we propose that variation in FoR use partly reflects the discriminability of the relevant spatial continua. In an initial test of this proposal in a group of indigenous Bolivians, we compared FoR use across spatial axes that are known to differ in discriminability. In both verbal and nonverbal tests, participants spontaneously used different FoRs on different spatial axes: On the lateral axis, where egocentric (left-right) discrimination is difficult, their spatial behavior and language was predominantly allocentric; on the sagittal axis, where egocentric (front-back) discrimination is relatively easy, they were predominantly egocentric. These findings challenge the claim that each language group can be characterized by a predominant spatial frame of reference. Rather, both spatial memory and language can differ categorically across axes, even within the same individuals. We suggest that differences in spatial discrimination can explain differences in both spatial memory and language within and across human groups.},
  langid = {american},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/Q5F3AZC9/Pitt et al. - 2021 - Individuals use different spatial reference frames.pdf}
}

@online{InformationForagingTheorya,
  title = {Information Foraging Theory},
  url = {https://irwinkwan.com/tag/information-foraging-theory/},
  urldate = {2021-10-21},
  abstract = {Posts about information foraging theory written by irwinhkwan},
  langid = {english},
  organization = {{Irwin Kwan}}
}

@article{InformationForagingTheoryPerspectiveToolsFleming2013,
  title = {An {{Information Foraging Theory Perspective}} on {{Tools}} for {{Debugging}}, {{Refactoring}}, and {{Reuse Tasks}}},
  author = {Fleming, Scott D. and Scaffidi, Chris and Piorkowski, David and Burnett, Margaret and Bellamy, Rachel and Lawrance, Joseph and Kwan, Irwin},
  date = {2013-03-01},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {22},
  number = {2},
  pages = {14:1--14:41},
  issn = {1049-331X},
  doi = {10.1145/2430545.2430551},
  url = {https://doi.org/10.1145/2430545.2430551},
  urldate = {2021-10-21},
  abstract = {Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers’ needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective.},
  keywords = {Information foraging,software maintenance},
  annotation = {43 citations (Crossref) [2022-06-09]}
}

@inproceedings{InformingAgeAppropriateAIExaminingPrinciplesWang2022,
  title = {Informing {{Age-Appropriate AI}}: {{Examining Principles}} and {{Practices}} of {{AI}} for {{Children}}},
  shorttitle = {Informing {{Age-Appropriate AI}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Ge and Zhao, Jun and Van Kleek, Max and Shadbolt, Nigel},
  date = {2022-04-29},
  pages = {1--29},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3502057},
  url = {https://dl.acm.org/doi/10.1145/3491102.3502057},
  urldate = {2022-08-30},
  abstract = {AI systems are becoming increasingly pervasive within children’s devices, apps, and services. However, it is not yet well-understood how risks and ethical considerations of AI relate to children. This paper makes three contributions to this area: frst, it identifes ten areas of alignment between general AI frameworks and codes for age-appropriate design for children. Then, to understand how such principles relate to real application contexts, we conducted a landscape analysis of children’s AI systems, via a systematic literature review including 188 papers. This analysis revealed a wide assortment of applications, and that most systems’ designs addressed only a small subset of principles among those we identifed. Finally, we synthesised our fndings in a framework to inform a new “Code for Age-Appropriate AI”, which aims to provide timely input to emerging policies and standards, and inspire increased interactions between the AI and child-computer interaction communities.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/M4AUN578/Wang et al. - 2022 - Informing Age-Appropriate AI Examining Principles.pdf}
}

@inproceedings{InformingAgeAppropriateAIExaminingPrinciplesWang2022a,
  title = {Informing {{Age-Appropriate AI}}: {{Examining Principles}} and {{Practices}} of {{AI}} for {{Children}}},
  shorttitle = {Informing {{Age-Appropriate AI}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Ge and Zhao, Jun and Van Kleek, Max and Shadbolt, Nigel},
  date = {2022-04-29},
  pages = {1--29},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3502057},
  url = {https://dl.acm.org/doi/10.1145/3491102.3502057},
  urldate = {2022-08-30},
  abstract = {AI systems are becoming increasingly pervasive within children’s devices, apps, and services. However, it is not yet well-understood how risks and ethical considerations of AI relate to children. This paper makes three contributions to this area: frst, it identifes ten areas of alignment between general AI frameworks and codes for age-appropriate design for children. Then, to understand how such principles relate to real application contexts, we conducted a landscape analysis of children’s AI systems, via a systematic literature review including 188 papers. This analysis revealed a wide assortment of applications, and that most systems’ designs addressed only a small subset of principles among those we identifed. Finally, we synthesised our fndings in a framework to inform a new “Code for Age-Appropriate AI”, which aims to provide timely input to emerging policies and standards, and inspire increased interactions between the AI and child-computer interaction communities.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/MGCM82CR/Wang et al. - 2022 - Informing Age-Appropriate AI Examining Principles.pdf}
}

@inproceedings{InformingAgeAppropriateAIExaminingPrinciplesWang2022b,
  title = {Informing {{Age-Appropriate AI}}: {{Examining Principles}} and {{Practices}} of {{AI}} for {{Children}}},
  shorttitle = {Informing {{Age-Appropriate AI}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Ge and Zhao, Jun and Van Kleek, Max and Shadbolt, Nigel},
  date = {2022-04-29},
  series = {{{CHI}} '22},
  pages = {1--29},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3491102.3502057},
  url = {https://doi.org/10.1145/3491102.3502057},
  urldate = {2022-08-29},
  abstract = {AI systems are becoming increasingly pervasive within children’s devices, apps, and services. However, it is not yet well-understood how risks and ethical considerations of AI relate to children. This paper makes three contributions to this area: first, it identifies ten areas of alignment between general AI frameworks and codes for age-appropriate design for children. Then, to understand how such principles relate to real application contexts, we conducted a landscape analysis of children’s AI systems, via a systematic literature review including 188 papers. This analysis revealed a wide assortment of applications, and that most systems’ designs addressed only a small subset of principles among those we identified. Finally, we synthesised our findings in a framework to inform a new “Code for Age-Appropriate AI”, which aims to provide timely input to emerging policies and standards, and inspire increased interactions between the AI and child-computer interaction communities.},
  isbn = {978-1-4503-9157-3},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/J7VC6NW9/Wang et al. - 2022 - Informing Age-Appropriate AI Examining Principles.pdf}
}

@inproceedings{InfusingDesignThinkingSoftwareEngineeringPalacin-Silva2017,
  title = {Infusing {{Design Thinking}} into a {{Software Engineering Capstone Course}}},
  booktitle = {2017 {{IEEE}} 30th {{Conference}} on {{Software Engineering Education}} and {{Training}} ({{CSEE T}})},
  author = {Palacin-Silva, Maria and Khakurel, Jayden and Happonen, Ari and Hynninen, Timo and Porras, Jari},
  date = {2017-11},
  pages = {212--221},
  issn = {2377-570X},
  doi = {10.1109/CSEET.2017.41},
  abstract = {Software engineering (SE) educators are challenged to balance the scope and depth in their courses to train students in skills which will fulfill the ever-evolving industry needs. Capstone courses are a tool for educators to transfer hands-on experience into practical knowledge and skills of SE students. This paper describes the design of a Casptone course, at Lappeenranta University of Technology. The designed course is "human-centric SE capstone", infusing design thinking methods and agile practices into the project life-cycle knowhow. The capstone was offered in spring of 2017 as a 16-week course for 29 students. Design thinking was effective to perform requirement elicitation, software design and testing. Also, the applied approach allowed students to be self-directed which increased their motivation, as a result there was zero dropout rate. Design thinking is a powerful mean of problem solving and effectively supports SE education in bringing a more hands-on and minds-on, problem-based curriculum.},
  eventtitle = {2017 {{IEEE}} 30th {{Conference}} on {{Software Engineering Education}} and {{Training}} ({{CSEE T}})},
  keywords = {capstone,course design,Design methodology,design thinking,education,Education,human-centric,Industries,Software,software curricula,software engineering,Software engineering,Testing,Tools},
  annotation = {20 citations (Crossref) [2022-06-09]}
}

@incollection{InhalingExhalingHowTechnologiesCanPrpa2020,
  title = {Inhaling and {{Exhaling}}: {{How Technologies Can Perceptually Extend}} Our {{Breath Awareness}}},
  shorttitle = {Inhaling and {{Exhaling}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Prpa, Mirjana and Stepanova, Ekaterina R. and Schiphorst, Thecla and Riecke, Bernhard E. and Pasquier, Philippe},
  date = {2020-04-21},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3313831.3376183},
  urldate = {2022-06-09},
  abstract = {Attending to breath is a self-awareness practice that exists within many contemplative and reflective traditions and is recognized for its benefits to well-being. Our current technological landscape embraces a large body of systems that utilize breath data in order to foster self-awareness. This paper seeks to deepen our understanding of the design space of systems that perceptually extend breath awareness. Our contribution is twofold: (1) our analysis reveals how the underlying theoretical frameworks shape the system design and its evaluation, and (2) how system design features support perceptual extension of breath awareness. We review and critically analyze 31 breath-based interactive systems. We identify 4 theoretical frameworks and 3 design strategies for interactive systems that perceptually extend breath awareness. We reflect upon this design space from both a theoretical and system design perspective, and propose future design directions for developing systems that "listen to" breath and perceptually extend it.},
  isbn = {978-1-4503-6708-0},
  keywords = {AI Education Project,breath,breathing regulation,breathing synchronization,mindfulness-based design,notion,perceptually extending,soma design,systematic literature review}
}

@article{IntegratedInformationTheoryConsciousnessItsTononi2016,
  title = {Integrated Information Theory: {{From}} Consciousness to Its Physical Substrate},
  shorttitle = {Integrated Information Theory},
  author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
  date = {2016-05-26},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nature Reviews Neuroscience},
  volume = {17},
  doi = {10.1038/nrn.2016.44},
  abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause–effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
  keywords = {COGS 269,notion},
  file = {/Users/brookeryan/Zotero/storage/YQUR3VK6/Tononi et al. - 2016 - Integrated information theory From consciousness .pdf}
}

@article{IntelligenceFrontalLobeOrganizationGoaldirectedDuncan1996,
  title = {Intelligence and the Frontal Lobe: The Organization of Goal-Directed Behavior},
  shorttitle = {Intelligence and the Frontal Lobe},
  author = {Duncan, J. and Emslie, H. and Williams, P. and Johnson, R. and Freer, C.},
  date = {1996-06},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cogn Psychol},
  volume = {30},
  number = {3},
  eprint = {8660786},
  eprinttype = {pmid},
  pages = {257--303},
  issn = {0010-0285},
  doi = {10.1006/cogp.1996.0008},
  abstract = {Basic to the study of individual differences is the concept of 'general intelligence' or Spearman's g. In this article we suggest that g is largely a reflection of the control functions of the frontal lobe. A series of experiments investigates a phenomenon we call goal neglect: disregard of a task requirement event though it has been understood and remembered. Subjectively it is as though the neglected requirement "slips the subject's mind." Previously described in frontal patients, we show that goal neglect can also be seen in some members of the normal population. In line with conventional distinctions between controlled and automatic processing, eliciting conditions for goal neglect include novelty, weak error feedback, and multiple concurrent task requirements. Under these conditions neglect is linked closely to g and extremely common after frontal lesions. Following many other models, we suggest that behavior in any task is structured by a set of action constraints or requirements, derived in part from verbal instructions and specified at multiple levels of abstraction. A frontal process of constraint or requirement activation is fundamental to Spearman's g.},
  langid = {english},
  keywords = {Advisor Literature}
}

@inproceedings{InteractiveLearningMethodEngageStudentsKrusche2020,
  title = {An Interactive Learning Method to Engage Students in Modeling},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}}},
  author = {Krusche, Stephan and von Frankenberg, Nadine and Reimer, Lara Marie and Bruegge, Bernd},
  options = {useprefix=true},
  date = {2020-06-27},
  series = {{{ICSE-SEET}} '20},
  pages = {12--22},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3377814.3381701},
  url = {https://doi.org/10.1145/3377814.3381701},
  urldate = {2021-09-24},
  abstract = {Modeling is an important skill in software engineering. However, it is often not tangible for students and not appreciated. Students prefer coding because they receive immediate feedback from the compiler. Engaging students in modeling is difficult, especially in large introductory courses. We have developed an interactive learning method for modeling which is based on an easy to use online editor. Students learn modeling in guided tutorials in the lecture right after the theory is introduced and deepen their modeling skills in group work and homework exercises. This learning method was applied in a large introductory course with more than 1000 students. An empirical evaluation of the method demonstrated that the students' learning outcome in modeling improved significantly by up to 87 \%. Students are motivated to use models in their future projects and understand how to approach problems with models. The use of interactive models in programming exercises improves their understanding of the taught concepts.},
  isbn = {978-1-4503-7124-7},
  keywords = {education,interactive,learning management system,learning success,modeling,online editor,SEET 2020,software engineering},
  annotation = {3 citations (Crossref) [2022-06-09]}
}

@online{Interpolationa,
  title = {Interpolation},
  url = {https://magenta.tensorflow.org/assets/sketch_rnn_demo/multi_predict.html},
  keywords = {AI Generated Art,notion}
}

@online{InterpretabilityVsNeuroscienceRoughNote,
  title = {Interpretability vs {{Neuroscience}} [Rough Note]},
  url = {https://colah.github.io/notes/interp-v-neuro/},
  urldate = {2022-08-23},
  abstract = {A list of advantages that make understanding artificial nerural networks much easier than biological ones.},
  langid = {english},
  keywords = {Neural Networks Research Project,Neural Networks to Humans},
  file = {/Users/brookeryan/Zotero/storage/YD8DHD4C/interp-v-neuro.html}
}

@unpublished{InterviewJohnDoherty,
  title = {Interview with {{John O}}'{{Doherty}}},
  url = {https://www.youtube.com/watch?v=POfrgcD5WLw&feature=youtu.be},
  keywords = {Advisor Literature,notion}
}

@online{IntroductionNeuromatchAcademyDeepLearning,
  title = {Introduction — {{Neuromatch Academy}}: {{Deep Learning}}},
  url = {https://deeplearning.neuromatch.io/tutorials/intro.html},
  urldate = {2022-07-22},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/WFW7P6U9/intro.html}
}

@inproceedings{IntroductorySoftwareEngineeringCourseThatLudi2005,
  title = {An Introductory Software Engineering Course That Facilitates Active Learning},
  booktitle = {Proceedings of the 36th {{SIGCSE}} Technical Symposium on {{Computer}} Science Education},
  author = {Ludi, Stephanie and Natarajan, Swaminathan and Reichlmayr, Thomas},
  date = {2005-02-23},
  series = {{{SIGCSE}} '05},
  pages = {302--306},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1047344.1047449},
  url = {https://doi.org/10.1145/1047344.1047449},
  urldate = {2021-10-21},
  abstract = {At the Rochester Institute of Technology, the undergraduate introductory software engineering course has been redesigned from a lecture-lab format to a project-centric studio format. The new format blends the lecture material with the project work. As a result, students drive their own learning experience based on scaffolding created by the course design. The challenges faced and the techniques and strategies utilized in the planning and delivery of the course will be discussed, including the utilization of online learning support infrastructure. This paper presents instructor experiences, analysis of student feedback, lessons learned and recommendations for other educators considering an active learning approach for their courses.},
  isbn = {978-1-58113-997-6},
  keywords = {active learning,cooperative learning,software engineering,student centered instruction,student teams},
  annotation = {11 citations (Crossref) [2022-06-09]}
}

@online{IOSUsingGoodNoteObsidianSame2021,
  title = {[{{IOS}}] {{Using GoodNote}} and {{Obsidian}} in the Same Vault - {{Share}} \& Showcase},
  date = {2021-04-27T23:07:49+00:00},
  url = {https://forum.obsidian.md/t/ios-using-goodnote-and-obsidian-in-the-same-vault/17298},
  urldate = {2022-06-10},
  abstract = {Hello there!  Yesterday, on the discord, someone ask how to deal with Goodnote and Obsidian, in the same Vault.  I do that, so I create this post to explain HOW I do this.  First, my setup :   An Ipad A Google Drive (with a lot of place) Obsidian (…) GoodNote.  Obviously, you can replace Google Drive with OneDrive or Dropbox.  Step One : [GOODNOTE] ⚠  If you have already created an automated backup : stop it and click on “OK” to quit the menu. You need to quit totally the parameters.  No...},
  langid = {english},
  organization = {{Obsidian Forum}},
  keywords = {notion,Smart Notes},
  file = {/Users/brookeryan/Zotero/storage/Y76KEMJN/17298.html}
}

@article{JammingTogetherConceptMappingPandemicPothier2021,
  title = {Jamming {{Together}}: {{Concept Mapping}} in the {{Pandemic Classroom}}},
  shorttitle = {Jamming {{Together}}},
  author = {Pothier, Wendy},
  date = {2021-03-12},
  journaltitle = {Ticker: The Academic Business Librarianship Review},
  volume = {5},
  number = {2},
  issn = {2369-9779},
  doi = {10.3998/ticker.16481003.0005.220},
  url = {http://hdl.handle.net/2027/spo.16481003.0005.220},
  urldate = {2021-10-21},
  langid = {english},
  annotation = {1 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Documents/Obsidian/reference/JammingTogetherConceptMappingPandemicPothier2021.md}
}

@online{KDIFFUSIONRETARDGUIDEGUI,
  title = {--{{K-DIFFUSION RETARD GUIDE}} ({{GUI}})--},
  url = {https://rentry.co/kretard},
  urldate = {2022-08-30},
  abstract = {This guide has been replaced Please visit https://rentry.org/GUItard for the latest features in an even easier guide NEWEST SCRIPT UPDATES AVAILABLE HERE: https://github.com/hlky/stable-diffusion-webui \^(warning: bleeding edge, may have bugs- use guide script below if concerned about stability)\^...},
  keywords = {AI Generated Art Educational},
  file = {/Users/brookeryan/Zotero/storage/BHI248ZV/kretard.html}
}

@inproceedings{KidsMakingAIIntegratingMachineSakulkueakulsuk2018,
  title = {Kids Making {{AI}}: {{Integrating Machine Learning}}, {{Gamification}}, and {{Social Context}} in {{STEM Education}}},
  shorttitle = {Kids Making {{AI}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}}, and {{Learning}} for {{Engineering}} ({{TALE}})},
  author = {Sakulkueakulsuk, Bawornsak and Witoon, Siyada and Ngarmkajornwiwat, Potiwat and Pataranutaporn, Pornpen and Surareungchai, Werasak and Pataranutaporn, Pat and Subsoontorn, Pakpoom},
  date = {2018-12},
  pages = {1005--1010},
  issn = {2470-6698},
  doi = {10.1109/TALE.2018.8615249},
  abstract = {We present an approach in STEM education at the intersection of machine learning, gamification, and social context based in Thailand. We designed an agricultural based AI challenge that fostered students to learn the process of creating machine learning models in the form of a game with the emphasis on the Four P's of Creative Learning (Projects, Passion, Play, and Peers). Our goal is to come up with an innovative education model that encourages the students to connect the emerging technological solutions such as AI with the pressing realworld problems in the playful environment. We found that machine learning can be used as a tool to successfully conduct interdisciplinary education at the middle school level.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}}, and {{Learning}} for {{Engineering}} ({{TALE}})},
  keywords = {agriculture,AI,Conferences,Education,Games,machine learning,Machine learning,Predictive models,STEM,Tools}
}

@article{LargescaleEvidenceDependencyLengthMinimizationFutrell2015,
  title = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  date = {2015-08-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {33},
  pages = {10336--10341},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1502134112},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1502134112},
  urldate = {2022-08-13},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/TE6A2ZVN/Futrell et al. - 2015 - Large-scale evidence of dependency length minimiza.pdf}
}

@unpublished{LearningTransferableVisualModelsNaturalRadford2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-06-10},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/LearningTransferableVisualModelsNaturalRadford2021.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/LearningTransferableVisualModelsNaturalRadford2021.pdf;/Users/brookeryan/Zotero/storage/NSUI6B4G/2103.html}
}

@inproceedings{LetChancePlayfulProbabilisticProgrammingDhariwal2020a,
  title = {Let's {{Chance}}: {{Playful Probabilistic Programming}} for {{Children}}},
  shorttitle = {Let's {{Chance}}},
  booktitle = {Extended {{Abstracts}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Dhariwal, Manuj and Dhariwal, Shruti},
  date = {2020-04-25},
  pages = {1--7},
  publisher = {{ACM}},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3334480.3383071},
  url = {https://dl.acm.org/doi/10.1145/3334480.3383071},
  urldate = {2022-04-20},
  abstract = {Probabilistic thinking has been one of the most powerful ideas in the history of science, and it is rapidly gaining even more relevance as it lies at the core of artificial intelligence (AI) systems and machine learning (ML) algorithms that are increasingly pervading our everyday lives. In this paper, we introduce Let’s Chance—a novel computational microworld that extends the widely popular Scratch Programming Language with new types of code blocks and representations that make it accessible for children to encounter and tinker with the rich ideas and sophisticated concepts of probabilistic modeling and learning. Using the tool, children can imagine and code their own expressive, playful, and personally meaningful probabilistic projects, such as—generative art, music, or text; chance-based games and stories; interactive visualizations; and even advanced projects for making a computer learn from input data using simple Markov models of probabilistic learning, among many other creative possibilities.},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-6819-3},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/LetChancePlayfulProbabilisticProgrammingDhariwal2020a.pdf}
}

@inproceedings{LetGoWhiteboardHowWhyCherubini2007,
  title = {Let's {{Go}} to the {{Whiteboard}}: {{How}} and {{Why Software Developers Use Drawings}}},
  shorttitle = {Let's Go to the Whiteboard},
  booktitle = {{{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Cherubini, Mauro and Venolia, Gina and DeLine, Rob and Ko, Amy J.},
  date = {2007},
  series = {{{CHI}} '07},
  pages = {557--566},
  doi = {10.1145/1240624.1240714},
  url = {https://doi.org/10.1145/1240624.1240714},
  urldate = {2021-10-13},
  abstract = {Software developers are rooted in the written form of their code, yet they often draw diagrams representing their code. Unfortunately, we still know little about how and why they create these diagrams, and so there is little research to inform the design of visual tools to support developers' work. This paper presents findings from semi-structured interviews that have been validated with a structured survey. Results show that most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings. Diagrams that documented design decisions were often externalized in these temporary drawings and then subsequently lost. Current visualization tools and the software development practices that we observed do not solve these issues, but these results suggest several directions for future research.},
  isbn = {978-1-59593-593-9},
  keywords = {diagrams,exploratory/field study,software visualization},
  annotation = {110 citations (Crossref) [2022-06-09]}
}

@article{LetMeGuidelinesSuccessfulOnboardingSteinmacher2018a,
  title = {Let {{Me In}}: {{Guidelines}} for the {{Successful Onboarding}} of {{Newcomers}} to {{Open Source Projects}}},
  shorttitle = {Let {{Me In}}},
  author = {Steinmacher, Igor and Treude, Christoph and Gerosa, Marco Aurelio},
  date = {2018-01-11},
  journaltitle = {IEEE Software},
  shortjournal = {IEEE Software},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/MS.2018.110162131},
  abstract = {Many community-based open source software (OSS) projects depend on a continuous influx of newcomers for their survival and continuity; yet, newcomers face many barriers to contributing to a project for the first time, leading in many cases to dropouts. In this paper, we provide guidelines for both OSS communities interested in receiving more external contributions, and newcomers who want to contribute to OSS projects. These guidelines are based on our previous work, which characterized barriers encountered by newcomers and proposed tools to support them in overcoming these barriers. Since newcomers are critical for OSS growth and continuity, our work may help increase contributions to OSS projects, as well as promote a more diverse community.},
  annotation = {26 citations (Crossref) [2022-06-09]}
}

@inproceedings{LeveragingBiometricDataBoostSoftwareFritz2016a,
  title = {Leveraging {{Biometric Data}} to {{Boost Software Developer Productivity}}},
  author = {Fritz, Thomas and Muller, Sebastian},
  date = {2016-03-01},
  pages = {66--77},
  doi = {10.1109/SANER.2016.107},
  abstract = {Producing great software requires great productive developers. Yet, what does it really mean for an individual developer to be productive, and what can we do to best help developers to be productive? To answer these questions, research has traditionally focused on measuring a developer’s output and therefore suffered from two drawbacks: the measures can only be calculated after a developer finished her work and these measures do not account for individual differences between developers. The recent advances in biometric sensor technology offer new opportunities to measure a developer’s cognitive and emotional states in real-time and thus allow us to know more about what an individual developer is currently experiencing and what might foster or impede the developer’s productivity. Results from recent research studies demonstrate the potential that biometric data has to accurately predict aspects of a developer’s work, such as perceived task and code difficulty, progress and interruptibility of a developer. This opens up new opportunities for better supporting developers in their work and, for instance, prevent bugs from entering the code, reduce costly interruptions, and foster a better and more productive work day. Our vision is that biometric sensing will be integrated into a developer’s work and that biometrics can be},
  annotation = {12 citations (Crossref) [2022-06-09]}
}

@online{Links,
  title = {Links},
  url = {https://cognitivesciencesociety.org/links/},
  urldate = {2022-08-10},
  langid = {american},
  organization = {{Cognitive Science Society}},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/8ZA2H37W/links.html}
}

@online{ListStableDiffusionSystemsWiskkey2022,
  type = {Reddit Post},
  title = {List of {{Stable Diffusion}} Systems},
  author = {Wiskkey},
  date = {2022-08-17T00:34:01},
  url = {www.reddit.com/r/StableDiffusion/comments/wqaizj/list_of_stable_diffusion_systems/},
  urldate = {2022-08-30},
  organization = {{r/StableDiffusion}},
  keywords = {AI Generated Art Educational}
}

@online{ListToolsResourcesAIArt,
  title = {List of {{Tools}} and {{Resources}} for {{AI Art}}},
  url = {https://pharmapsychotic.com/tools.html},
  urldate = {2022-05-04},
  langid = {english},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/ListToolsResourcesAIArt.md;/Users/brookeryan/Zotero/storage/LGK7N644/tools.html}
}

@inproceedings{LocalizedOpenSourceCollaborationSoftwareBuffardi2015,
  title = {Localized Open Source Collaboration in Software Engineering Education},
  booktitle = {2015 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  author = {Buffardi, Kevin},
  date = {2015-10},
  pages = {1--5},
  doi = {10.1109/FIE.2015.7344142},
  abstract = {Involving computer science students in open source software projects provides opportunities for them to contribute to real products with more authentic scope than typical computer science assignments. However, the environment of collaborating with external, distributed teams also poses unique challenges and may distance students from the potential for valuable, direct contact and mentorship from software professionals. In addition, while the technology industry continues to grow, smaller communities have a vested interest in growing a culture for collaboration between students and local software developers. We formed a local open source organization to collaborate on a product by combining efforts from students and professionals. This paper describes the localized free and open source software (LFOSS) organization and reports initial findings from software engineering students' involvement.},
  eventtitle = {2015 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  keywords = {Collaboration,Computer science,computer science education,Education,free and open source software (FOSS),Hafnium compounds,humanitarian free and open source software (HFOSS),industry collaboration,Open source software,software engineering,Software engineering,team projects},
  annotation = {10 citations (Crossref) [2022-06-09]}
}

@article{LossycontextSurprisalInformationtheoreticModelMemoryFutrell2020,
  title = {Lossy-Context Surprisal: {{An}} Information-Theoretic Model of Memory Effects in Sentence Processing},
  shorttitle = {Lossy-Context Surprisal},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger P.},
  date = {2020},
  journaltitle = {Cognitive science},
  volume = {44},
  number = {3},
  pages = {e12814},
  publisher = {{Wiley Online Library}},
  keywords = {Advisor Literature,notion}
}

@online{MachineLearningKidsb,
  title = {Machine {{Learning}} for {{Kids}}},
  url = {https://machinelearningforkids.co.uk},
  urldate = {2022-04-20},
  abstract = {An educational tool for teaching kids about machine learning, by letting them train a computer to recognise text, pictures, numbers, or sounds, and make things with it in Scratch.},
  langid = {english},
  file = {/Users/brookeryan/Zotero/storage/7T84I7J7/machinelearningforkids.co.uk.html}
}

@inproceedings{MaintainingMentalModelsStudyDeveloperLaToza2006,
  title = {Maintaining Mental Models: A Study of Developer Work Habits},
  shorttitle = {Maintaining Mental Models},
  booktitle = {Proceedings of the 28th International Conference on {{Software}} Engineering},
  author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
  date = {2006-05-28},
  series = {{{ICSE}} '06},
  pages = {492--501},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1134285.1134355},
  url = {https://doi.org/10.1145/1134285.1134355},
  urldate = {2021-10-21},
  abstract = {To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.},
  isbn = {978-1-59593-375-1},
  keywords = {agile software development,code duplication,code ownership,communication,debugging,interruptions},
  annotation = {638 citations (Semantic Scholar/DOI) [2022-06-09]}
}

@misc{MasterFineArtsSoftwareGabriel,
  title = {Master of {{Fine Arts}} in {{Software}}},
  author = {Gabriel, Richard P.},
  journaltitle = {Dreamsongs.com},
  url = {https://dreamsongs.com/MFASoftware.html},
  urldate = {2021-10-21}
}

@online{MasterFineArtsSoftwareGabriela,
  title = {Master of {{Fine Arts}} in {{Software}}},
  author = {Gabriel, Richard P.},
  url = {https://dreamsongs.com/MFASoftware.html},
  urldate = {2021-10-21},
  organization = {{Dreamsongs.com}}
}

@misc{MasterSoftwareEngineeringMSE2019,
  title = {Master of {{Software Engineering}} ({{MSE}})},
  date = {2019},
  journaltitle = {https://mswe.ics.uci.edu/},
  url = {https://mswe.ics.uci.edu/},
  urldate = {2021-10-21}
}

@online{MasterSoftwareEngineeringMSE2019a,
  title = {Master of {{Software Engineering}} ({{MSE}})},
  date = {2019},
  url = {https://mswe.ics.uci.edu/},
  urldate = {2021-10-21},
  organization = {{https://mswe.ics.uci.edu/}}
}

@inproceedings{MeasuringNeuralEfficiencyProgramComprehensionSiegmund2017,
  title = {Measuring Neural Efficiency of Program Comprehension},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Siegmund, Janet and Peitek, Norman and Parnin, Chris and Apel, Sven and Hofmeister, Johannes and Kästner, Christian and Begel, Andrew and Bethmann, Anja and Brechmann, André},
  date = {2017-08-21},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {140--150},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3106237.3106268},
  url = {https://doi.org/10.1145/3106237.3106268},
  urldate = {2021-10-21},
  abstract = {Most modern software programs cannot be understood in their entirety by a single programmer. Instead, programmers must rely on a set of cognitive processes that aid in seeking, filtering, and shaping relevant information for a given programming task. Several theories have been proposed to explain these processes, such as ``beacons,' for locating relevant code, and ``plans,'' for encoding cognitive models. However, these theories are decades old and lack validation with modern cognitive-neuroscience methods. In this paper, we report on a study using functional magnetic resonance imaging (fMRI) with 11 participants who performed program comprehension tasks. We manipulated experimental conditions related to beacons and layout to isolate specific cognitive processes related to bottom-up comprehension and comprehension based on semantic cues. We found evidence of semantic chunking during bottom-up comprehension and lower activation of brain areas during comprehension based on semantic cues, confirming that beacons ease comprehension.},
  isbn = {978-1-4503-5105-8},
  keywords = {functional magnetic resonance imaging,neural efficiency,program comprehension},
  annotation = {52 citations (Crossref) [2022-06-09]}
}

@article{MeetDALLEThatDrawsAnythingMetz2022,
  title = {Meet {{DALL-E}}, the {{A}}.{{I}}. {{That Draws Anything}} at {{Your Command}}},
  author = {Metz, Cade},
  date = {2022-04-06},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2022/04/06/technology/openai-images-dall-e.html},
  urldate = {2022-05-12},
  abstract = {New technology that blends language and images could serve graphic artists — and speed disinformation campaigns.},
  entrysubtype = {newspaper},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {AISpecialReport2022,Allen Institute for Artificial Intelligence,Artificial Intelligence,Computers and the Internet,OpenAI Labs,Research,Rumors and Misinformation},
  file = {/Users/brookeryan/Zotero/storage/ZW3H4APV/openai-images-dall-e.html}
}

@online{MGHGuidesZoteroCompleteGuideBell,
  title = {{{MGH Guides}}: {{Zotero Complete Guide}}: {{Organizing Your Library}}},
  shorttitle = {{{MGH Guides}}},
  author = {Bell, Jessica},
  url = {https://libguides.massgeneral.org/zotero/organizing-library},
  urldate = {2022-05-11},
  abstract = {The MGH IHP's complete guide to using and getting the most out of Zotero},
  langid = {english},
  keywords = {notion,Smart Notes},
  file = {/Users/brookeryan/Zotero/storage/GPN3D8NJ/organizing-library.html}
}

@article{MiceLabyrinthShowRapidLearningRosenberg2021,
  title = {Mice in a Labyrinth Show Rapid Learning, Sudden Insight, and Efficient Exploration},
  author = {Rosenberg, Matthew and Zhang, Tony and Perona, Pietro and Meister, Markus},
  date = {2021-07-01},
  journaltitle = {eLife},
  volume = {10},
  pages = {e66175},
  issn = {2050-084X},
  doi = {10.7554/eLife.66175},
  url = {https://elifesciences.org/articles/66175},
  urldate = {2022-07-25},
  abstract = {Animals learn certain complex tasks remarkably fast, sometimes after a single experience. What behavioral algorithms support this efficiency? Many contemporary studies based on two-alternative-forced-choice (2AFC) tasks observe only slow or incomplete learning. As an alternative, we study the unconstrained behavior of mice in a complex labyrinth and measure the dynamics of learning and the behaviors that enable it. A mouse in the labyrinth makes \textasciitilde 2000 navigation decisions per hour. The animal explores the maze, quickly discovers the location of a reward, and executes correct 10-bit choices after only 10 reward experiences — a learning rate 1000-fold higher than in 2AFC experiments. Many mice improve discontinuously from one minute to the next, suggesting moments of sudden insight about the structure of the labyrinth. The underlying search algorithm does not require a global memory of places visited and is largely explained by purely local turning rules.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/ENSTGEB4/Rosenberg et al. - 2021 - Mice in a labyrinth show rapid learning, sudden in.pdf}
}

@article{MICROWORLDSTRANSFORMINGEDUCATIONPaperta,
  title = {{{MICRO WORLDS}}: {{TRANSFORMING EDUCA TION}}},
  author = {Papert, Seymour},
  pages = {16},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/MICROWORLDSTRANSFORMINGEDUCATIONPaperta.pdf}
}

@online{MITCogNet,
  title = {{{MIT CogNet}}},
  url = {http://cognet.mit.edu/},
  urldate = {2022-08-10},
  keywords = {Advice}
}

@online{MmantsetsaMarope,
  title = {Mmantsetsa {{Marope}}},
  url = {https://gsolen.ucsd.edu/team/mmantsetsa-marope/},
  urldate = {2022-05-11},
  abstract = {Professor Mmantsetsa Marope is the Executive Director of The World Heritage Group, and the Honorary President of the Indian Ocean Comparative and International Education Societies. She holds a Ph.D. in Education from the University of Chicago, an MEd from Pennsylvania State University, a BA, and a CDE from the University of Botswana and Swaziland. Her […]},
  langid = {american},
  organization = {{Global Science of Learning Education Network}},
  keywords = {ObsCite},
  file = {/Users/brookeryan/Zotero/storage/V4MBDIJV/mmantsetsa-marope.html}
}

@article{ModelingInfluenceWorkingMemoryReinforcementMcDougle2021,
  title = {Modeling the Influence of Working Memory, Reinforcement, and Action Uncertainty on Reaction Time and Choice during Instrumental Learning},
  author = {McDougle, Samuel D. and Collins, Anne G. E.},
  date = {2021-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {28},
  number = {1},
  pages = {20--39},
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01774-z},
  url = {https://doi.org/10.3758/s13423-020-01774-z},
  urldate = {2022-08-13},
  abstract = {What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/L6FYU7DB/McDougle and Collins - 2021 - Modeling the influence of working memory, reinforc.pdf}
}

@article{ModelsThatLearnHowHumansDezfouli2019,
  title = {Models That Learn How Humans Learn: {{The}} Case of Decision-Making and Its Disorders},
  shorttitle = {Models That Learn How Humans Learn},
  author = {Dezfouli, Amir and Griffiths, Kristi and Ramos, Fabio and Dayan, Peter and Balleine, Bernard W.},
  date = {2019-06-11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  number = {6},
  pages = {e1006903},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006903},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006903},
  urldate = {2022-08-27},
  abstract = {Popular computational models of decision-making make specific assumptions about learning processes that may cause them to underfit observed behaviours. Here we suggest an alternative method using recurrent neural networks (RNNs) to generate a flexible family of models that have sufficient capacity to represent the complex learning and decision- making strategies used by humans. In this approach, an RNN is trained to predict the next action that a subject will take in a decision-making task and, in this way, learns to imitate the processes underlying subjects’ choices and their learning abilities. We demonstrate the benefits of this approach using a new dataset drawn from patients with either unipolar (n = 34) or bipolar (n = 33) depression and matched healthy controls (n = 34) making decisions on a two-armed bandit task. The results indicate that this new approach is better than baseline reinforcement-learning methods in terms of overall performance and its capacity to predict subjects’ choices. We show that the model can be interpreted using off-policy simulations and thereby provides a novel clustering of subjects’ learning processes—something that often eludes traditional approaches to modelling and behavioural analysis.},
  langid = {english},
  keywords = {MattarLab RNN,notion,research},
  file = {/Users/brookeryan/Zotero/storage/DSUSAFR2/Dezfouli et al. - 2019 - Models that learn how humans learn The case of de.pdf;/Users/brookeryan/Zotero/storage/4KTYYGUP/article.html}
}

@article{MultidirectionalMappingsMindsTsimaneSizePitt,
  title = {Multi-Directional Mappings in the Minds of the {{Tsimane}}’: {{Size}}, Time, and Number on Three Spatial Axes},
  author = {Pitt, Benjamin and Casasanto, Daniel and Ferrigno, Stephen and Gibson, Edward and Piantadosi, Steven T},
  pages = {7},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/BVJE8LCK/Pitt et al. - Multi-directional mappings in the minds of the Tsi.pdf}
}

@article{MultimodalNeuronsArtificialNeuralNetworksGoh2021,
  title = {Multimodal {{Neurons}} in {{Artificial Neural Networks}}},
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  date = {2021-03-04},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {3},
  pages = {e30},
  issn = {2476-0757},
  doi = {10.23915/distill.00030},
  url = {https://distill.pub/2021/multimodal-neurons},
  urldate = {2022-05-31},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  langid = {english},
  keywords = {ObsCite},
  annotation = {20 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/X635XEPB/multimodal-neurons.html}
}

@unpublished{NeuralDiscreteRepresentationLearningOord2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2018-05-30},
  number = {arXiv:1711.00937},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1711.00937},
  urldate = {2022-05-31},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/brookeryan/Zotero/storage/CRMACJXJ/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf;/Users/brookeryan/Zotero/storage/DKQJ7FLC/1711.html}
}

@misc{NeuralLanguageModelsPsycholinguisticSubjectsFutrell2019,
  title = {Neural {{Language Models}} as {{Psycholinguistic Subjects}}: {{Representations}} of {{Syntactic State}}},
  shorttitle = {Neural {{Language Models}} as {{Psycholinguistic Subjects}}},
  author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
  date = {2019-03-07},
  number = {arXiv:1903.03260},
  eprint = {1903.03260},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.03260},
  url = {http://arxiv.org/abs/1903.03260},
  urldate = {2022-08-13},
  abstract = {We deploy the methods of controlled psycholinguistic experimentation to shed light on the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we examine model behavior on artificial sentences containing a variety of syntactically complex structures. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNNG (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence that the LSTMs trained on large datasets represent syntactic state over large spans of text in a way that is comparable to the RNNG, while the LSTM trained on the small dataset does not or does so only weakly.},
  archiveprefix = {arXiv},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/FPCM24TI/Futrell et al. - 2019 - Neural Language Models as Psycholinguistic Subject.pdf;/Users/brookeryan/Zotero/storage/6X4SN2S3/1903.html}
}

@article{NeuralMarkerPerceptualConsciousnessInfantsKouider2013,
  title = {A Neural Marker of Perceptual Consciousness in Infants},
  author = {Kouider, Sid and Stahlhut, Carsten and Gelskov, Sofie V. and Barbosa, Leonardo S. and Dutat, Michel and de Gardelle, Vincent and Christophe, Anne and Dehaene, Stanislas and Dehaene-Lambertz, Ghislaine},
  options = {useprefix=true},
  date = {2013-04-19},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {340},
  number = {6130},
  eprint = {23599498},
  eprinttype = {pmid},
  pages = {376--380},
  issn = {1095-9203},
  doi = {10.1126/science.1232509},
  abstract = {Infants have a sophisticated behavioral and cognitive repertoire suggestive of a capacity for conscious reflection. Yet, demonstrating conscious access in infants remains challenging, mainly because they cannot report their thoughts. Here, to circumvent this problem, we studied whether an electrophysiological signature of consciousness found in adults, corresponding to a late nonlinear cortical response [\textasciitilde 300 milliseconds (ms)] to brief pictures, already exists in infants. We recorded event-related potentials while 5-, 12-, and 15-month-old infants (N = 80) viewed masked faces at various levels of visibility. In all age groups, we found a late slow wave showing a nonlinear profile at the expected perceptual thresholds. However, this late component shifted from a weak and delayed response in 5-month-olds (starting around 900 ms) to a more sustained and faster response in older infants (around 750 ms). These results reveal that the brain mechanisms underlying the threshold for conscious perception are already present in infancy but undergo a slow acceleration during development.},
  langid = {english},
  keywords = {Adult,Brain,COGS 269,Consciousness,Electroencephalography,Evoked Potentials,Female,Humans,Infant,Male,Neurons,notion,Perception,Perceptual Masking,Photic Stimulation},
  annotation = {106 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/ITRXXEHQ/Kouider et al. - 2013 - A neural marker of perceptual consciousness in inf.pdf}
}

@article{NeuralSubstratePredictionRewardSchultz,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P Read},
  pages = {7},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/38MAIBUD/Schultz et al. - A Neural Substrate of Prediction and Reward.pdf}
}

@article{NeuromatchAcademy3weekOnlineSummer2022,
  title = {Neuromatch {{Academy}}: A 3-Week, Online Summer School in Computational Neuroscience},
  date = {2022},
  pages = {10},
  abstract = {Neuromatch Academy (https://academy.neuromatch.io; (van Viegen et al., 2021)) was designed as an online summer school to cover the basics of computational neuroscience in three weeks. The materials cover dominant and emerging computational neuroscience tools, how they complement one another, and specifically focus on how they can help us to better understand how the brain functions. An original component of the materials is its focus on modeling choices, i.e. how do we choose the right approach, how do we build models, and how can we evaluate models to determine if they provide real (meaningful) insight. This meta-modeling component of the instructional materials asks what questions can be answered by different techniques, and how to apply them meaningfully to get insight about brain function.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/IZKLH2EW/2022 - Neuromatch Academy a 3-week, online summer school.pdf}
}

@article{NeuromatchAcademyTeachingComputationalNeurosciencevanViegen2021,
  title = {Neuromatch {{Academy}}: {{Teaching Computational Neuroscience}} with {{Global Accessibility}}},
  shorttitle = {Neuromatch {{Academy}}},
  author = {van Viegen, Tara and Akrami, Athena and Bonnen, Kathryn and DeWitt, Eric and Hyafil, Alexandre and Ledmyr, Helena and Lindsay, Grace W. and Mineault, Patrick and Murray, John D. and Pitkow, Xaq and Puce, Aina and Sedigh-Sarvestani, Madineh and Stringer, Carsen and Achakulvisut, Titipat and Alikarami, Elnaz and Atay, Melvin Selim and Batty, Eleanor and Erlich, Jeffrey C. and Galbraith, Byron V. and Guo, Yueqi and Juavinett, Ashley L. and Krause, Matthew R. and Li, Songting and Pachitariu, Marius and Straley, Elizabeth and Valeriani, Davide and Vaughan, Emma and Vaziri-Pashkam, Maryam and Waskom, Michael L. and Blohm, Gunnar and Kording, Konrad and Schrater, Paul and Wyble, Brad and Escola, Sean and Peters, Megan A.K.},
  options = {useprefix=true},
  date = {2021-07},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {7},
  pages = {535--538},
  issn = {13646613},
  doi = {10.1016/j.tics.2021.03.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661321000954},
  urldate = {2022-07-22},
  abstract = {Neuromatch Academy designed and ran a fully online 3-week Computational Neuroscience summer school for 1757 students with 191 teaching assistants working in virtual inverted (or flipped) classrooms and on small group projects. Fourteen languages, active community management, and low cost allowed for an unprecedented level of inclusivity and universal accessibility.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/5IK4II8T/van Viegen et al. - 2021 - Neuromatch Academy Teaching Computational Neurosc.pdf}
}

@article{NewcomerOSSCandidatesCharacterizingContributionsNoviceRehman2022,
  title = {Newcomer {{OSS-Candidates}}: {{Characterizing Contributions}} of {{Novice Developers}} to {{GitHub}}},
  shorttitle = {Newcomer {{OSS-Candidates}}},
  author = {Rehman, Ifraz and Wang, Dong and Kula, Raula Gaikovina and Ishio, Takashi and Matsumoto, Kenichi},
  date = {2022-05-30},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {27},
  number = {5},
  pages = {109},
  issn = {1573-7616},
  doi = {10.1007/s10664-022-10163-0},
  url = {https://doi.org/10.1007/s10664-022-10163-0},
  urldate = {2022-06-10},
  abstract = {The ability of an Open Source Software (OSS) project to attract, onboard, and retain any newcomer is vital to its livelihood. Although, evidence suggests an upsurge in novice developers joining social coding platforms (such as GitHub), the extent to which their activities result in a OSS contribution is unknown. Henceforth, we execute the protocols of a registered report to study activities of a “Newcomer OSS-Candidate”, who is a novice developer that is new to that social coding platform, and has the intention to later onboard an OSS project. Using GitHub as a case platform, we analyze 171 identified Newcomer OSS-Candidates to characterize their contribution activities. Results show that Newcomer OSS-Candidates are likely to target software based repositories (i.e., 66\%), and their first contributions are mainly associated with development (commits) and maintenance (PRs). Newcomer OSS-Candidates are less likely to practice social coding, but eventually end up onboarding (i.e., 30\% quantitative, 70\% follow-up survey) an OSS project. Furthermore, they cite finding a way to start as the most challenging barrier to contribute. Our work reveals insights on how newcomers to social coding platforms are potential sources of OSS contributions.},
  langid = {english},
  keywords = {GitHub,Newcomers,Open source software},
  file = {/Users/brookeryan/Documents/Obsidian/reference/NewcomerOSSCandidatesCharacterizingContributionsNoviceRehman2022a.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/NewcomerOSSCandidatesCharacterizingContributionsNoviceRehman2022a.pdf}
}

@article{NewcomersBarriersThatAllAnalysisBalali2018,
  title = {Newcomers’ {{Barriers}}. . . {{Is That All}}? {{An Analysis}} of {{Mentors}}’ and {{Newcomers}}’ {{Barriers}} in {{OSS Projects}}},
  shorttitle = {Newcomers’ {{Barriers}}. . . {{Is That All}}?},
  author = {Balali, Sogol and Steinmacher, Igor and Annamalai, Umayal and Sarma, Anita and Gerosa, Marco Aurelio},
  date = {2018-12},
  journaltitle = {Computer Supported Cooperative Work (CSCW)},
  volume = {27},
  number = {3},
  pages = {679--714},
  issn = {1573-7551},
  doi = {10.1007/s10606-018-9310-8},
  url = {https://doi.org/10.1007/s10606-018-9310-8},
  urldate = {2021-10-22},
  abstract = {Newcomers’ seamless onboarding is important for open collaboration communities, particularly those that leverage outsiders’ contributions to remain sustainable. Nevertheless, previous work shows that OSS newcomers often face several barriers to contribute, which lead them to lose motivation and even give up on contributing. A well-known way to help newcomers overcome initial contribution barriers is mentoring. This strategy has proven effective in offline and online communities, and to some extent has been employed in OSS projects. Studying mentors’ perspectives on the barriers that newcomers face play a vital role in improving onboarding processes; yet, OSS mentors face their own barriers, which hinder the effectiveness of the strategy. Since little is known about the barriers mentors face, in this paper, we investigate the barriers that affect mentors and their newcomer mentees. We interviewed mentors from OSS projects and qualitatively analyzed their answers. We found 44 barriers: 19 that affect mentors; and 34 that affect newcomers (9 affect both newcomers and mentors). Interestingly, most of the barriers we identified (66\%) have a social nature. Additionally, we identified 10 strategies that mentors indicated to potentially alleviate some of the barriers. Since gender-related challenges emerged in our analysis, we conducted nine follow-up structured interviews to further explore this perspective. The contributions of this paper include: identifying the barriers mentors face; bringing the unique perspective of mentors on barriers faced by newcomers; unveiling strategies that can be used by mentors to support newcomers; and investigating gender-specific challenges in OSS mentorship. Mentors, newcomers, online communities, and educators can leverage this knowledge to foster new contributors to OSS projects.},
  langid = {english},
  annotation = {32 citations (Crossref) [2022-06-09]}
}

@book{NewOpportunitiesInterestDrivenArtsLearningPeppler2013,
  title = {New {{Opportunities}} for {{Interest-Driven Arts Learning}} in a {{Digital Age}}},
  author = {Peppler, Kylie},
  date = {2013-07},
  journaltitle = {Wallace Foundation},
  publisher = {{Wallace Foundation}},
  abstract = {Arts may be scarce in many public schools, especially in disadvantaged communities. But outside school, one sees a "strikingly different landscape," according to this report. Why? Digital technologies are offering young people new ways to engage in the arts on their own time and according to their own interests. The report describes the new technologies, young people's media use and a framework for thinking about "interest-driven" arts learning. Three appendices include: (1) Communities that support interest-driven digital arts learning; (2) Apps that support interest-driven digital arts learning; and (3) Online platforms that support interest-driven digital arts learning.},
  langid = {english},
  keywords = {Adolescents,Art Activities,Art Education,Communities of Practice,Computer Oriented Programs,Computer Use,Creative Activities,Information Technology,Interests,Internet,Learning,Mass Media Use,Participation}
}

@online{NightCafea,
  title = {{{NightCafe}}},
  url = {https://creator.nightcafe.studio/},
  urldate = {2022-05-04},
  abstract = {AI Art Generator App. \&\#9989; Fast \&\#9989; Free \&\#9989; Easy. Create amazing artworks using artificial intelligence.},
  langid = {english},
  organization = {{NightCafe Creator}},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/NightCafea.md;/Users/brookeryan/Zotero/storage/I455QQWH/  AI Art Generator, AI Art Maker.html}
}

@misc{NovelHiddenMarkovApproachStudyingHussain2022,
  title = {A Novel Hidden {{Markov}} Approach to Studying Dynamic Functional Connectivity States in Human Neuroimaging},
  author = {Hussain, Sana and Langley, Jason and Seitz, Aaron R. and Hu, Xiaoping P. and Peters, Megan A. K.},
  date = {2022-04-30},
  pages = {2022.02.02.478844},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.02.02.478844},
  url = {https://www.biorxiv.org/content/10.1101/2022.02.02.478844v3},
  urldate = {2022-07-22},
  abstract = {Introduction Hidden Markov models are a popular choice to extract and examine recurring patterns of activity or functional connectivity in neuroimaging data, both in terms of spatial patterns and their temporal progression. Although many diverse hidden Markov models have been applied to neuroimaging data, most have defined states based on activity levels (intensity-based states) rather than patterns of functional connectivity between brain areas (connectivity-based states), which is problematic if we want to understand connectivity dynamics: intensity-based states are unlikely to provide useful information about dynamic connectivity patterns. Methods We addressed this problem by introducing a new hidden Markov model that defines states based on full functional connectivity profiles among brain regions. We empirically explored the behavior of this new model in comparison to existing approaches based on intensity-based or summed functional connectivity states using the HCP unrelated 100 functional magnetic resonance imaging “resting state” dataset. Results Our ‘full functional connectivity’ model discovered connectivity states with more distinguishable patterns than previous approaches, and recovered simulated connectivity-based states more faithfully. Discussion Thus, if our goal is to extract and interpret connectivity states in neuroimaging data, our new model outperforms previous methods which miss crucial information about the evolution of functional connectivity in the brain. Impact statement Hidden Markov models can be used to investigate brain states noninvasively. Previous models “recover” connectivity from intensity-based hidden states, or from connectivity ‘summed’ across nodes. Here we introduce a novel connectivity-based hidden Markov model and show how it can reveal true connectivity hidden states under minimal assumptions.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/XYABST7V/Hussain et al. - 2022 - A novel hidden Markov approach to studying dynamic.pdf;/Users/brookeryan/Zotero/storage/M6IVITYA/2022.02.02.html}
}

@misc{NoveltyUncertaintyInteractRegulateBalanceCockburn2021,
  title = {Novelty and Uncertainty Interact to Regulate the Balance between Exploration and Exploitation in the Human Brain},
  author = {Cockburn, Jeffrey and Man, Vincent and Cunningham, William and O’Doherty, John P.},
  date = {2021-10-14},
  pages = {2021.10.13.464279},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.10.13.464279},
  url = {https://www.biorxiv.org/content/10.1101/2021.10.13.464279v1},
  urldate = {2022-07-25},
  abstract = {Recent evidence suggests that both novelty and uncertainty act as potent features guiding exploration. However, these variables are often conflated with each other experimentally, and an understanding of how these attributes interact to regulate the balance between exploration and exploitation has proved elusive. Using a novel task designed to decouple stimulus novelty and estimation uncertainty, we identify separable behavioral and neural mechanisms by which exploration is colored. We show that uncertainty was avoided except when the information gained through exploration could be reliably exploited in the future. In contrast, and contrary to existing theory, novel options grew increasingly attractive relative to familiar counterparts irrespective of the opportunity to leverage their consequences and despite the uncertainty inherent to novel options. These findings led us to develop a formal computational framework in which uncertainty directed choice adapts to the prospective utility of exploration, while novel stimuli persistently draw favor as a result of inflated reward expectations biasing an exploitative strategy. Crucially, novelty is proposed to actively modulate uncertainty processing, effectively blunting the influence of uncertainty in shaping the subjective utility ascribed to novel stimuli. Both behavioral data and fMRI activity sampled from the ventromedial prefrontal cortex, frontopolar cortex and ventral striatum validate this model, thereby establishing a computational account that can not only explain behavior but also shed light on the functional contribution of these key brain regions to the exploration/exploitation trade-off. Our results point to multiple strategies and neural substrates charged with balancing the explore/exploit dilemma, with each targeting distinct aspects of the decision problem to foster a manageable decomposition of an otherwise intractable task.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/EUQCNQVD/Cockburn et al. - 2021 - Novelty and uncertainty interact to regulate the b.pdf;/Users/brookeryan/Zotero/storage/WGWILYSH/2021.10.13.html}
}

@inproceedings{NoviceSoftwareDevelopersAllAgainBegel2008,
  title = {Novice Software Developers, All over Again},
  booktitle = {Proceedings of the {{Fourth}} International {{Workshop}} on {{Computing Education Research}}},
  author = {Begel, Andrew and Simon, Beth},
  date = {2008-09},
  series = {{{ICER}} '08},
  pages = {3--14},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1404520.1404522},
  url = {https://doi.org/10.1145/1404520.1404522},
  urldate = {2021-10-21},
  abstract = {Transitions from novice to expert often cause stress and anxiety and require specialized instruction and support to enact efficiently. While many studies have looked at novice computer science students, very little research has been conducted on professional novices. We conducted a two-month in-situ qualitative case study of new software developers in their first six months working at Microsoft. We shadowed them in all aspects of their jobs: coding, debugging, designing, and engaging with their team, and analyzed the types of tasks in which they engage. We can explain many of the behaviors revealed by our analyses if viewed through the lens of newcomer socialization from the field of organizational man-agement. This new perspective also enables us to better understand how current computer science pedagogy prepares students for jobs in the software industry. We consider the implications of this data and analysis for developing new processes for learning in both university and industrial settings to help accelerate the transition from novice to expert software developer.},
  isbn = {978-1-60558-216-0},
  keywords = {computer science pedagogy,human aspects of software engineering,software development,training},
  annotation = {84 citations (Crossref) [2022-06-09]}
}

@misc{NumberSenseEmergentPropertyManipulatingKondapaneni2021,
  title = {A {{Number Sense}} as an {{Emergent Property}} of the {{Manipulating Brain}}},
  author = {Kondapaneni, Neehar and Perona, Pietro},
  date = {2021-02-22},
  number = {arXiv:2012.04132},
  eprint = {2012.04132},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.04132},
  urldate = {2022-07-25},
  abstract = {The ability to understand and manipulate numbers and quantities emerges during childhood, but the mechanism through which humans acquire and develop this ability is still poorly understood. In particular, it is not known whether acquiring such a number sense is possible without supervision from a teacher.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/T5TTV6LT/Kondapaneni and Perona - 2021 - A Number Sense as an Emergent Property of the Mani.pdf}
}

@inproceedings{OnboardingProgramsWorkLabuschagne2015,
  title = {Do Onboarding Programs Work?},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Labuschagne, Adriaan and Holmes, Reid},
  date = {2015},
  pages = {381--385},
  publisher = {{IEEE}}
}

@online{OneModelLearningLanguagepiantadosi,
  title = {One Model for the Learning of Language},
  author = {{piantadosi}},
  doi = {10.1073/pnas.2021865119},
  url = {https://www.pnas.org/doi/10.1073/pnas.2021865119},
  urldate = {2022-08-10},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/AIG5VAJ9/One model for the learning of language.pdf}
}

@article{OneModelLearningLanguageYang2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  date = {2022-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {119},
  number = {5},
  pages = {e2021865119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021865119},
  url = {https://pnas.org/doi/full/10.1073/pnas.2021865119},
  urldate = {2022-08-09},
  abstract = {Significance             It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.           ,                             A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,                                a                 n                              ,                                                                                                                        (                         a                         b                         )                                              n                                                                                       , and                                                                                                                        \{                         a                         ,                         b                         \}                                              +                                                                                       ), context-free (e.g.,                                                                                               a                       n                                                                 b                       n                                          ,                     \,                                            a                       n                                                                 b                                                n                         +                         m                                                                                                              , and                                                                        x                                            x                       R                                                                                       ), and context-sensitive (e.g.,                                                                                               a                       n                                                                 b                       n                                                                 c                       n                                          ,                     \,                                            a                       n                                                                 b                       m                                                                 c                       n                                                                 d                       m                                                                                       , and               xx               ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  langid = {english},
  keywords = {Advisor Literature},
  file = {/Users/brookeryan/Zotero/storage/ISIM7UWC/Yang and Piantadosi - 2022 - One model for the learning of language.pdf}
}

@inproceedings{OpenRepositoryTeachingSoftwareModelingSilva2020,
  title = {Towards an {{Open Repository}} for {{Teaching Software Modeling}} Applying {{Active Learning Strategies}}},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  author = {Silva, Williamson and Gadelha, Bruno and Steinmacher, Igor and Conte, Tayana},
  date = {2020-10},
  pages = {162--172},
  abstract = {Modeling is a core topic in Software Engineering Education. Nevertheless, students face difficulties while learning software modeling. To teach software modeling effectively in computing courses, instructors who usually employed traditional methods could use active learning strategies. However, instructors are reluctant to change their teaching approaches due to several barriers that hinder the application of active learning strategies. Besides, relatively little research addresses how to mitigate them. The objective of this research is to help instructors implementing active learning strategies when teaching software modeling with UML diagrams. To achieve this objective, we conducted a Design Science Research (DSR). We proposed an artifact called OpenSMALS, an Open Repository for teaching Software Modeling applying Active Learning Strategies. OpenSMALS provides specific guidelines on how instructors can apply these strategies and helping instructors to identify the active learning strategies best suit their teaching context. We performed four DSR Design Cycles-in four different universities-to evaluate and evolve OpenSMALS. Our results show that OpenSMALS satisfactorily reduced the barriers faced by instructors, and it achieved an appropriate maturity level to be adopted by other instructors.},
  eventtitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  keywords = {Active Learning Strategies,Computational modeling,Education,Faces,Knowledge based systems,Modeling Education,Software,Software engineering,Software Engineering Education,UML,Unified modeling language}
}

@article{OriginsShapeBiasEvidenceTsimaneJara-Ettinger2022,
  title = {The Origins of the Shape Bias: {{Evidence}} from the {{Tsimane}}’},
  shorttitle = {The Origins of the Shape Bias},
  author = {Jara-Ettinger, Julian and Levy, Roger and Sakel, Jeanette and Huanca, Tomas and Gibson, Edward},
  date = {2022},
  journaltitle = {Journal of Experimental Psychology: General},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0001195},
  abstract = {In the United States, children often generalize the meaning of new words by assuming that objects with the same shape have the same name. We propose that this shape bias is influenced by children’s exposure to objects of different categories (artifacts and natural kinds) and language to talk about them. We present a cross-cultural study between English speakers in the United States and Tsimane’ speakers in the Bolivian Amazon. We found that U.S. children and adults were more likely to generalize novel labels by shape rather than by material or color, relative to Tsimane’ participants. Critically, Tsimane’ children and adults systematically avoided generalizing labels to objects that shared no common features with the novel referent. Our results provide initial evidence that the relative exposure to objects of different kinds and language to talk about them can lead to cross-cultural differences on object name learning. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/C4TEBTLI/Jara-Ettinger et al. - 2022 - The origins of the shape bias Evidence from the T.pdf}
}

@inproceedings{PABLOHelpingNovicesDebugPythonCosman2020,
  title = {{{PABLO}}: {{Helping Novices Debug Python Code Through Data-Driven Fault Localization}}},
  shorttitle = {{{PABLO}}},
  booktitle = {Proceedings of the 51st {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Cosman, Benjamin and Endres, Madeline and Sakkas, Georgios and Medvinsky, Leon and Yang, Yao-Yuan and Jhala, Ranjit and Chaudhuri, Kamalika and Weimer, Westley},
  date = {2020-02-26},
  series = {{{SIGCSE}} '20},
  pages = {1047--1053},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3328778.3366860},
  url = {https://doi.org/10.1145/3328778.3366860},
  urldate = {2021-09-27},
  abstract = {As dynamically-typed languages grow in popularity, especially among beginning programmers, there is an increased need to pinpoint their defects. Localization for novice bugs can be ambiguous: not all locations formally implicated are equally useful for beginners. We propose a scalable fault localization approach for dynamic languages that is helpful for debugging and generalizes to handle a wide variety of errors commonly faced by novice programmers. We base our approach on a combination of static, dynamic, and contextual features, guided by machine learning. We evaluate on over 980,000 diverse real user interactions across four years from the popular PythonTutor.com website, which is used both in classes and by non-traditional learners. We find that our approach is scalable, general, and quite accurate: up to 77\% of these historical novice users would have been helped by our top-three responses, compared to 45\% for the default interpreter. We also conducted a human study: participants preferred our approach to the baseline (\$p = 0.018)\$, and found it additionally useful for bugs meriting multiple edits.},
  isbn = {978-1-4503-6793-6},
  keywords = {debugging,fault localization,machine learning},
  annotation = {1 citations (Crossref) [2022-06-09]}
}

@report{PacmanWilmer2010,
  title = {Pacman},
  author = {Wilmer, D. W. H. and De Ridder, G. R. and Kol, A. A. and Harkes, D. C.},
  date = {2010},
  url = {https://repository.tudelft.nl/islandora/object/uuid%3A8bc685df-fcf4-4ed8-8ef3-a11e75f82c76},
  urldate = {2021-10-21},
  abstract = {This bachelor report describes the creation of a fun and informative robot game for the Science Centre Delft. The project entails simulating the classic Pacman computer game using real robots. One robot, Pacman, is controlled by a player and has to fulfil certain assignments. Simultaneously the other robots, monsters, try to catch Pacman. A camera captures the movements of all the robots and with the help of image processing the camera input is used to run a control program. The same image is also used to display information on monitors. This gives the player and other visitors an insight in the inner workings of the game.},
  langid = {english}
}

@online{PeopleCenterHumanCompatibleArtificialIntelligence,
  title = {People – {{Center}} for {{Human-Compatible Artificial Intelligence}}},
  url = {https://humancompatible.ai/people/},
  urldate = {2022-08-10},
  langid = {american},
  keywords = {Advice},
  file = {/Users/brookeryan/Zotero/storage/9CZS2TIM/people.html}
}

@article{PerformanceoptimizedHierarchicalModelsPredictNeuralYamins2014,
  title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
  author = {Yamins, D. L. K. and Hong, H. and Cadieu, C. F. and Solomon, E. A. and Seibert, D. and DiCarlo, J. J.},
  date = {2014-06-10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {23},
  pages = {8619--8624},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403112111},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1403112111},
  urldate = {2021-10-03},
  langid = {english},
  keywords = {Literature Review,notion},
  annotation = {884 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/EQDS7E9V/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf}
}

@online{PhDAdmissionsEmailingPotentialAdvisorsenricab72020,
  title = {{{PhD Admissions}}: {{Emailing Potential Advisors}}},
  shorttitle = {{{PhD Admissions}}},
  author = {{enricab7}},
  date = {2020-07-24T17:35:05+00:00},
  url = {https://first-gen-guide.com/2020/07/24/phd-admissions-emailing-potential-advisors/},
  urldate = {2022-08-10},
  abstract = {A great way to initiate contact with a potential advisor is by introducing yourself through email! If you are applying to programs where you don’t need an advisor lined up, then you don\&\#8217…},
  langid = {english},
  organization = {{A First-Gen's Guide to Grad School: How to Get in, Survive, and Thrive}},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/88SQ6P7K/phd-admissions-emailing-potential-advisors.html}
}

@inproceedings{PhysicalProgrammingDesigningToolsChildrenMontemayor2002a,
  title = {Physical Programming: {{Designing}} Tools for Children to Create Physical Interactive Environments},
  shorttitle = {Physical Programming},
  author = {Montemayor, Jaime and Druin, Allison and Farber, Allison and Simms, Sante and Churaman, Wayne and D'Amour, Allison},
  date = {2002-01-01},
  volume = {4},
  pages = {299--306},
  doi = {10.1145/503376.503430},
  abstract = {Physical interactive environments can come in many forms: museum installations, amusement parks, experimental theaters, and more. Programming these environments has historically been done by adults, and children, as the visiting participants, have been offered few pre-created choices to explore. Given these creative limitations, the goal of our research has been to develop programming tools for physical interactive environments that are appropriate for use by young children (ages 4-6). We have explored numerous design approaches over the past two years. Recently we began focusing on a "physical programming" approach and developed a wizard-of-oz prototype for young children. This paper presents the motivation for this research, the evolution of our programming approach, and our recent explorations with children},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/PhysicalProgrammingDesigningToolsChildrenMontemayor2002a.pdf}
}

@unpublished{PhysionEvaluatingPhysicalPredictionVisionBear2021,
  title = {Physion: {{Evaluating Physical Prediction}} from {{Vision}} in {{Humans}} and {{Machines}}},
  shorttitle = {Physion},
  author = {Bear, Daniel M. and Wang, Elias and Mrowca, Damian and Binder, Felix J. and Tung, Hsiau-Yu Fish and Pramod, R. T. and Holdaway, Cameron and Tao, Sirui and Smith, Kevin and Sun, Fan-Yun and Fei-Fei, Li and Kanwisher, Nancy and Tenenbaum, Joshua B. and Yamins, Daniel L. K. and Fan, Judith E.},
  date = {2021-06-17},
  eprint = {2106.08261},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.08261},
  urldate = {2021-10-22},
  abstract = {While machine learning algorithms excel at many challenging visual tasks, it is unclear that they can make predictions about commonplace real world physical events. Here, we present a visual and physical prediction benchmark that precisely measures this capability. In realistically simulating a wide variety of physical phenomena -- rigid and soft-body collisions, stable multi-object configurations, rolling and sliding, projectile motion -- our dataset presents a more comprehensive challenge than existing benchmarks. Moreover, we have collected human responses for our stimuli so that model predictions can be directly compared to human judgments. We compare an array of algorithms -- varying in their architecture, learning objective, input-output structure, and training data -- on their ability to make diverse physical predictions. We find that graph neural networks with access to the physical state best capture human behavior, whereas among models that receive only visual input, those with object-centric representations or pretraining do best but fall far short of human accuracy. This suggests that extracting physically meaningful representations of scenes is the main bottleneck to achieving human-like visual prediction. We thus demonstrate how our benchmark can identify areas for improvement and measure progress on this key aspect of physical understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.4.8,I.5,Literature Review,notion},
  file = {/Users/brookeryan/Zotero/storage/79A9KGWG/Bear et al. - 2021 - Physion Evaluating Physical Prediction from Visio.pdf;/Users/brookeryan/Zotero/storage/JQ262EXM/2106.html}
}

@misc{PhysionEvaluatingPhysicalPredictionVisionBear2022,
  title = {Physion: {{Evaluating Physical Prediction}} from {{Vision}} in {{Humans}} and {{Machines}}},
  shorttitle = {Physion},
  author = {Bear, Daniel M. and Wang, Elias and Mrowca, Damian and Binder, Felix J. and Tung, Hsiao-Yu Fish and Pramod, R. T. and Holdaway, Cameron and Tao, Sirui and Smith, Kevin and Sun, Fan-Yun and Fei-Fei, Li and Kanwisher, Nancy and Tenenbaum, Joshua B. and Yamins, Daniel L. K. and Fan, Judith E.},
  date = {2022-06-20},
  number = {arXiv:2106.08261},
  eprint = {2106.08261},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.08261},
  url = {http://arxiv.org/abs/2106.08261},
  urldate = {2022-08-08},
  abstract = {While current vision algorithms excel at many challenging tasks, it is unclear how well they understand the physical dynamics of real-world environments. Here we introduce Physion, a dataset and benchmark for rigorously evaluating the ability to predict how physical scenarios will evolve over time. Our dataset features realistic simulations of a wide range of physical phenomena, including rigid and soft-body collisions, stable multi-object configurations, rolling, sliding, and projectile motion, thus providing a more comprehensive challenge than previous benchmarks. We used Physion to benchmark a suite of models varying in their architecture, learning objective, input-output structure, and training data. In parallel, we obtained precise measurements of human prediction behavior on the same set of scenarios, allowing us to directly evaluate how well any model could approximate human behavior. We found that vision algorithms that learn object-centric representations generally outperform those that do not, yet still fall far short of human performance. On the other hand, graph neural networks with direct access to physical state information both perform substantially better and make predictions that are more similar to those made by humans. These results suggest that extracting physical representations of scenes is the main bottleneck to achieving human-level and human-like physical understanding in vision algorithms. We have publicly released all data and code to facilitate the use of Physion to benchmark additional models in a fully reproducible manner, enabling systematic evaluation of progress towards vision algorithms that understand physical environments as robustly as people do.},
  archiveprefix = {arXiv},
  keywords = {Advisor Literature},
  file = {/Users/brookeryan/Zotero/storage/ZW9IRUYS/Bear et al. - 2022 - Physion Evaluating Physical Prediction from Visio.pdf;/Users/brookeryan/Zotero/storage/FLAIGKIV/2106.html}
}

@article{PlanningBrainMattar2022,
  title = {Planning in the Brain},
  author = {Mattar, Marcelo G. and Lengyel, Máté},
  date = {2022-03-16},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {110},
  number = {6},
  pages = {914--934},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.12.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321010357},
  urldate = {2022-08-05},
  abstract = {Recent breakthroughs in artificial intelligence (AI) have enabled machines to plan in tasks previously thought to be uniquely human. Meanwhile, the planning algorithms implemented by the brain itself remain largely unknown. Here, we review neural and behavioral data in sequential decision-making tasks that elucidate the ways in which the brain does—and does not—plan. To systematically review available biological data, we create a taxonomy of planning algorithms by summarizing the relevant design choices for such algorithms in AI. Across species, recording techniques, and task paradigms, we find converging evidence that the brain represents future states consistent with a class of planning algorithms within our taxonomy—focused, depth-limited, and serial. However, we argue that current data are insufficient for addressing more detailed algorithmic questions. We propose a new approach leveraging AI advances to drive experiments that can adjudicate between competing candidate algorithms.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/KPSTZ6KJ/Mattar and Lengyel - 2022 - Planning in the brain.pdf;/Users/brookeryan/Zotero/storage/7IRANAMV/S0896627321010357.html}
}

@inproceedings{PlayerAIInteractionWhatNeuralNetworkZhu2021,
  title = {Player-{{AI Interaction}}: {{What Neural Network Games Reveal About AI}} as {{Play}}},
  shorttitle = {Player-{{AI Interaction}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhu, Jichen and Villareale, Jennifer and Javvaji, Nithesh and Risi, Sebastian and Löwe, Mathias and Weigelt, Rush and Harteveld, Casper},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445307},
  url = {https://doi.org/10.1145/3411764.3445307},
  urldate = {2022-08-29},
  abstract = {The advent of artificial intelligence (AI) and machine learning (ML) bring human-AI interaction to the forefront of HCI research. This paper argues that games are an ideal domain for studying and experimenting with how humans interact with AI. Through a systematic survey of neural network games (n = 38), we identified the dominant interaction metaphors and AI interaction patterns in these games. In addition, we applied existing human-AI interaction guidelines to further shed light on player-AI interaction in the context of AI-infused systems. Our core finding is that AI as play can expand current notions of human-AI interaction, which are predominantly productivity-based. In particular, our work suggests that game and UX designers should consider flow to structure the learning curve of human-AI interaction, incorporate discovery-based learning to play around with the AI and observe the consequences, and offer users an invitation to play to explore new forms of human-AI interaction.},
  isbn = {978-1-4503-8096-6},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/UW5FIEKM/Zhu et al. - 2021 - Player-AI Interaction What Neural Network Games R.pdf}
}

@article{PopBotsDesigningArtificialIntelligenceCurriculumWilliams2019b,
  title = {{{PopBots}}: {{Designing}} an {{Artificial Intelligence Curriculum}} for {{Early Childhood Education}}},
  shorttitle = {{{PopBots}}},
  author = {Williams, Randi and Park, Hae Won and Oh, Lauren and Breazeal, Cynthia},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {9729--9736},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33019729},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/5040},
  urldate = {2022-06-09},
  abstract = {PopBots is a hands-on toolkit and curriculum designed to help young children learn about artificial intelligence (AI) by building, programming, training, and interacting with a social robot. Today’s children encounter AI in the forms of smart toys and computationally curated educational and entertainment content. However, children have not yet been empowered to understand or create with this technology. Existing computational thinking platforms have made ideas like sequencing and conditionals accessible to young learners. Going beyond this, we seek to make AI concepts accessible. We designed PopBots to address the specific learning needs of children ages four to seven by adapting constructionist ideas into an AI curriculum. This paper describes how we designed the curriculum and evaluated its effectiveness with 80 Pre-K and Kindergarten children. We found that the use of a social robot as a learning companion and programmable artifact was effective in helping young children grasp AI concepts. We also identified teaching approaches that had the greatest impact on student’s learning. Based on these, we make recommendations for future modules and iterations for the PopBots platform.},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/PopBotsDesigningArtificialIntelligenceCurriculumWilliams2019c.pdf}
}

@online{Post,
  title = {Post},
  url = {https://womenincocosci.com/posts/what_is_a_labtech.html},
  urldate = {2022-07-22},
  keywords = {Lab Assistant Position,notion},
  file = {/Users/brookeryan/Zotero/storage/EPIJZV9T/what_is_a_labtech.html}
}

@online{Posta,
  title = {Post},
  url = {https://womenincocosci.com/posts/what_are_questions_to_ask_grad_students_during_interviews.html},
  urldate = {2022-08-05},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/VCJ2Y2NB/what_are_questions_to_ask_grad_students_during_interviews.html}
}

@online{Postb,
  title = {Post},
  url = {https://womenincocosci.com/posts/how_to_pick_a_supervisor.html},
  urldate = {2022-08-05},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/85R8TCD6/how_to_pick_a_supervisor.html}
}

@online{Postc,
  title = {Post},
  url = {https://womenincocosci.com/posts/how_to_apply_to_graduate_school.html},
  urldate = {2022-08-05},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/D77T32XM/how_to_apply_to_graduate_school.html}
}

@article{PowerPeopleRoleHumansInteractiveAmershi2014b,
  title = {Power to the {{People}}: {{The Role}} of {{Humans}} in {{Interactive Machine Learning}}},
  shorttitle = {Power to the {{People}}},
  author = {Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  date = {2014-12-22},
  journaltitle = {AI Magazine},
  shortjournal = {AIMag},
  volume = {35},
  number = {4},
  pages = {105--120},
  issn = {2371-9621, 0738-4602},
  doi = {10.1609/aimag.v35i4.2513},
  url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2513},
  urldate = {2022-06-09},
  abstract = {Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.}
}

@misc{PredecessorFeaturesBailey2022,
  title = {Predecessor {{Features}}},
  author = {Bailey, Duncan and Mattar, Marcelo G.},
  date = {2022-07-25},
  number = {arXiv:2206.00303},
  eprint = {2206.00303},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.00303},
  urldate = {2022-08-05},
  abstract = {Any reinforcement learning system must be able to identify which past events contributed to observed outcomes, a problem known as credit assignment. A common solution to this problem is to use an eligibility trace to assign credit to recency-weighted set of experienced events. However, in many realistic tasks, the set of recently experienced events are only one of the many possible action events that could have preceded the current outcome. This suggests that reinforcement learning can be made more efficient by allowing credit assignment to any viable preceding state, rather than only those most recently experienced. Accordingly, we examine “Predecessor Features”, the fully bootstrapped version of van Hasselt’s “Expected Trace”, an algorithm that achieves this richer form of credit assignment. By maintaining a representation that approximates the expected sum of past occupancies, this algorithm allows temporal difference (TD) errors to be propagated accurately to a larger number of predecessor states than conventional methods, greatly improving learning speed. The algorithm can also be naturally extended from tabular state representation to feature representations allowing for increased performance on a wide range of environments. We demonstrate several use cases for Predecessor Features and compare its performance with other approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/9RA86VAZ/Bailey and Mattar - 2022 - Predecessor Features.pdf}
}

@article{PrioritizedMemoryAccessExplainsPlanningMattar2018,
  title = {Prioritized Memory Access Explains Planning and Hippocampal Replay},
  author = {Mattar, Marcelo G. and Daw, Nathaniel D.},
  date = {2018-11},
  journaltitle = {Nature neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {11},
  eprint = {30349103},
  eprinttype = {pmid},
  pages = {1609--1617},
  issn = {1097-6256},
  doi = {10.1038/s41593-018-0232-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6203620/},
  urldate = {2022-08-05},
  abstract = {To make decisions, animals must evaluate candidate choices by accessing memories of relevant experiences. Yet little is known about which experiences are considered or ignored during deliberation, which ultimately governs choice. We propose a normative theory predicting which memories should be accessed at each moment to optimize future decisions. Using nonlocal “replay” of spatial locations in hippocampus as a window into memory access, we simulate a spatial navigation task where an agent accesses memories of locations sequentially, ordered by utility: how much extra reward would be earned due to better choices. This prioritization balances two desiderata: the need to evaluate imminent choices, vs. the gain from propagating newly encountered information to preceding locations. Our theory offers a simple explanation for numerous findings about place cells; unifies seemingly disparate proposed functions of replay including planning, learning, and consolidation; and posits a mechanism whose dysfunction may underlie pathologies like rumination and craving.},
  pmcid = {PMC6203620},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/BCM84VGM/Mattar and Daw - 2018 - Prioritized memory access explains planning and hi.pdf}
}

@inproceedings{ProductionProgrammingClassroomAllen2003,
  title = {Production Programming in the Classroom},
  booktitle = {Proceedings of the 34th {{SIGCSE}} Technical Symposium on {{Computer}} Science Education},
  author = {Allen, Eric and Cartwright, Robert and Reis, Charles},
  date = {2003-01},
  series = {{{SIGCSE}} '03},
  pages = {89--93},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/611892.611940},
  url = {https://doi.org/10.1145/611892.611940},
  urldate = {2021-10-22},
  abstract = {Students in programming courses generally write "toy" programs that are superficially tested, graded, and then discarded. This approach to teaching programming leaves students unprepared for production programming because the gap between writing toy programs and developing reliable software products is enormous.This paper describes how production programming can be effectively taught to undergraduate students in the classroom. The key to teaching such a course is using Extreme Programming methodology to develop a sustainable open source project with real customers, including the students themselves. Extreme Programming and open source project management are facilitated by a growing collection of free tools such as the JUnit testing framework, the Ant scripting tool, and the SourceForge website for managing open source projects.},
  isbn = {978-1-58113-648-7},
  keywords = {ant,DrJava,extreme programming,JUnit,open source,production programming,software engineering,sourceforge},
  annotation = {22 citations (Crossref) [2022-06-09]}
}

@online{ProductivityToolsUseEndorseFelix,
  title = {Productivity Tools {{I}} Use \& Endorse · {{Felix Binder}}},
  url = {https://ac.felixbinder.net/writings/2020/07/01/the-stack.html},
  urldate = {2022-05-18},
  keywords = {notion,Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/73DVAP4Z/the-stack.html}
}

@inproceedings{ProgramComprehensionPresentFutureSiegmund2016,
  title = {Program {{Comprehension}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Program {{Comprehension}}},
  booktitle = {2016 {{IEEE}} 23rd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Siegmund, Janet},
  date = {2016-03},
  volume = {5},
  pages = {13--20},
  doi = {10.1109/SANER.2016.35},
  abstract = {Program comprehension is the main activity of the software developers. Although there has been substantial research to support the programmer, the high amount of time developers need to understand source code remained constant over thirty years. Beside more complex software, what might be the reason? In this paper, I explore the past of program-comprehension research, discuss the current state, and outline what future research on program comprehension might bring.},
  keywords = {Atmospheric measurements,Computer languages,empirical software engineering,Particle measurements,Program comprehension,Programming,Protocols,Software,Time factors},
  annotation = {26 citations (Crossref) [2022-06-09]}
}

@inproceedings{ProgrammingProblemSolvingSelfAwarenessEffectsLoksa2016,
  title = {Programming, {{Problem Solving}}, and {{Self-Awareness}}: {{Effects}} of {{Explicit Guidance}}},
  shorttitle = {Programming, {{Problem Solving}}, and {{Self-Awareness}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Loksa, Dastyni and Ko, Amy J. and Jernigan, Will and Oleson, Alannah and Mendez, Christopher J. and Burnett, Margaret M.},
  date = {2016-05},
  series = {{{CHI}} '16},
  pages = {1449--1461},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858252},
  url = {https://doi.org/10.1145/2858036.2858252},
  urldate = {2021-10-21},
  abstract = {More people are learning to code than ever, but most learning opportunities do not explicitly teach the problem solving skills necessary to succeed at open-ended programming problems. In this paper, we present a new approach to impart these skills, consisting of: 1) explicit instruction on programming problem solving, which frames coding as a process of translating mental representations of problems and solutions into source code, 2) a method of visualizing and monitoring progression through six problem solving stages, 3) explicit, on-demand prompts for learners to reflect on their strategies when seeking help from instructors, and 4) context-sensitive help embedded in a code editor that reinforces the problem solving instruction. We experimentally evaluated the effects of our intervention across two 2-week web development summer camps with 48 high school students, finding that the intervention increased productivity, independence, programming self-efficacy, metacognitive awareness, and growth mindset. We discuss the implications of these results on learning technologies and classroom instruction.},
  isbn = {978-1-4503-3362-7},
  keywords = {computer science education,metacognition,problem-solving,programming},
  annotation = {64 citations (Crossref) [2022-06-09]}
}

@inproceedings{ProgrammingStorytellingOpportunitiesLearningCodingBurke2010a,
  title = {Programming \& Storytelling: Opportunities for Learning about Coding \& Composition},
  shorttitle = {Programming \& Storytelling},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Interaction Design}} and {{Children}} - {{IDC}} '10},
  author = {Burke, Quinn and Kafai, Yasmin B.},
  date = {2010},
  pages = {348},
  publisher = {{ACM Press}},
  location = {{Barcelona, Spain}},
  doi = {10.1145/1810543.1810611},
  url = {http://portal.acm.org/citation.cfm?doid=1810543.1810611},
  urldate = {2022-04-20},
  abstract = {The focus of this paper is to investigate how writing computer programs can help children develop their storytelling and creative writing abilities. The process of writing a program—coding—has long been considered only in terms of computer science, but such coding is also reflective of the imaginative and narrative elements of fiction writing workshops. Writing to program can also serve as programming to write, in which a child learns the importance of sequence, structure, and clarity of expression—three aspects characteristic of effective coding and good storytelling alike. While there have been efforts examining how learning to write code can be facilitated by storytelling, there has been little exploration as to how such creative coding can also be directed to teach students about the narrative and storytelling process. Using the introductory programming language Scratch, this paper explores the potential of having children create their own digital stories with the software and how the narrative structure of these stories offers kids the opportunity to better understand the process of expanding an idea into the arc of a story.},
  eventtitle = {The 9th {{International Conference}}},
  isbn = {978-1-60558-951-0},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ProgrammingStorytellingOpportunitiesLearningCodingBurke2010a.pdf}
}

@unpublished{PromptEngineeringTextBasedGenerativeArtOppenlaender2022,
  title = {Prompt {{Engineering}} for {{Text-Based Generative Art}}},
  author = {Oppenlaender, Jonas},
  date = {2022-04-20},
  number = {arXiv:2204.13988},
  eprint = {2204.13988},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.13988},
  url = {http://arxiv.org/abs/2204.13988},
  urldate = {2022-06-10},
  abstract = {Text-based generative art has seen an explosion of interest in 2021. Online communities around text-based generative art as a novel digital medium have quickly emerged. This short paper identifies five types of prompt modifiers used by practitioners in the community of text-based generative art based on a 3-month ethnographic study on Twitter. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practices of text-based generative art, but also may help practitioners of text-based generative art improve their images. The paper concludes with a discussion of research opportunities in the space of text-based generative art and the broader implications of prompt engineering from the perspective of human-AI interaction in future applications beyond the use case of text-based generative art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Multimedia,H.5,H.m,J.5},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/PromptEngineeringTextBasedGenerativeArtOppenlaender2022.pdf;/Users/brookeryan/Zotero/storage/BYEBI7JD/2204.html}
}

@inproceedings{PuttingInformationForagingTheoryWorkNabi2016,
  title = {Putting Information Foraging Theory to Work: {{Community-based}} Design Patterns for Programming Tools},
  shorttitle = {Putting Information Foraging Theory to Work},
  booktitle = {2016 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Nabi, Tahmid and Sweeney, Kyle M. D. and Lichlyter, Sam and Piorkowski, David and Scaffidi, Chris and Burnett, Margaret and Fleming, Scott D.},
  date = {2016-09},
  pages = {129--133},
  issn = {1943-6106},
  doi = {10.1109/VLHCC.2016.7739675},
  abstract = {The design of programming tools is slow and costly. To ease this process, we developed a design pattern catalog aimed at providing guidance for tool designers. This catalog is grounded in Information Foraging Theory (IFT), which empirical studies have shown to be useful for understanding how developers look for information during development tasks. New design patterns, authored by members of the research community for the catalog, concretely explain how to apply IFT in tool design. In our evaluation, qualitative analyses revealed the community-written design patterns compared well in quality to patterns that we had ourselves published in a smaller, peer-reviewed catalog.},
  eventtitle = {2016 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  keywords = {applied theory,Documentation,Encoding,Navigation,Software,software engineering,Software engineering,tool design,Topology,Visualization},
  annotation = {6 citations (Crossref) [2022-06-09]}
}

@inproceedings{ReadingSourceCodeRaymond1991,
  title = {Reading Source Code},
  booktitle = {Proceedings of the 1991 Conference of the {{Centre}} for {{Advanced Studies}} on {{Collaborative}} Research},
  author = {Raymond, Darrell R.},
  date = {1991-10-28},
  series = {{{CASCON}} '91},
  pages = {3--16},
  publisher = {{IBM Press}},
  location = {{Toronto, Ontario, Canada}},
  abstract = {Source code is, among other things, a text to be read. In this paper I argue that reading source code is a key activity in software maintenance, and that we can profitably apply experiences and reading systems from text databases to the problem of reading source code. Three prototype systems are presented, and the main features of their design are discussed.}
}

@inproceedings{ReadYouWriteHilburn2011,
  title = {Read before You Write},
  booktitle = {2011 24th {{IEEE-CS Conference}} on {{Software Engineering Education}} and {{Training}} ({{CSEE T}})},
  author = {Hilburn, Thomas B. and Towhidnejad, Massood and Salamah, Salamah},
  date = {2011-05},
  pages = {371--380},
  doi = {10.1109/CSEET.2011.5876108},
  abstract = {This paper describes and advocates a focused approach to using inspections of software artifacts as an active learning technique in software engineering education. A central thesis is that one must “learn to read before they write” that is, you should read and study an existing software artifact, before you develop one. There is discussion of how software artifacts and supporting instructional materials from a Digital Home case study project can be used to support and guide software inspection exercises. These inspection exercises are designed to introduce students to realistic software engineering artifacts and involve them in rigorous examination of their contents. Instances of the use of software inspections to teach software engineering are described and analyzed: the experiences of students and instructors, what worked and what did not, and how this influenced the cases study project. The authors also outline a set of topics and courses in which software inspections might be used as a teaching tool throughout a computing curriculum.},
  keywords = {DH-HEMTs,Education,Inspection,Programming,Software,Software engineering,Temperature sensors},
  annotation = {4 citations (Crossref) [2022-06-09]}
}

@online{ReadYouWriteIEEEConference,
  title = {Read before You Write | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5876108},
  urldate = {2021-10-22}
}

@misc{ReadYouWriteTextbarIEEE,
  title = {Read before You Write \textbackslash textbar {{IEEE Conference Publication}} \textbackslash textbar {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5876108},
  urldate = {2021-10-22}
}

@article{ReasoningLearningCreativityFrontalLobeCollins2012,
  title = {Reasoning, {{Learning}}, and {{Creativity}}: {{Frontal Lobe Function}} and {{Human Decision-Making}}},
  shorttitle = {Reasoning, {{Learning}}, and {{Creativity}}},
  author = {Collins, Anne and Koechlin, Etienne},
  date = {2012-03-27},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {10},
  number = {3},
  pages = {e1001293},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001293},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001293},
  urldate = {2022-08-13},
  abstract = {Computational modeling and behavioral experimentation suggest that human frontal lobe function is capable of monitoring three or four concurrent behavioral strategies in order to select the most suitable one during decision-making.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/KWUZYH58/Collins and Koechlin - 2012 - Reasoning, Learning, and Creativity Frontal Lobe .pdf}
}

@inproceedings{ReflectionsOnboardingPracticesMidsizedCompaniesViviani2019,
  title = {Reflections on Onboarding Practices in Mid-Sized Companies},
  booktitle = {2019 {{IEEE}}/{{ACM}} 12th {{International Workshop}} on {{Cooperative}} and {{Human Aspects}} of {{Software Engineering}} ({{CHASE}})},
  author = {Viviani, Giovanni and Murphy, Gail C.},
  date = {2019},
  pages = {83--84},
  publisher = {{IEEE}}
}

@online{ReplyMurphyLeivadaProgramInductionpiantadosi,
  title = {Reply to {{Murphy}} and {{Leivada}}: {{Program}} Induction Can Learn Language},
  shorttitle = {Reply to {{Murphy}} and {{Leivada}}},
  author = {{piantadosi}},
  doi = {10.1073/pnas.2202925119},
  url = {https://www.pnas.org/doi/10.1073/pnas.2202925119},
  urldate = {2022-08-10},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/I7M23XFV/pnas.html}
}

@inproceedings{RequirementsEngineeringOutClassroomAnticipatingMarques2020,
  title = {Requirements {{Engineering Out}} of the {{Classroom}}: {{Anticipating Challenges Experienced}} in {{Practice}}},
  shorttitle = {Requirements {{Engineering Out}} of the {{Classroom}}},
  booktitle = {2020 {{IEEE}} 32nd {{Conference}} on {{Software Engineering Education}} and {{Training}} ({{CSEE T}})},
  author = {Marques, Pedro and Silva, Murilo and Gusmão, Camila and Castro, Diego and Schots, Marcelo},
  date = {2020-11},
  pages = {1--9},
  issn = {2377-570X},
  doi = {10.1109/CSEET49119.2020.9206220},
  abstract = {Requirements Engineering (RE) education makes use of different methods to motivate and prepare students to perform well this activity in the practical scenario. Such educational methods, however, are typically based on some simulation, be it in the scope of the scenario, the problem, or the stakeholders. A change in the traditional teaching paradigm becomes necessary, bringing the student to the center of the RE activities. This helps them realize the importance of properly identifying and organizing the demands from the target audience, so that a quality requirements specification document can be produced. This paper describes a method applied during the Requirements Engineering course using a real scenario, in which students sought to identify the diverse stakeholders' needs with respect to the university institute website. Through this work, students achieved effective engagement in practical RE activities, whose results has benefits not only to the university community, but also to the external public interested in institute information. The conduction of this work revealed - and reinforced - some relevant challenges and lessons learned.},
  eventtitle = {2020 {{IEEE}} 32nd {{Conference}} on {{Software Engineering Education}} and {{Training}} ({{CSEE T}})},
  keywords = {Computer science,Education,Interviews,requirements engineering,Requirements engineering,Situated learning,Software engineering,software engineering education,Stakeholders,Task analysis},
  annotation = {1 citations (Crossref) [2022-06-09]}
}

@online{ResearchProjectsCognitiveNeuralComputation,
  title = {Research Projects | Cognitive \& Neural Computation Lab},
  url = {https://faculty.sites.uci.edu/cnclab/research/},
  urldate = {2022-07-22},
  langid = {american},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/3JWVH6HZ/research.html}
}

@inproceedings{ReverseEngineeringEducationToolComputerKlimek2011,
  title = {Reverse Engineering as an Education Tool in Computer Science},
  booktitle = {2011 9th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  author = {Klimek, Ivan and Keltika, Marián and Jakab, František},
  date = {2011-10},
  pages = {123--126},
  doi = {10.1109/ICETA.2011.6112599},
  abstract = {The concept of Reverse Engineering is used in many fields of IT every day, to name just a few: legacy compatibility, binary code patching, malware analysis, network protocols analysis, debugging or even rapid prototyping. Despite its broad use, reverse engineering is not actively taught as a part of computer science courses. This paper tries to provide a survey of some of the real life usage scenarios of reverse engineering, analyzes what skills and ways of thinking are developed by reverse engineering and provides examples how reverse engineering could be taught by practical problem solving, introducing creative thinking models and strategies. We focus on the importance of reverse engineering as a tool to ignite the self-motivation of students and systematically build their logical thinking capabilities and analytical skills.},
  eventtitle = {2011 9th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  keywords = {Educational institutions,Internet,Protocols,Reverse engineering,Security,Software},
  annotation = {4 citations (Crossref) [2022-06-09]}
}

@inproceedings{RoleCollaborationCreativityEmbodimentAILong2021,
  title = {The {{Role}} of {{Collaboration}}, {{Creativity}}, and {{Embodiment}} in {{AI Learning Experiences}}},
  booktitle = {Creativity and {{Cognition}}},
  author = {Long, Duri and Padiyath, Aadarsh and Teachey, Anthony and Magerko, Brian},
  date = {2021-06-22},
  series = {C\&amp;{{C}} '21},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3450741.3465264},
  url = {https://doi.org/10.1145/3450741.3465264},
  urldate = {2022-04-20},
  abstract = {Fostering public AI literacy (i.e. a high-level understanding of artificial intelligence (AI) that allows individuals to critically and effectively use AI technologies) is increasingly important as AI is integrated into individuals’ everyday lives and as concerns about AI grow. This paper investigates how to design collaborative, creative, and embodied interactions that foster AI learning and interest development. We designed three prototypes of collaborative, creative, and/or embodied learning experiences that aim to communicate AI literacy competencies. We present the design of these prototypes as well as the results from a user study that we conducted with 14 family groups (38 participants). Our data analysis explores how collaboration, creativity, and embodiment contributed to AI learning and interest development across the three prototypes. The main contributions of this paper are: 1) three designs of AI literacy learning activities and 2) insights into the role creativity, collaboration, and embodiment play in AI learning experiences.},
  isbn = {978-1-4503-8376-9},
  keywords = {AI education,AI literacy,co-creative,collaboration,creativity,embodiment,family learning,informal learning},
  file = {/Users/brookeryan/Documents/Obsidian/reference/RoleCollaborationCreativityEmbodimentAILong2021a.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/RoleCollaborationCreativityEmbodimentAILong2021a.pdf}
}

@article{RoleExecutiveFunctionShapingReinforcementRmus2021,
  title = {The Role of Executive Function in Shaping Reinforcement Learning},
  author = {Rmus, Milena and McDougle, Samuel D and Collins, Anne GE},
  date = {2021-04-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Computational Cognitive Neuroscience},
  volume = {38},
  pages = {66--73},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2020.10.003},
  url = {https://www.sciencedirect.com/science/article/pii/S2352154620301480},
  urldate = {2022-08-13},
  abstract = {Reinforcement learning (RL) models have advanced our understanding of how animals learn and make decisions, and how the brain supports learning. However, the neural computations that are explained by RL algorithms fall short of explaining many sophisticated aspects of human learning and decision making, including the generalization of behavior to novel contexts, one-shot learning, and the synthesis of task information in complex environments. Instead, these aspects of behavior are assumed to be supported by the brain’s executive functions (EF). We review recent findings that highlight the importance of EF in instrumental learning. Specifically, we advance the theory that EF sets the stage for canonical RL computations in the brain, providing inputs that broaden their flexibility and applicability. Our theory has important implications for how to interpret RL computations in both brain and behavior.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/9XNDB86X/Rmus et al. - 2021 - The role of executive function in shaping reinforc.pdf}
}

@inproceedings{ScopingReviewDomainsApplicationsHumanDroneHerdel2022,
  title = {Above and {{Beyond}}: {{A Scoping Review}} of {{Domains}} and {{Applications}} for {{Human-Drone Interaction}}},
  shorttitle = {Above and {{Beyond}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Herdel, Viviane and Yamin, Lee J. and Cauchard, Jessica R.},
  date = {2022-04-29},
  series = {{{CHI}} '22},
  pages = {1--22},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501881},
  url = {https://doi.org/10.1145/3491102.3501881},
  urldate = {2022-08-29},
  abstract = {Interacting with flying objects has fueled people’s imagination throughout history. Over the past decade, the Human-Drone Interaction (HDI) community has been working towards making this dream a reality. Despite notable findings, we lack a high-level perspective on the current and future use cases for interacting with drones. We present a holistic view of domains and applications of use that are described, studied, and envisioned in the HDI body of work. To map the extent and nature of the prior research, we performed a scoping review (N=217). We identified 16 domains and over 100 applications where drones and people interact together. We then describe in depth the main domains and applications reported in the literature and further present under-explored use cases with great potential. We conclude with fundamental challenges and opportunities for future research in the field. This work contributes a systematic step towards increased replicability and generalizability of HDI research.},
  isbn = {978-1-4503-9157-3},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/LP2RWRLW/Herdel et al. - 2022 - Above and Beyond A Scoping Review of Domains and .pdf}
}

@article{ScopingStudiesMethodologicalFrameworkArksey2005,
  title = {Scoping Studies: Towards a Methodological Framework},
  shorttitle = {Scoping Studies},
  author = {Arksey, Hilary and O'Malley, Lisa},
  date = {2005-02-01},
  journaltitle = {International Journal of Social Research Methodology},
  volume = {8},
  number = {1},
  pages = {19--32},
  publisher = {{Routledge}},
  issn = {1364-5579},
  doi = {10.1080/1364557032000119616},
  url = {https://doi.org/10.1080/1364557032000119616},
  urldate = {2022-06-10},
  abstract = {This paper focuses on scoping studies, an approach to reviewing the literature which to date has received little attention in the research methods literature. We distinguish between different types of scoping studies and indicate where these stand in relation to full systematic reviews. We outline a framework for conducting a scoping study based on our recent experiences of reviewing the literature on services for carers for people with mental health problems. Where appropriate, our approach to scoping the field is contrasted with the procedures followed in systematic reviews. We emphasize how including a consultation exercise in this sort of study may enhance the results, making them more useful to policy makers, practitioners and service users. Finally, we consider the advantages and limitations of the approach and suggest that a wider debate is called for about the role of the scoping study in relation to other types of literature reviews.},
  keywords = {AI Education Project,notion,ObsCite,systematic literature review},
  annotation = {\_eprint: https://doi.org/10.1080/1364557032000119616},
  file = {/Users/brookeryan/Documents/Obsidian/reference/ScopingStudiesMethodologicalFrameworkArksey2005.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/ScopingStudiesMethodologicalFrameworkArksey2005.pdf;/Users/brookeryan/Zotero/storage/J9VQZ332/1364557032000119616.html}
}

@inproceedings{ScratchMicroworldsDesigningProjectBasedIntroductionsTsur2018a,
  title = {Scratch {{Microworlds}}: {{Designing Project-Based Introductions}} to {{Coding}}},
  shorttitle = {Scratch {{Microworlds}}},
  booktitle = {Proceedings of the 49th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Tsur, Moran and Rusk, Natalie},
  date = {2018-02-21},
  pages = {894--899},
  publisher = {{ACM}},
  location = {{Baltimore Maryland USA}},
  doi = {10.1145/3159450.3159559},
  url = {https://dl.acm.org/doi/10.1145/3159450.3159559},
  urldate = {2022-04-20},
  abstract = {In this paper, we present our experience developing introductory coding environments called Scratch Microworlds. These interactive environments enable learners to get started with coding by creating projects, rather than solving puzzles. The primary educational goal of these microworlds is to engage learners (ages 8 to 14) who otherwise may not be drawn to coding. The microworlds are simplified versions of the Scratch coding environment that contain a small set of blocks and are designed to encourage exploration and experimentation. They are also interest-based, so learners can choose to work on a topic that is motivating to them (such as dance, music, or soccer). We present three main design principles and related challenges that we addressed through the iterative process of developing Scratch Microworlds: (1) how to simplify initial experiences while still supporting creativity, (2) how to provide scaffolding while maintaining learners’ agency, and (3) how to provide starting points that spark rather than limit the imagination. We share observations and feedback from workshops with children and educators, which informed our iterative design process. We conclude by considering next steps for providing more entry points into coding that support children as creative thinkers.},
  eventtitle = {{{SIGCSE}} '18: {{The}} 49th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  isbn = {978-1-4503-5103-4},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ScratchMicroworldsDesigningProjectBasedIntroductionsTsur2018a.pdf}
}

@article{ScratchProgrammingAllResnick2009a,
  title = {Scratch: Programming for All},
  shorttitle = {Scratch},
  author = {Resnick, Mitchel and Maloney, John and Monroy-Hernández, Andrés and Rusk, Natalie and Eastmond, Evelyn and Brennan, Karen and Millner, Amon and Rosenbaum, Eric and Silver, Jay and Silverman, Brian and Kafai, Yasmin},
  date = {2009-11},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {52},
  number = {11},
  pages = {60--67},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1592761.1592779},
  url = {https://dl.acm.org/doi/10.1145/1592761.1592779},
  urldate = {2022-04-20},
  abstract = {"Digital fluency" should mean designing, creating, and remixing, not just browsing, chatting, and interacting.},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ScratchProgrammingAllResnick2009a.pdf}
}

@article{SearchBeautyStruggleComplexityChristopherGabriel2019,
  title = {A {{Search}} for {{Beauty}}/{{A Struggle}} with {{Complexity}}: {{Christopher Alexander}}},
  shorttitle = {A {{Search}} for {{Beauty}}/{{A Struggle}} with {{Complexity}}},
  author = {Gabriel, Richard and Quillien, Jenny},
  date = {2019-06-16},
  journaltitle = {Urban Science},
  shortjournal = {Urban Science},
  volume = {3},
  number = {2},
  pages = {64},
  issn = {2413-8851},
  doi = {10.3390/urbansci3020064},
  url = {https://www.mdpi.com/2413-8851/3/2/64},
  urldate = {2021-10-03},
  abstract = {Beauty. Christopher Alexander’s prolific journey in building, writing, and teaching was fueled by a relentless search for Beauty and its meaning. While all around him the world was intent on figuring out how to simplify, Alexander came to embrace complexity as the only path to his goal. The Beauty and life of that which he encountered and appreciated—an Indian village, a city, a subway network, an old Turkish carpet, or a campus—lay in its well-ordered complexity. As a designer and maker he found that simplicity came from choosing—at every step—the simplest way to add the necessary complexity. The failure of so much of our modern world, in Alexander’s eyes, was oversimplification, wantonly bulldozing context, misunderstanding the relationships of part and whole, ignoring the required role of time in the shaping of shapes, and ultimately dismissing, like Esau, our birthright of Value in favor of a lentil pottage of mere Fact. Ever elusive, Beauty demands of her suitors a constant return of attention to see what might be newly revealed, and Alexander duly returned again and again in pursuit of the mystery. In this essay—essentially biographical and descriptive of one man’s endeavors—we examine the full arc of his work from dissertation to most recent memoir. We don’t shy away from his failures, and we don’t simplify his journey. We leave work done by other scholars for another day. We reach no conclusion, rather, we invite readers to reflect on what Alexander’s lifelong effort suggests to them about their own path, their own sense of aesthetics and order, innate cognitive shortfalls, and professional blind alleys.},
  langid = {english},
  keywords = {Misc.,notion},
  annotation = {10 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/LQHGZEEE/Gabriel and Quillien - 2019 - A Search for BeautyA Struggle with Complexity Ch.pdf}
}

@inproceedings{SecureMessagingPlatformBasedBlockchainEllewala2020,
  title = {Secure {{Messaging Platform Based}} on {{Blockchain}}},
  booktitle = {2020 2nd {{International Conference}} on {{Advancements}} in {{Computing}} ({{ICAC}})},
  author = {Ellewala, U. P. and Amarasena, W.D.H.U and Lakmali, H.V Sachini and Senanayaka, L.M.K and Senarathne, A.N.},
  date = {2020-12},
  volume = {1},
  pages = {317--322},
  doi = {10.1109/ICAC51239.2020.9357306},
  abstract = {The boundaries between personal and business communications is a key issue faced by most organizations. Use of unsecured and unsafe applications in workplaces pose enormous security risks. Companies are not adequately aware about the applications that are being used in their employees' devices. When it comes to critical business communication involving exchanging trade secrets, making business referrals and strategic business decisions, protecting of messages and shared files becomes a challenge. Most publicly available communication platforms do not empower organizations to regulate, track and scale their communication and does not provide compliance with data protection frameworks, which can result in cross industry system risks. As a result, both individuals and organizations express deep concern about data security and protection of privacy when using Instant Messaging applications. Non-repudiation in communications not only conveys to the user, recognition of the communication process, but it is also a crucial way to establish a relationship of trust and to overcome trust disputes. Our primary objective, through this research, is to develop a chat application with more secure channels of enterprise level communication. Using new technologies such as blockchain, which operate on a decentralized model, we can surmount the drawbacks of traditional messaging applications, thereby ensuring confidentiality, integrity and availability of official data, along with advanced auditing features.},
  eventtitle = {2020 2nd {{International Conference}} on {{Advancements}} in {{Computing}} ({{ICAC}})},
  keywords = {Authentication,Blockchain,Business communication,Data Loss Prevention,Data protection,Data security,Employment,Encryption,Industries,Instant messaging,Instant Messaging,Smart Contract},
  annotation = {1 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/GJT44H55/Ellewala et al. - 2020 - Secure Messaging Platform Based on Blockchain.pdf;/Users/brookeryan/Zotero/storage/DRSC5LRX/9357306.html}
}

@inproceedings{SelectingOpenSourceSoftwareProjectsSmith2014,
  title = {Selecting Open Source Software Projects to Teach Software Engineering},
  booktitle = {Proceedings of the 45th {{ACM}} Technical Symposium on {{Computer}} Science Education},
  author = {Smith, Therese Mary and McCartney, Robert and Gokhale, Swapna S. and Kaczmarczyk, Lisa C.},
  date = {2014-03-05},
  series = {{{SIGCSE}} '14},
  pages = {397--402},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2538862.2538932},
  url = {https://doi.org/10.1145/2538862.2538932},
  urldate = {2021-09-27},
  abstract = {Aspiring software engineers must be able to comprehend and evolve legacy code, which is challenging because the code may be poorly documented, ill structured, and lacking in human support. These challenges of understanding and evolving existing code can be illustrated in academic settings by leveraging the rich and varied volume of Open Source Software (OSS) code. To teach SE with OSS, however, it is necessary to select uniform projects of appropriate size and complexity. This paper reports on our search for suitable OSS projects to teach an introductory SE course with a focus on maintenance and evolution. The search turned out to be quite labor intensive and cumbersome, contrary to our expectations that it would be quick and simple. The chosen projects successfully demonstrated the maintenance challenges, highlighting the promise of using OSS. The burden of selecting projects, however, may impede widespread integration of OSS into SE and other computing courses.},
  isbn = {978-1-4503-2605-6},
  keywords = {maintenance,open source,program comprehension,software engineering},
  annotation = {24 citations (Crossref) [2022-06-09]}
}

@misc{SequenceDiagramIntelliJIDEAAndroidStudio2022,
  title = {{{SequenceDiagram}} - {{IntelliJ IDEA}} \& {{Android Studio Plugin}} \textbackslash textbar {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/8286-sequencediagram},
  urldate = {2022-01-31}
}

@online{SequenceDiagramIntelliJIDEAAndroidStudio2022a,
  title = {{{SequenceDiagram}} - {{IntelliJ IDEA}} \& {{Android Studio Plugin}} | {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/8286-sequencediagram},
  urldate = {2022-01-31}
}

@article{SharedMentalModelsCoordinationLargeScaleEspinosa2001,
  title = {Shared {{Mental Models}} and {{Coordination}} in {{Large-Scale}}, {{Distributed Software Development}}},
  author = {Espinosa, Alberto and Kraut, Robert and Lerch, Javier and Slaughter, Sandra and Herbsleb, James and Mockus, Audris},
  date = {2001-12-31},
  journaltitle = {ICIS 2001 Proceedings},
  url = {https://aisel.aisnet.org/icis2001/64}
}

@article{ShouldFewNullFindingsFalsifyOdegaard2017,
  title = {Should a {{Few Null Findings Falsify Prefrontal Theories}} of {{Conscious Perception}}?},
  author = {Odegaard, Brian and Knight, Robert T. and Lau, Hakwan},
  date = {2017-10-04},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {37},
  number = {40},
  eprint = {28978696},
  eprinttype = {pmid},
  pages = {9593--9602},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3217-16.2017},
  url = {https://www.jneurosci.org/content/37/40/9593},
  urldate = {2022-07-19},
  abstract = {Is activity in prefrontal cortex (PFC) critical for conscious perception? Major theories of consciousness make distinct predictions about the role of PFC, providing an opportunity to arbitrate between these views empirically. Here we address three common misconceptions: (1) PFC lesions do not affect subjective perception; (2) PFC activity does not reflect specific perceptual content; and (3) PFC involvement in studies of perceptual awareness is solely driven by the need to make reports required by the experimental tasks rather than subjective experience per se. These claims are incompatible with empirical findings, unless one focuses only on studies using methods with limited sensitivity. The literature highlights PFC's essential role in enabling the subjective experience in perception, contra the objective capacity to perform visual tasks; conflating the two can also be a source of confusion. Dual Perspectives Companion Paper: Are the Neural Correlates of Consciousness in the Front or in the Back of the Cerebral Cortex? Clinical and Neuroimaging Evidence, by Melanie Boly, Marcello Massimini, Naotsugu Tsuchiya, Bradley R. Postle, Christof Koch, and Giulio Tononi},
  langid = {english},
  keywords = {Neuroscience of Consciousness,notion},
  file = {/Users/brookeryan/Zotero/storage/CIHQZQFP/Odegaard et al. - 2017 - Should a Few Null Findings Falsify Prefrontal Theo.pdf;/Users/brookeryan/Zotero/storage/6T9RX44B/9593.html}
}

@inproceedings{SignLanguageCentricDesignASLSurveyMahajan2022,
  title = {Towards {{Sign Language-Centric Design}} of {{ASL Survey Tools}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mahajan, Shruti and Walker, Zoey and Boll, Rachel and Santacreu, Michelle and Salvino, Ally and Westfort, Michael and Reis, Jeanne and Solovey, Erin},
  date = {2022-04-29},
  series = {{{CHI}} '22},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3491102.3502047},
  url = {https://doi.org/10.1145/3491102.3502047},
  urldate = {2022-06-09},
  abstract = {Questionnaires are fundamental learning and research tools for gathering insights and information from individuals, and now can be created easily using online tools. However, existing resources for creating questionnaires are designed for written languages (e.g. English) and do not support sign languages (e.g. American Sign Language). Sign languages (SLs) have unique visual characteristics that do not fit into user interface paradigms designed for written, text-based languages. Through a series of formative studies with the ASL signing community, this paper takes steps towards understanding the viability, potential benefit, challenges, and user interest in SL-centric surveys, a novel approach for creating questionnaires that meet the needs of deaf individuals using sign languages, without obligatory reliance on a written language to complete a questionnaire.},
  isbn = {978-1-4503-9157-3},
  keywords = {AI Education Project,American Sign Language,ASL,notion,systematic literature review},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/SignLanguageCentricDesignASLSurveyMahajan2022.pdf}
}

@article{SixfoldOverrepresentationGraduatesPrestigiousUniversitiesMiuccio2017,
  title = {Six-Fold over-Representation of Graduates from Prestigious Universities Does Not Necessitate Unmeritocratic Selection in the Faculty Hiring Process},
  author = {Miuccio, Michael and Liu, Ka-yuet and Lau, Hakwan and Peters, Megan AK},
  date = {2017},
  journaltitle = {PloS one},
  volume = {12},
  number = {10},
  pages = {e0185900},
  publisher = {{Public Library of Science San Francisco, CA USA}},
  keywords = {Advisor Literature,notion}
}

@article{SizeWeightIllusionNotAntiBayesianAllPeters2016,
  title = {The {{Size-Weight Illusion}} Is Not Anti-{{Bayesian}} after All: A Unifying {{Bayesian}} Account},
  shorttitle = {The {{Size-Weight Illusion}} Is Not Anti-{{Bayesian}} after All},
  author = {Peters, Megan A. K. and Ma, Wei Ji and Shams, Ladan},
  date = {2016-06-16},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {4},
  pages = {e2124},
  publisher = {{PeerJ Inc.}},
  issn = {2167-8359},
  doi = {10.7717/peerj.2124},
  url = {https://peerj.com/articles/2124},
  urldate = {2022-07-22},
  abstract = {When we lift two differently-sized but equally-weighted objects, we expect the larger to be heavier, but the smaller feels heavier. However, traditional Bayesian approaches with “larger is heavier” priors predict the smaller object should feel lighter; this Size-Weight Illusion (SWI) has thus been labeled “anti-Bayesian” and has stymied psychologists for generations. We propose that previous Bayesian approaches neglect the brain’s inference process about density. In our Bayesian model, objects’ perceived heaviness relationship is based on both their size and inferred density relationship: observers evaluate competing, categorical hypotheses about objects’ relative densities, the inference about which is then used to produce the final estimate of weight. The model can qualitatively and quantitatively reproduce the SWI and explain other researchers’ findings, and also makes a novel prediction, which we confirmed. This same computational mechanism accounts for other multisensory phenomena and illusions; that the SWI follows the same process suggests that competitive-prior Bayesian inference can explain human perception across many domains.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/IVPIU6U5/Peters et al. - 2016 - The Size-Weight Illusion is not anti-Bayesian afte.pdf}
}

@inproceedings{SocialBarriersFacedNewcomersPlacingSteinmacher2015,
  title = {Social {{Barriers Faced}} by {{Newcomers Placing Their First Contribution}} in {{Open Source Software Projects}}},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Computer Supported Cooperative Work}} \& {{Social Computing}}},
  author = {Steinmacher, Igor and Conte, Tayana and Gerosa, Marco Aurélio and Redmiles, David},
  date = {2015-02},
  series = {{{CSCW}} '15},
  pages = {1379--1392},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2675133.2675215},
  url = {https://doi.org/10.1145/2675133.2675215},
  urldate = {2021-10-21},
  abstract = {Newcomers' seamless onboarding is important for online communities that depend upon leveraging the contribution of outsiders. Previous studies investigated aspects of the joining process and motivation in open collaboration communities, but few have focused on identifying and understanding the critical barriers newcomers face when placing their first contribution, a period that frequently leads to dropout. This is important for Open Source Software (OSS) projects, which receive contributions from many one-time contributors. Focusing on OSS, our study qualitatively analyzed social barriers that hindered newcomers' first contributions. We defined a conceptual model composed of 58 barriers including 13 social barriers. The barriers were identified from a qualitative data analysis considering different sources: a systematic literature review; open question responses gathered from OSS projects' contributors; students contributing to OSS projects; and semi-structured interviews with 36 developers from 14 different projects. This paper focuses on social barriers and its contributions include gathering empirical evidence of the barriers faced by newcomers, organizing and better understanding these barriers, surveying the literature from the perspective of the barriers, and identifying new potential research streams.},
  isbn = {978-1-4503-2922-4},
  keywords = {barriers,entry,joining,new contributor,newcomers,onboarding,online communities,open collaboration,open source software,qualitative study,social barriers,socialization},
  annotation = {118 citations (Crossref) [2022-06-09]}
}

@book{SoftwareDesignDecoded66WaysPetre2016,
  title = {Software {{Design Decoded}}: 66 {{Ways Experts Think}}},
  shorttitle = {Software {{Design Decoded}}},
  author = {Petre, Marian and van der Hoek, André},
  date = {2016-10-06},
  eprint = {EVE4DQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{MIT Press}},
  abstract = {An engaging, illustrated collection of insights revealing the practices and principles that expert software designers use to create great software.What makes an expert software designer? It is more than experience or innate ability. Expert software designers have specific habits, learned practices, and observed principles that they apply deliberately during their design work. This book offers sixty-six insights, distilled from years of studying experts at work, that capture what successful software designers actually do to create great software. The book presents these insights in a series of two-page illustrated spreads, with the principle and a short explanatory text on one page, and a drawing on the facing page. For example, “Experts generate alternatives” is illustrated by the same few balloons turned into a set of very different balloon animals. The text is engaging and accessible; the drawings are thought-provoking and often playful.Organized into such categories as “Experts reflect,” “Experts are not afraid,” and “Experts break the rules,” the insights range from “Experts prefer simple solutions” to “Experts see error as opportunity.” Readers learn that “Experts involve the user”; “Experts take inspiration from wherever they can”; “Experts design throughout the creation of software”; and “Experts draw the problem as much as they draw the solution.” One habit for an aspiring expert software designer to develop would be to read and reread this entertaining but essential little book. The insights described offer a guide for the novice or a reference for the veteran—in software design or any design profession.A companion web site provides an annotated bibliography that compiles key underpinning literature, the opportunity to suggest additional insights, and more.},
  isbn = {978-0-262-03518-7},
  langid = {english},
  pagetotal = {185},
  keywords = {Computers / Software Development & Engineering / Systems Analysis & Design}
}

@book{SolidCodeMarshall2009,
  title = {Solid {{Code}}},
  author = {Marshall, Donis and Bruno, John},
  date = {2009-02-18},
  eprint = {ZZtCAwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Microsoft Press}},
  abstract = {Get best-in-class engineering practices to help you write more-robust, bug-free code. Two Microsoft .NET development experts share real-world examples and proven methods for optimizing the software development life cycle—from avoiding costly programming pitfalls to making your development team more efficient. Managed code developers at all levels will find design, prototyping, implementation, debugging, and testing tips to boost the quality of their code—today. Optimize each stage of the development process—from design to testing—and produce higher-quality applications.  Use metaprogramming to reduce code complexity, while increasing flexibility and maintainability Treat performance as a feature—and manage it throughout the development life cycle Apply best practices for application scalability Employ preventative security measures to ward off malicious attacks Practice defensive programming to catch bugs before run time Incorporate automated builds, code analysis, and testing into the daily engineering process Implement better source-control management and check-in procedures Establish a quality-driven, milestone-based project rhythm—and improve your results!},
  isbn = {978-0-7356-3851-8},
  langid = {english},
  pagetotal = {620},
  keywords = {Computers / Software Development & Engineering / General}
}

@article{SpatialConceptsNumberSizeTimePitt2021,
  title = {Spatial Concepts of Number, Size, and Time in an Indigenous Culture},
  author = {Pitt, Benjamin and Ferrigno, Stephen and Cantlon, Jessica F. and Casasanto, Daniel and Gibson, Edward and Piantadosi, Steven T.},
  date = {2021-08-11},
  journaltitle = {Science Advances},
  volume = {7},
  number = {33},
  pages = {eabg4141},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.abg4141},
  url = {https://www.science.org/doi/full/10.1126/sciadv.abg4141},
  urldate = {2022-08-10},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/2HGCYZZY/Pitt et al. - 2021 - Spatial concepts of number, size, and time in an i.pdf}
}

@inproceedings{SquishyCircuitsTangibleMediumElectronicsJohnson2010a,
  title = {Squishy Circuits: A Tangible Medium for Electronics Education},
  shorttitle = {Squishy Circuits},
  booktitle = {{{CHI}} '10 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Johnson, Samuel and Thomas, AnnMarie P.},
  date = {2010-04-10},
  pages = {4099--4104},
  publisher = {{ACM}},
  location = {{Atlanta Georgia USA}},
  doi = {10.1145/1753846.1754109},
  url = {https://dl.acm.org/doi/10.1145/1753846.1754109},
  urldate = {2022-04-20},
  abstract = {This paper reports on the design of a circuit building activity intended for children, which replaces wires with malleable conductive and non-conductive dough. By eliminating the need for soldering or breadboards, it becomes possible to very quickly incorporate movement and light into sculptures, and to introduce simple circuit concepts to children at a younger age. Future applications in both structured and unstructured learning environments, based on results from a preliminary pilot study, are presented.},
  eventtitle = {{{CHI}} '10: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-60558-930-5},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/SquishyCircuitsTangibleMediumElectronicsJohnson2010a.pdf}
}

@software{StableDiffusionAkashicRecordsMaks2022,
  title = {Stable {{Diffusion Akashic Records}}},
  author = {Maks},
  date = {2022-08-30T22:06:24Z},
  origdate = {2022-08-09T10:04:17Z},
  url = {https://github.com/Maks-s/sd-akashic},
  urldate = {2022-08-30},
  abstract = {A compendium of informations regarding Stable Diffusion (SD)},
  keywords = {AI Generated Art Educational}
}

@online{StableDiffusionLaunchAnnouncement,
  title = {Stable {{Diffusion}} Launch Announcement},
  url = {https://stability.ai/blog/stable-diffusion-announcement},
  urldate = {2022-08-30},
  abstract = {Stable Diffusion launch announcement},
  langid = {british},
  organization = {{Stability.Ai}},
  keywords = {AI Generated Art Algorithms,notion},
  file = {/Users/brookeryan/Zotero/storage/PN5Z2T8E/stable-diffusion-announcement.html}
}

@online{StatisticIntelliJIDEsPluginMarketplace2022,
  title = {Statistic - {{IntelliJ IDEs Plugin}} | {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/4509-statistic},
  urldate = {2022-01-31}
}

@misc{StatisticIntelliJIDEsPluginTextbar2022,
  title = {Statistic - {{IntelliJ IDEs Plugin}} \textbackslash textbar {{Marketplace}}},
  date = {2022},
  url = {https://plugins.jetbrains.com/plugin/4509-statistic},
  urldate = {2022-01-31}
}

@article{StressfulEventsTeachingSignalsBrainTrapp2018,
  title = {Stressful {{Events}} as {{Teaching Signals}} for the {{Brain}}},
  author = {Trapp, Sabrina and O’Doherty, John P. and Schwabe, Lars},
  date = {2018-06},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {6},
  pages = {475--478},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.03.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661318300688},
  urldate = {2022-07-22},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/4NH3UNS2/Trapp et al. - 2018 - Stressful Events as Teaching Signals for the Brain.pdf}
}

@article{SURVEYASSESSMENTMAKERSPACESPeppler,
  title = {{{SURVEY OF ASSESSMENT IN MAKERSPACES}}},
  author = {Peppler, Kylie and Keune, Anna and Xia, Fangli and Chang, Stephanie},
  pages = {29},
  langid = {english},
  keywords = {AI Education Project,Example Surveys,notion},
  file = {/Users/brookeryan/Zotero/storage/74ZGL2AM/Peppler et al. - SURVEY OF ASSESSMENT IN MAKERSPACES.pdf}
}

@inproceedings{SurveyAudioProgrammingToolsAdams2013,
  title = {Survey of Audio Programming Tools},
  booktitle = {{{CHI}} '13 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Adams, Alexander T. and Latulipe, Celine},
  date = {2013-04-27},
  series = {{{CHI EA}} '13},
  pages = {781--786},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2468356.2468495},
  url = {https://doi.org/10.1145/2468356.2468495},
  urldate = {2022-06-09},
  abstract = {Audio programming can be an overwhelming and confusing task that many developers are not adequately prepared for. Even the seemingly simple task of choosing the right software developers kit (SDK) to use can become a difficult task. This paper presents an analysis of the most extensive and widely used audio programming SDKs organized by audio task and highlighting factors such as usability, support, and functionality.},
  isbn = {978-1-4503-1952-2},
  keywords = {AI Education Project,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/SurveyAudioProgrammingToolsAdams2013.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/SurveyAudioProgrammingToolsAdams2013.pdf}
}

@inproceedings{SurveyCoverageBasedTestingToolsYang2006,
  title = {A Survey of Coverage Based Testing Tools},
  booktitle = {Proceedings of the 2006 International Workshop on {{Automation}} of Software Test},
  author = {Yang, Qian and Li, J. Jenny and Weiss, David},
  date = {2006-05-23},
  series = {{{AST}} '06},
  pages = {99--103},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1138929.1138949},
  url = {https://doi.org/10.1145/1138929.1138949},
  urldate = {2022-06-09},
  abstract = {Test coverage is sometimes used as a way to measure how thoroughly software is tested. Coverage is used by software developers and sometimes by vendors to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools focusing on, but not restricted to coverage measurement. We also survey additional features, including program prioritization for testing, assistance in debugging, automatic generation of test cases, and customization of test reports. Such features make tools more useful and practical, especially for large-scale, real-life commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage1 (a tool suite that includes code coverage testing, debugging, performance profiling, and reporting). Our study shows that each tool has its unique features tailored to its application domains. Therefore this study can be used to pick the right coverage testing tools depending on various requirements.},
  isbn = {978-1-59593-408-6},
  keywords = {AI Education Project,automate test case generation,code coverage,coverage-based testing tool,dominator analysis,eXVantage,notion,prioritization,systematic literature review},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/SurveyCoverageBasedTestingToolsYang2006.pdf}
}

@inproceedings{SurveyIntelligentDigitalInkToolsImtiaz2017,
  title = {A Survey of Intelligent Digital Ink Tools Use in {{STEM}} Education},
  booktitle = {Proceedings of the {{Australasian Computer Science Week Multiconference}}},
  author = {Imtiaz, Md Athar and Blagojevic, Rachel and Luxton-Reilly, Andrew and Plimmer, Beryl},
  date = {2017-01-31},
  series = {{{ACSW}} '17},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3014812.3014825},
  url = {https://doi.org/10.1145/3014812.3014825},
  urldate = {2022-06-09},
  abstract = {Digital ink affords numerous opportunities to broaden the use of computer supported learning in science, technology, engineering and mathematics (STEM) education. This review of the recent literature on digital ink tools demonstrates that they offer significant potential, and although there are relatively few studies on intelligent digital ink tools, it is notable that they span the STEM disciplines. In this review, we identify the major challenges for applications of digital ink in STEM education, and discuss the primary areas of use, feedback mechanisms and interaction methods.},
  isbn = {978-1-4503-4768-6},
  keywords = {AI Education Project,digital ink,educational tools,intelligent tools,notion,STEM education,systematic literature review},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/SurveyIntelligentDigitalInkToolsImtiaz2017.pdf}
}

@article{SystematicLiteratureReviewBlockchainbasedApplicationsCasino2019,
  title = {A Systematic Literature Review of Blockchain-Based Applications: {{Current}} Status, Classification and Open Issues},
  author = {Casino, Fran and Dasaklis, Thomas K. and Patsakis, Constantinos},
  date = {2019},
  journaltitle = {Telematics and Informatics},
  volume = {36},
  pages = {55--81},
  issn = {0736-5853},
  doi = {10.1016/j.tele.2018.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0736585318306324},
  abstract = {This work provides a systematic literature review of blockchain-based applications across multiple domains. The aim is to investigate the current state of blockchain technology and its applications and to highlight how specific characteristics of this disruptive technology can revolutionise “business-as-usual” practices. To this end, the theoretical underpinnings of numerous research papers published in high ranked scientific journals during the last decade, along with several reports from grey literature as a means of streamlining our assessment and capturing the continuously expanding blockchain domain, are included in this review. Based on a structured, systematic review and thematic content analysis of the discovered literature, we present a comprehensive classification of blockchain-enabled applications across diverse sectors such as supply chain, business, healthcare, IoT, privacy, and data management, and we establish key themes, trends and emerging areas for research. We also point to the shortcomings identified in the relevant literature, particularly limitations the blockchain technology presents and how these limitations spawn across different sectors and industries. Building on these findings, we identify various research gaps and future exploratory directions that are anticipated to be of significant value both for academics and practitioners.},
  keywords = {Applications,Blockchain,Classification},
  annotation = {694 citations (Crossref) [2022-06-09]}
}

@online{TamingTransformersHighResolutionImageSynthesis,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  url = {https://compvis.github.io/taming-transformers/},
  urldate = {2022-05-31},
  file = {/Users/brookeryan/Zotero/storage/QRJLZHF9/taming-transformers.html}
}

@unpublished{TamingTransformersHighResolutionImageSynthesisEsser2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  date = {2021-06-23},
  number = {arXiv:2012.09841},
  eprint = {2012.09841},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.09841},
  url = {http://arxiv.org/abs/2012.09841},
  urldate = {2022-06-10},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/TamingTransformersHighResolutionImageSynthesisEsser2021.pdf;/Users/brookeryan/Zotero/storage/5Q2QH7JR/2012.html}
}

@inproceedings{TASSALAutofoldingSourceCodeSummarizationFowkes2016,
  title = {{{TASSAL}}: {{Autofolding}} for {{Source Code Summarization}}},
  shorttitle = {{{TASSAL}}},
  booktitle = {2016 {{IEEE}}/{{ACM}} 38th {{International Conference}} on {{Software Engineering Companion}} ({{ICSE-C}})},
  author = {Fowkes, Jaroslav and Chanthirasegaran, Pankajan and Ranca, Razvan and Allamanis, Miltiadis and Lapata, Mirella and Sutton, Charles},
  date = {2016-05},
  pages = {649--652},
  abstract = {We present a novel tool, TASSAL, that automatically creates a summary of each source file in a project by folding its least salient code regions. The intended use-case for our tool is the first-look problem: to help developers who are unfamiliar with a new codebase and are attempting to understand it. TASSAL is intended to aid developers in this task by folding away less informative regions of code and allowing them to focus their efforts on the most informative ones. While modern code editors do provide \textbackslash textbackslashemphcode folding to selectively hide blocks of code, it is impractical to use as folding decisions must be made manually or based on simple rules. We find through a case study that TASSAL is strongly preferred by experienced developers over simple folding baselines, demonstrating its usefulness. In short, we strongly believe TASSAL can aid program comprehension by turning code folding into a usable and valuable tool. A video highlighting the main features of TASSAL can be found at https://youtu.be/\_yu7JZgiBA4.},
  keywords = {code folding,code summarization,Conferences,Java,Optimization,program comprehension,Programming,Software engineering,Standards,topic modelling,Turning}
}

@inproceedings{TeachableMachineApproachableWebBasedToolCarney2020,
  title = {Teachable {{Machine}}: {{Approachable Web-Based Tool}} for {{Exploring Machine Learning Classification}}},
  shorttitle = {Teachable {{Machine}}},
  booktitle = {Extended {{Abstracts}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Carney, Michelle and Webster, Barron and Alvarado, Irene and Phillips, Kyle and Howell, Noura and Griffith, Jordan and Jongejan, Jonas and Pitaru, Amit and Chen, Alexander},
  date = {2020-04-25},
  series = {{{CHI EA}} '20},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3334480.3382839},
  url = {https://doi.org/10.1145/3334480.3382839},
  urldate = {2022-06-09},
  abstract = {Teachable Machine (teachablemachine.withgoogle.com) is a web-based GUI tool for creating custom machine learning classification models without specialized technical expertise. (Machine learning, or ML, lets systems learn to analyze data without being explicitly programmed.) We created it to help students, teachers, designers, and others learn about ML by creating and using their own classification models. Its broad uptake suggests it has empowered people to learn, teach, and explore ML concepts: People have created curriculum, tutorials, and other resources using Teachable Machine on topics like AI ethics at institutions including the Stanford d.school, NYU's Interactive Telecommunications Program, the MIT Media Lab, as well as creative experiments. Users in 201 countries have created over 125,000 classification models. Here we outline the project and its key contributions of (1) a flexible, approachable interface for ML classification models without ML or coding expertise, (2) a set of technical and design decisions that can inform future interactive machine learning tools, and (3) an example of how structured learning content surrounding the tool supports people accessing ML concepts.},
  isbn = {978-1-4503-6819-3},
  keywords = {human-centered ML,interactive ML}
}

@online{TeachableMachineb,
  title = {Teachable {{Machine}}},
  url = {https://teachablemachine.withgoogle.com/},
  urldate = {2022-04-20},
  abstract = {Train a computer to recognize your own images, sounds, \& poses.                 A fast, easy way to create machine learning models for your sites, apps, and more – no expertise or coding required.},
  file = {/Users/brookeryan/Zotero/storage/L46CMQCP/teachablemachine.withgoogle.com.html}
}

@inproceedings{TeachingCategoriesHumanLearnersVisualAodha2018,
  title = {Teaching {{Categories}} to {{Human Learners}} with {{Visual Explanations}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Aodha, Oisin Mac and Su, Shihan and Chen, Yuxin and Perona, Pietro and Yue, Yisong},
  date = {2018-06},
  pages = {3820--3828},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00402},
  url = {https://ieeexplore.ieee.org/document/8578500/},
  urldate = {2022-07-25},
  abstract = {We study the problem of computer-assisted teaching with explanations. Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance. However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student’s ability to learn a new concept. To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information. In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/7I77KXKR/Aodha et al. - 2018 - Teaching Categories to Human Learners with Visual .pdf}
}

@inproceedings{TeachingFoundationsAIMobileRobotsDavidA.Shammab,
  title = {Teaching the {{Foundations}} in {{AI}}: {{Mobile Robots}} and {{Symbolic Victories}}},
  author = {David A. Shamma, Carl W. Turner},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/TeachingFoundationsAIMobileRobotsDavidA.Shammab.pdf}
}

@inproceedings{TeachingSoftwareEngineeringProjectsLudewig2012,
  title = {Teaching Software Engineering with Projects},
  booktitle = {2012 {{First International Workshop}} on {{Software Engineering Education Based}} on {{Real-World Experiences}} ({{EduRex}})},
  author = {Ludewig, Jochen and Bogicevic, Ivan},
  date = {2012-06},
  pages = {25--28},
  doi = {10.1109/EduRex.2012.6225701},
  abstract = {The software engineering curriculum offered by the University of Stuttgart emphasizes project work from the first to the last semester. While some of the projects are similar to those in other programs, others are less common. In this paper, we describe an introductory course called “Program Understanding”, and the so called “Consulting Task”. We also give a short description of what we call the First Software Project and the Large Software Project. In the Program Understanding course, new students learn to understand a fairly large, complex program in order to implement some modifications. In the Large Software Project, some ten people work on a serious software development for one year. In the Consulting Task, a group of three students analyses a problem, usually one given by an industrial partner. The students investigate possible solutions, and finally deliver a recommendation. Both our experience and feedback from students, including feedback from alumni, prove that these projects are very successful and highly esteemed.},
  eventtitle = {2012 {{First International Workshop}} on {{Software Engineering Education Based}} on {{Real-World Experiences}} ({{EduRex}})},
  keywords = {Companies,Consulting Task,Educational institutions,Industries,Java,Program Understanding,Programming,Software,Software engineering,software engineering curriculum,software projects},
  annotation = {2 citations (Crossref) [2022-06-09]}
}

@inproceedings{TeachingSoftwareModellingUndergraduateIntroductionWestphal2019,
  title = {Teaching {{Software Modelling}} in an {{Undergraduate Introduction}} to {{Software Engineering}}},
  booktitle = {2019 {{ACM}}/{{IEEE}} 22nd {{International Conference}} on {{Model Driven Engineering Languages}} and {{Systems Companion}} ({{MODELS-C}})},
  author = {Westphal, Bernd},
  date = {2019-09},
  pages = {690--699},
  doi = {10.1109/MODELS-C.2019.00105},
  abstract = {In this article we present our design of an (otherwise completely ordinary) undergraduate introduction to software engineering with an emphasis on contemporary software modelling. A distinguishing aspect of our course is that we aim at a comprehensive introduction of modelling in two regards. Firstly, we introduce proper sub-languages of common modelling languages like UML class diagrams (rather than sampling examples or covering as many building blocks as possible) with a complete formal abstract syntax and semantics (so to give exact meaning to all models from the sub-language). Secondly, we emphasise issues arising from software models in the context of software engineering, e.g., that (formal) analysis results needs proper interpretation wrt. the considered software. We discuss our objectives wrt. formal modelling in software engineering, and outline the content of the course and the narratives that we use to reach these objectives. Evaluation results from four seasons of teaching the course give no indication of over-straining students wrt. level or workload.},
  eventtitle = {2019 {{ACM}}/{{IEEE}} 22nd {{International Conference}} on {{Model Driven Engineering Languages}} and {{Systems Companion}} ({{MODELS-C}})},
  keywords = {education,formal methods,software engineering,software modelling,teaching},
  annotation = {2 citations (Crossref) [2022-06-09]}
}

@article{TeachingTechTalkK12ConversationalBrummelen,
  title = {Teaching {{Tech}} to {{Talk}}: {{K-12 Conversational Artiﬁcial Intelligence Literacy Curriculum}} and {{Development Tools}}},
  author = {Brummelen, Jessica Van and Heng, Tommy and Tabunshchyk, Viktoriya},
  pages = {8},
  abstract = {With children talking to smart-speakers, smart-phones and even smart-microwaves daily, it is increasingly important to educate students on how these agents work—from underlying mechanisms to societal implications. Researchers are developing tools and curriculum to teach K-12 students broadly about artificial intelligence (AI); however, few studies have evaluated these tools with respect to AI-specific learning outcomes, and even fewer have addressed student learning about AI-based conversational agents. We evaluate our Conversational Agent Interface for MIT App Inventor and workshop curriculum with respect to 8 AI competencies from the literature. Furthermore, we analyze teacher (n=9) and student (n=47) feedback from workshops with the interface and recommend that future work leverages design considerations from the literature to optimize engagement, collaborates with teachers, and addresses a range of student abilities through pacing and opportunities for extension. We found students struggled most with the concepts of AI ethics and learning, and recommend emphasizing these topics when teaching.},
  langid = {english},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/4DUQKZKB/Brummelen et al. - Teaching Tech to Talk K-12 Conversational Artiﬁci.pdf}
}

@inproceedings{TeachingUndergraduateSoftwareEngineeringCourseDorodchi2019,
  title = {Teaching an {{Undergraduate Software Engineering Course}} Using {{Active Learning}} and {{Open Source Projects}}},
  booktitle = {2019 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  author = {Dorodchi, Mohsen and Al-Hossami, Erfan and Nagahisarchoghaei, Mohammad and Diwadkar, Rohit Shenvi and Benedict, Aileen},
  date = {2019-10},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Covington, KY, USA}},
  doi = {10.1109/FIE43999.2019.9028517},
  url = {https://ieeexplore.ieee.org/document/9028517/},
  urldate = {2021-09-28},
  eventtitle = {2019 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  isbn = {978-1-72811-746-1},
  annotation = {8 citations (Crossref) [2022-06-09]}
}

@inproceedings{TellMeWhatWrongPythonKohn2020,
  title = {Tell {{Me What}}'s {{Wrong}}: {{A Python IDE}} with {{Error Messages}}},
  shorttitle = {Tell {{Me What}}'s {{Wrong}}},
  booktitle = {Proceedings of the 51st {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Kohn, Tobias and Manaris, Bill},
  date = {2020-02-26},
  series = {{{SIGCSE}} '20},
  pages = {1054--1060},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3328778.3366920},
  url = {https://doi.org/10.1145/3328778.3366920},
  urldate = {2021-09-27},
  abstract = {Development environments play a crucial role for novice programmers. Not only do they act as interface to type in and execute programs, but a programming environment is also responsible for reporting errors, managing in- and output when the program is running, or offering the programmer access to the underlying notional machine. In recent years several new educational programming environments for Python have been presented. However, the important issue of reporting errors has rarely been addressed and evaluations often hint that students main issue is the poor quality of Python's error messages. We have therefore written an educational Python environment with enhanced error messages. This paper presents the design and rationale of its three primary features: modifications to Python, enhanced error messages, and the visual debugger.},
  isbn = {978-1-4503-6793-6},
  keywords = {compiler error messages,ide,python},
  annotation = {3 citations (Crossref) [2022-06-09]}
}

@article{TemporalDynamicsOpportunityCostsNormative,
  title = {The {{Temporal Dynamics}} of {{Opportunity Costs}}: {{A Normative Account}} of {{Cognitive Fatigue}} and {{Boredom}}},
  pages = {67},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/FRN5CAER/The Temporal Dynamics of Opportunity Costs A Norm.pdf}
}

@article{TenSimpleRulesComputationalModelingWilson2019,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  date = {2019-11-26},
  journaltitle = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  url = {https://doi.org/10.7554/eLife.49547},
  urldate = {2022-08-13},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/C5UKEAKB/Wilson and Collins - 2019 - Ten simple rules for the computational modeling of.pdf}
}

@online{ThisMightBeExactDateFriedman2022,
  title = {This {{Might Be The Exact Date Starbucks}}' {{PSL Returns In}} 2022},
  author = {Friedman, Elaina},
  date = {2022-06-22T15:46:26+00:00},
  url = {https://www.mashed.com/904107/this-might-be-the-exact-date-starbucks-psl-returns-in-2022/},
  urldate = {2022-08-17},
  abstract = {Though the summer solstice only just occurred, PSL fans are already knitting their scarves and marking their calendars for the drink's annual homecoming.},
  langid = {american},
  organization = {{Mashed.com}},
  keywords = {Advisor Literature},
  file = {/Users/brookeryan/Zotero/storage/VBWJIKHZ/popular-starbucks-menu-items-ranked-worst-to-best.html}
}

@article{ThreadCircuitsCammarata2020,
  title = {Thread: {{Circuits}}},
  shorttitle = {Thread},
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  date = {2020-03-10},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {3},
  pages = {e24},
  issn = {2476-0757},
  doi = {10.23915/distill.00024},
  url = {https://distill.pub/2020/circuits},
  urldate = {2022-09-06},
  abstract = {What can we learn if we invest heavily in reverse engineering a single neural network?},
  langid = {english},
  keywords = {Neural Networks Research Project},
  file = {/Users/brookeryan/Zotero/storage/9C3WTB2X/circuits.html}
}

@online{TipsHelpYouCreateAwesome,
  title = {6 Tips to Help You Create Awesome {{AI}} Generated Artworks with Neural Style Transfer},
  url = {https://nightcafe.studio/blogs/blog/6-tips-ai-generated-art-neural-style-transfer},
  urldate = {2022-06-01},
  abstract = {In the last year I’ve developed an intuition for how to make good AI generated artworks. Here are some tips to help you do the same.},
  langid = {english},
  organization = {{NightCafe Creator}}
}

@article{TodayWasGoodDayDailyMeyer2021,
  title = {Today {{Was}} a {{Good Day}}: {{The Daily Life}} of {{Software Developers}}},
  shorttitle = {Today {{Was}} a {{Good Day}}},
  author = {Meyer, André N. and Barr, Earl T. and Bird, Christian and Zimmermann, Thomas},
  date = {2021-05},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {5},
  pages = {863--880},
  issn = {1939-3520},
  doi = {10.1109/TSE.2019.2904957},
  abstract = {What is a good workday for a software developer? What is a typical workday? We seek to answer these two questions to learn how to make good days typical. Concretely, answering these questions will help to optimize development processes and select tools that increase job satisfaction and productivity. Our work adds to a large body of research on how software developers spend their time. We report the results from 5,971 responses of professional developers at Microsoft, who reflected about what made their workdays good and typical, and self-reported about how they spent their time on various activities at work. We developed conceptual frameworks to help define and characterize developer workdays from two new perspectives: good and typical. Our analysis confirms some findings in previous work, including the fact that developers actually spend little time on development and developers' aversion for meetings and interruptions. It also discovered new findings, such as that only 1.7 percent of survey responses mentioned emails as a reason for a bad workday, and that meetings and interruptions are only unproductive during development phases; during phases of planning, specification and release, they are common and constructive. One key finding is the importance of agency, developers' control over their workday and whether it goes as planned or is disrupted by external factors. We present actionable recommendations for researchers and managers to prioritize process and tool improvements that make good workdays typical. For instance, in light of our finding on the importance of agency, we recommend that, where possible, managers empower developers to choose their tools and tasks.},
  keywords = {Birds,Collaboration,Encoding,good workdays,job satisfaction,productivity,Productivity,quantified workplace,Software,Software developer workdays,Task analysis,Tools,typical workdays},
  annotation = {10 citations (Crossref) [2022-06-09]}
}

@online{TracingLegalSortNokariDeviantArt,
  title = {Tracing {{IS}} Legal. {{Sort}} of. by Nokari on {{DeviantArt}}},
  url = {https://www.deviantart.com/nokari/journal/Tracing-IS-legal-Sort-of-214186997},
  urldate = {2022-07-21},
  langid = {english},
  keywords = {Etsy,notion,wiki},
  file = {/Users/brookeryan/Zotero/storage/34ZEDNYP/Tracing-IS-legal-Sort-of-214186997.html}
}

@unpublished{TransfusionUnderstandingTransferLearningMedicalRaghu2019,
  title = {Transfusion: {{Understanding Transfer Learning}} for {{Medical Imaging}}},
  shorttitle = {Transfusion},
  author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  date = {2019-10-29},
  eprint = {1902.07208},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.07208},
  urldate = {2021-10-03},
  abstract = {Transfer learning from natural image datasets, particularly ImageNet, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to ImageNet architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brookeryan/Zotero/storage/GSTHKXW9/Raghu et al. - 2019 - Transfusion Understanding Transfer Learning for M.pdf;/Users/brookeryan/Zotero/storage/56LDV34H/1902.html}
}

@article{TroubleSTEAMWhyWeUseMejias2021,
  title = {The Trouble with {{STEAM}} and Why We Use It Anyway},
  author = {Mejias, Sam and Thompson, Naomi and Sedas, Raul Mishael and Rosin, Mark and Soep, Elisabeth and Peppler, Kylie and Roche, Joseph and Wong, Jen and Hurley, Mairéad and Bell, Philip and Bevan, Bronwyn},
  date = {2021},
  journaltitle = {Science Education},
  volume = {105},
  number = {2},
  pages = {209--231},
  issn = {1098-237X},
  doi = {10.1002/sce.21605},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sce.21605},
  urldate = {2022-06-10},
  abstract = {As an emerging field of theory, research, and practice, STEAM (Science, Technology, Engineering, Arts, and Mathematics) has received attention for its efforts to incorporate the arts into the rubric of STEM (Science, Technology, Engineering, and Mathematics) learning. In particular, many informal educators have embraced it as an inclusive and authentic approach to engaging young people with STEM. Yet, as with many nascent fields, the conceptualization and usage of STEAM is somewhat ambivalent and weakly theorized. On the one hand, STEAM offers significant promise through its focus on multiple ways of knowing and new pathways to equitable learning. On the other hand, it is often deployed in theory, pedagogy, and practice in ambiguous or potentially problematic ways toward varying ends. This paper attempts to disentangle some of the key tensions and contradictions of the STEAM concept as currently operationalized in educational research, policy, and practice. We pay particular attention to the transformative learning potential supported by contexts where STEAM is conceptualized as both pedagogical and mutually instrumental. That is, neither STEM nor arts are privileged over the other, but both are equally in play. We link the possibilities suggested by this approach to emerging theories for understanding how designing for and surfacing epistemic practices linked to the relevant disciplines being integrated into STEAM programs may point the way toward resolving tensions in inter- and transdisciplinary learning approaches.},
  langid = {english},
  keywords = {art,epistemic practices,instrumentalism,ObsCite,science,STEAM,STEM,transdisciplinary},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sce.21605},
  file = {/Users/brookeryan/Documents/Obsidian/reference/TroubleSTEAMWhyWeUseMejias2021.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/TroubleSTEAMWhyWeUseMejias2021.pdf;/Users/brookeryan/Zotero/storage/3GXWVXR4/sce.html}
}

@inproceedings{TrustHumanAIInteractionScopingOutUeno2022,
  title = {Trust in {{Human-AI Interaction}}: {{Scoping Out Models}}, {{Measures}}, and {{Methods}}},
  shorttitle = {Trust in {{Human-AI Interaction}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems Extended Abstracts}}},
  author = {Ueno, Takane and Sawa, Yuto and Kim, Yeongdae and Urakami, Jacqueline and Oura, Hiroki and Seaborn, Katie},
  date = {2022-04-27},
  pages = {1--7},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491101.3519772},
  url = {https://dl.acm.org/doi/10.1145/3491101.3519772},
  urldate = {2022-08-30},
  abstract = {Trust has emerged as a key factor in people’s interactions with AIinfused systems. Yet, little is known about what models of trust have been used and for what systems: robots, virtual characters, smart vehicles, decision aids, or others. Moreover, there is yet no known standard approach to measuring trust in AI. This scoping review maps out the state of affairs on trust in human-AI interaction (HAII) from the perspectives of models, measures, and methods. Findings suggest that trust is an important and multi-faceted topic of study within HAII contexts. However, most work is under-theorized and under-reported, generally not using established trust models and missing details about methods, especially Wizard of Oz. We offer several targets for systematic review work as well as a research agenda for combining the strengths and addressing the weaknesses of the current literature.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9156-6},
  langid = {english},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/TT5AK5TJ/Ueno et al. - 2022 - Trust in Human-AI Interaction Scoping Out Models,.pdf}
}

@inproceedings{TurnHeatUsingHeatMapsEdmison2020,
  title = {Turn up the {{Heat}}!: {{Using Heat Maps}} to {{Visualize Suspicious Code}} to {{Help Students Successfully Complete Programming Problems Faster}}},
  shorttitle = {Turn up the {{Heat}}!},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  author = {Edmison, Bob and Edwards, Stephen H.},
  date = {2020-10},
  pages = {34--44},
  abstract = {The following topics are dealt with: computer science education; educational courses; computer aided instruction; software engineering; teaching; programming; formal specification; further education; educational institutions; team working.},
  eventtitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  keywords = {automated grading,automatic fault localization,debugging,Debugging,heat map,Heating systems,Measurement,SFL,Software,spectrum-based fault localization,statistical fault localization,testing,Testing,Tools,visualization,Visualization}
}

@article{TUTORIALAIResearchCodingArtCiechanowski2020,
  title = {{{TUTORIAL}}: {{AI}} Research without Coding: {{The}} Art of Fighting without Fighting: {{Data}} Science for Qualitative Researchers},
  shorttitle = {{{TUTORIAL}}},
  author = {Ciechanowski, Leon and Jemielniak, Dariusz and Gloor, Peter A.},
  date = {2020-09-01},
  journaltitle = {Journal of Business Research},
  shortjournal = {Journal of Business Research},
  volume = {117},
  pages = {322--330},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2020.06.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0148296320303854},
  urldate = {2022-06-10},
  abstract = {In this tutorial, we show how to scrape and collect online data, perform sentiment analysis, social network analysis, tribe finding, and Wikidata cross-checks, all without using a single line of programming code. In a step-by-step example, we use self-collected data to perform several analyses of the glass ceiling. Our tutorial can serve as a standalone introduction to data science for qualitative researchers and business researchers, who have avoided learning to program. It should also be useful for experienced data scientists who want to learn about the tools that will allow them to collect and analyze data more easily and effectively.},
  langid = {english},
  keywords = {Data scraping,ObsCite,Sentiment analysis,Tribe finding,Twitter,Wikidata},
  file = {/Users/brookeryan/Zotero/storage/V5ZMH29R/S0148296320303854.html}
}

@inproceedings{UMLAUTDebuggingDeepLearningProgramsSchoop2021,
  title = {{{UMLAUT}}: {{Debugging Deep Learning Programs}} Using {{Program Structure}} and {{Model Behavior}}},
  shorttitle = {{{UMLAUT}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schoop, Eldon and Huang, Forrest and Hartmann, Bjoern},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445538},
  url = {https://doi.org/10.1145/3411764.3445538},
  urldate = {2022-06-09},
  abstract = {Training deep neural networks can generate non-descriptive error messages or produce unusual output without any explicit errors at all. While experts rely on tacit knowledge to apply debugging strategies, non-experts lack the experience required to interpret model output and correct Deep Learning (DL) programs. In this work, we identify DL debugging heuristics and strategies used by experts, andIn this work, we categorize the types of errors novices run into when writing ML code, and map them onto opportunities where tools could help. We use them to guide the design of Umlaut. Umlaut checks DL program structure and model behavior against these heuristics; provides human-readable error messages to users; and annotates erroneous model output to facilitate error correction. Umlaut links code, model output, and tutorial-driven error messages in a single interface. We evaluated Umlaut in a study with 15 participants to determine its effectiveness in helping developers find and fix errors in their DL programs. Participants using Umlaut found and fixed significantly more bugs and were able to implement fixes for more bugs compared to a baseline condition.},
  isbn = {978-1-4503-8096-6},
  keywords = {AI Education Project,End-User ML,ML Debugging,ML Development,notion,systematic literature review},
  file = {/Users/brookeryan/Zotero/storage/JL99VSQS/Schoop et al_2021_UMLAUT.pdf}
}

@misc{UnderstandingCreatingArtAIReviewCetinic2021,
  title = {Understanding and {{Creating Art}} with {{AI}}: {{Review}} and {{Outlook}}},
  shorttitle = {Understanding and {{Creating Art}} with {{AI}}},
  author = {Cetinic, Eva and She, James},
  date = {2021-02-17},
  number = {arXiv:2102.09109},
  eprint = {2102.09109},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.09109},
  urldate = {2022-08-29},
  abstract = {Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art, motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This paper provides an integrated review of two facets of AI and art: 1) AI is used for art analysis and employed on digitized artwork collections; 2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, computational aesthetics, etc. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Example Surveys},
  file = {/Users/brookeryan/Zotero/storage/3FHEPXJI/Cetinic and She - 2021 - Understanding and Creating Art with AI Review and.pdf}
}

@online{UnderstandingVQVAEDALLEExplainedPt,
  title = {Understanding {{VQ-VAE}} ({{DALL-E Explained Pt}}. 1) - {{ML}}@{{B Blog}}},
  url = {https://ml.berkeley.edu/blog/posts/vq-vae/},
  urldate = {2022-05-31},
  abstract = {VQ-VAE is a powerful technique for learning discrete representations of complex data types like images, video, or audio. This technique has played a key role in recent state of the art works like OpenAI's DALL-E and Jukebox models.},
  langid = {english},
  organization = {{Understanding VQ-VAE (DALL-E Explained Pt. 1) - ML@B Blog}},
  file = {/Users/brookeryan/Zotero/storage/F3GUJARY/vq-vae.html}
}

@inproceedings{UnlimitedTraceTutorLearningCodeQi2020,
  title = {Unlimited {{Trace Tutor}}: {{Learning Code Tracing With Automatically Generated Programs}}},
  shorttitle = {Unlimited {{Trace Tutor}}},
  booktitle = {Proceedings of the 51st {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Qi, Ruixiang and Fossati, Davide},
  date = {2020-02-26},
  series = {{{SIGCSE}} '20},
  pages = {427--433},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3328778.3366939},
  url = {https://doi.org/10.1145/3328778.3366939},
  urldate = {2021-09-27},
  abstract = {Previous research showed that creating specific types of tracing tables helps students learn code tracing, a fundamental skill in computer programming. This paper introduces Unlimited Trace Tutor, the first version of a code tracing tutoring system that can automatically generate tracing problems and create such tracing tables. We conducted a pilot experiment with volunteer students from an introductory level Computer Science course. We found that our software effectively helps student learn tracing "for" loops, "while" loops, and "if" statements. In this paper we describe the system's architecture, our algorithms for generating code and tracing tables, and the promising results of our pilot experiment.},
  isbn = {978-1-4503-6793-6},
  keywords = {automatic code generation,code tracing,tutoring systems},
  annotation = {2 citations (Crossref) [2022-06-09]}
}

@inproceedings{UseAIGeneratedVisualMediaInterviewsJennings2021,
  title = {Use of {{AI-Generated Visual Media}} in {{Interviews}} to {{Understand Power Differentials}} in {{Gender}}, {{Romantic}}, and {{Sexual Minority Students}}},
  booktitle = {2021 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  author = {Jennings, Madeleine and Sandoval, Jorge and Sanders, Jeanne and Koro, Mirka and Kellam, Nadia and Jayasuriya, Suren},
  date = {2021-10-13},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{Lincoln, NE, USA}},
  doi = {10.1109/FIE49875.2021.9637396},
  url = {https://ieeexplore.ieee.org/document/9637396/},
  urldate = {2022-06-10},
  abstract = {This work-in-progress briefly outlines the theoretical background, methods, and preliminary results of a qualitative study conducted with gender, romantic, and sexual minority (GRSM) students immersed in higher education spaces. We elaborate on the efficacy of our innovative qualitative methodologies through the use of AI-human art-making interactions during our interviews, which helped to produce richer qualitative data from our participants. Our methodology was constructed using a Foucauldian theoretical framework to inform the framework of this study, focusing explicitly on GRSM students’ experiences with power in higher education and when using technology, as well as the ways in which they resist power through the use of technology and AI-generated visual media.},
  eventtitle = {2021 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  isbn = {978-1-66543-851-3},
  langid = {english},
  file = {/Users/brookeryan/Zotero/storage/BHBP9B6D/Jennings et al. - 2021 - Use of AI-Generated Visual Media in Interviews to .pdf}
}

@inproceedings{UseCodeReadingTeachingProgrammingBusjahn2013,
  title = {The Use of Code Reading in Teaching Programming},
  booktitle = {Proceedings of the 13th {{Koli Calling International Conference}} on {{Computing Education Research}}},
  author = {Busjahn, Teresa and Schulte, Carsten},
  date = {2013-11},
  series = {Koli {{Calling}} '13},
  pages = {3--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2526968.2526969},
  url = {https://doi.org/10.1145/2526968.2526969},
  urldate = {2021-10-22},
  abstract = {Programming is an intertwined process of reading and writing. So far, computing education research has often focused on the writing part. This paper takes a further look into the role of reading source code in learning to program. In order to complement the findings from literature, we conducted interviews with programming instructors using the miracle question, on the role of code reading and comprehension. The analysis of these interviews describes this role in terms of the five categories conceptualization, occurrences, and effects of successful code reading, challenges for learners, as well as approaches to facilitate code reading. As a result, we suggest to take a further look into the different reading processes involved in programming, in order to add to the knowledge about programming instruction.},
  isbn = {978-1-4503-2482-3},
  keywords = {code comprehension,code reading,CS ed research,educational research,program comprehension,teaching programming},
  annotation = {27 citations (Crossref) [2022-06-09]}
}

@online{UsingArtificialIntelligenceMakeArt,
  title = {Using {{Artificial Intelligence To Make Art}}: {{Wombo}} and {{Deep Dream Generator}} – {{SLAP HAPPY LARRY}}},
  shorttitle = {Using {{Artificial Intelligence To Make Art}}},
  url = {https://www.slaphappylarry.com/using-artificial-intelligence-to-make-art-wombo-and-deep-dream-generator/},
  urldate = {2022-06-10},
  langid = {australian},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Zotero/storage/3IANBGL6/using-artificial-intelligence-to-make-art-wombo-and-deep-dream-generator.html}
}

@online{UsingDeepReinforcementLearningReveal,
  title = {Using Deep Reinforcement Learning to Reveal How the Brain Encodes Abstract State-Space Representations in High-Dimensional Environments | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neuron.2020.11.021},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627320308990?token=1B1097F343EB87485720B2D31A2D32952EDC8CC4448F65E2D63084260536E187E93DBE011557D4D31328188CE42467E6&originRegion=us-east-1&originCreation=20220722223823},
  urldate = {2022-07-22},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/R62Z5P2K/Using deep reinforcement learning to reveal how th.pdf;/Users/brookeryan/Zotero/storage/CLD7RF5N/S0896627320308990.html}
}

@article{UsingGoaldrivenDeepLearningModelsYamins2016,
  title = {Using Goal-Driven Deep Learning Models to Understand Sensory Cortex},
  author = {Yamins, Daniel L K and DiCarlo, James J},
  date = {2016-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {3},
  pages = {356--365},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4244},
  url = {http://www.nature.com/articles/nn.4244},
  urldate = {2021-10-03},
  langid = {english},
  keywords = {Literature Review,notion},
  annotation = {688 citations (Crossref) [2022-06-09]},
  file = {/Users/brookeryan/Zotero/storage/NSTRU3TS/Yamins and DiCarlo - 2016 - Using goal-driven deep learning models to understa.pdf}
}

@inproceedings{UsingHypothesesDebuggingAidAlaboudi2020,
  title = {Using {{Hypotheses}} as a {{Debugging Aid}}},
  booktitle = {2020 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Alaboudi, Abdulaziz and LaToza, Thomas D.},
  date = {2020-08},
  pages = {1--9},
  doi = {10.1109/VL/HCC50065.2020.9127273},
  abstract = {As developers debug, developers formulate hypotheses about the cause of the defect and gather evidence to test these hypotheses. To better understand the role of hypotheses in debugging, we conducted two studies. In a preliminary study, we found that, even with the benefit of modern internet resources, incorrect hypotheses can cause developers to investigate irrelevant information and block progress. We then conducted a controlled experiment where 20 developers debugged and recorded their hypotheses. We found that developers have few hypotheses, two per defect. Having a correct hypothesis early strongly predicted later success. We also studied the impact of two debugging aids: fault locations and potential hypotheses. Offering fault locations did not help developers formulate more correct hypotheses or debug more successfully. In contrast, offering potential hypotheses made developers six times more likely to succeed. These results demonstrate the potential of future debugging tools that enable finding and sharing relevant hypotheses.},
  keywords = {Debugging,fault localization,hypotheses},
  annotation = {1 citations (Crossref) [2022-06-09]}
}

@article{UsingLargescaleExperimentsMachineLearningPeterson2021,
  title = {Using Large-Scale Experiments and Machine Learning to Discover Theories of Human Decision-Making},
  author = {Peterson, Joshua C. and Bourgin, David D. and Agrawal, Mayank and Reichman, Daniel and Griffiths, Thomas L.},
  date = {2021-06-11},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {372},
  number = {6547},
  pages = {1209--1214},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abe2629},
  url = {https://www.science.org/doi/10.1126/science.abe2629},
  urldate = {2022-08-28},
  abstract = {Discovering better theories                            Theories of human decision-making have proliferated in recent years. However, these theories are often difficult to distinguish from each other and offer limited improvement in accounting for patterns in decision-making over earlier theories. Peterson               et al.               leverage machine learning to evaluate classical decision theories, increase their predictive power, and generate new theories of decision-making (see the Perspective by Bhatia and He). This method has implications for theory generation in other domains.                                         Science               , abe2629, this issue p.               1209               ; see also abi7668, p.               1150                        ,              A machine learning approach suggests that people make decisions in a way that violates the assumptions of classic decision-making theories.           ,              Predicting and understanding how people make decisions has been a long-standing goal in many fields, with quantitative models of human decision-making informing research in both the social sciences and engineering. We show how progress toward this goal can be accelerated by using large datasets to power machine-learning algorithms that are constrained to produce interpretable psychological theories. Conducting the largest experiment on risky choice to date and analyzing the results using gradient-based optimization of differentiable decision theories implemented through artificial neural networks, we were able to recapitulate historical discoveries, establish that there is room to improve on existing theories, and discover a new, more accurate model of human decision-making in a form that preserves the insights from centuries of research.},
  langid = {english},
  file = {/Users/brookeryan/Zotero/storage/XM7LWVKD/Peterson et al. - 2021 - Using large-scale experiments and machine learning.pdf}
}

@article{UsingMachineLearningSupportPedagogyMorris2013,
  title = {Using Machine Learning to Support Pedagogy in the Arts},
  author = {Morris, Dan and Fiebrink, Rebecca},
  date = {2013-12-01},
  journaltitle = {Personal and Ubiquitous Computing},
  shortjournal = {Pers Ubiquit Comput},
  volume = {17},
  number = {8},
  pages = {1631--1635},
  issn = {1617-4917},
  doi = {10.1007/s00779-012-0526-1},
  url = {https://doi.org/10.1007/s00779-012-0526-1},
  urldate = {2022-06-10},
  abstract = {Teaching artistic skills to children presents a unique challenge: High-level creative and social elements of an artistic discipline are often the most engaging and the most likely to sustain student enthusiasm, but these skills rely on low-level sensorimotor capabilities, and in some cases rote knowledge, which are often tedious to develop. We hypothesize that computer-based learning can play a critical role in connecting “bottom-up” (sensorimotor-first) learning in the arts to “top-down” (creativity-first) learning, by employing machine learning and artificial intelligence techniques that can play the role of the sensorimotor expert. This approach allows learners to experience components of higher-level creativity and social interaction even before developing the prerequisite sensorimotor skills or academic knowledge.},
  langid = {english},
  keywords = {Creativity,Education,Machine learning,ObsCite},
  file = {/Users/brookeryan/Documents/Obsidian/reference/UsingMachineLearningSupportPedagogyMorris2013.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/UsingMachineLearningSupportPedagogyMorris2013.pdf}
}

@online{UsingOverleafZoteroMeiYuxuan,
  title = {Using {{Overleaf}} with {{Zotero}}},
  author = {Mei, Yuxuan},
  url = {https://homes.cs.washington.edu/~ym2552/blogs/overleaf-with-zotero.html},
  urldate = {2022-06-07},
  keywords = {Blog References,notion},
  file = {/Users/brookeryan/Zotero/storage/M6S329ER/overleaf-with-zotero.html}
}

@article{VerbalCountingTimingNumberAcquisitionBoni2022,
  title = {Verbal Counting and the Timing of Number Acquisition in an Indigenous {{Amazonian}} Group},
  author = {Boni, Isabelle and Jara-Ettinger, Julian and Sackstein, Sophie and Piantadosi, Steven T.},
  date = {2022-08-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {17},
  number = {8},
  pages = {e0270739},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0270739},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0270739},
  urldate = {2022-08-10},
  abstract = {Children in industrialized cultures typically succeed on Give-N, a test of counting ability, by age 4. On the other hand, counting appears to be learned much later in the Tsimane’, an indigenous group in the Bolivian Amazon. This study tests three hypotheses for what may cause this difference in timing: (a) Tsimane’ children may be shy in providing behavioral responses to number tasks, (b) Tsimane’ children may not memorize the verbal list of number words early in acquisition, and/or (c) home environments may not support mathematical learning in the same way as in US samples, leading Tsimane’ children to primarily acquire mathematics through formalized schooling. Our results suggest that most of our subjects are not inhibited by shyness in responding to experimental tasks. We also find that Tsimane’ children (N = 100, ages 4-11) learn the verbal list later than US children, but even upon acquiring this list, still take time to pass Give-N tasks. We find that performance in counting varies across tasks and is related to formal schooling. These results highlight the importance of formal education, including instruction in the count list, in learning the meanings of the number words.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/LSF4UUZH/Boni et al. - 2022 - Verbal counting and the timing of number acquisiti.pdf}
}

@inproceedings{VisualizingTestsuitesAidSoftwareUnderstandingCornelissen2007,
  title = {Visualizing {{Testsuites}} to {{Aid}} in {{Software Understanding}}},
  booktitle = {11th {{European Conference}} on {{Software Maintenance}} and {{Reengineering}} ({{CSMR}}'07)},
  author = {Cornelissen, Bas and van Deursen, Arie and Moonen, Leon and Zaidman, Andy},
  options = {useprefix=true},
  date = {2007-03},
  pages = {213--222},
  issn = {1534-5351},
  doi = {10.1109/CSMR.2007.54},
  abstract = {Agile software development methods such as extreme programming have brought renewed attention to testing during the software development process, both as a quality assurance method and as a form of live documentation. It is for this reason that a software system's testsuite is an ideal starting point for gaining knowledge about its inner workings. In this paper, we propose to use sequence diagrams to visualize information that is dynamically obtained from testsuites. We employ abstraction techniques such as constructor hiding and stack depth limitation to make the diagrams more scalable. We use JPacman as a case study to validate our results by consulting with domain experts, and use their feedback to fine-tune our techniques},
  eventtitle = {11th {{European Conference}} on {{Software Maintenance}} and {{Reengineering}} ({{CSMR}}'07)},
  keywords = {Documentation,Feedback,Programming,Quality assurance,Software maintenance,Software quality,Software systems,Software testing,System testing,Visualization},
  annotation = {38 citations (Crossref) [2022-06-09]}
}

@misc{VisualKnowledgeTracingKondapaneni2022,
  title = {Visual {{Knowledge Tracing}}},
  author = {Kondapaneni, Neehar and Perona, Pietro and Mac Aodha, Oisin},
  date = {2022-07-21},
  number = {arXiv:2207.10157},
  eprint = {2207.10157},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10157},
  urldate = {2022-07-25},
  abstract = {Each year, thousands of people learn new visual categorization tasks – radiologists learn to recognize tumors, birdwatchers learn to distinguish similar species, and crowd workers learn how to annotate valuable data for applications like autonomous driving. As humans learn, their brain updates the visual features it extracts and attend to, which ultimately informs their final classification decisions. In this work, we propose a novel task of tracing the evolving classification behavior of human learners as they engage in challenging visual classification tasks. We propose models that jointly extract the visual features used by learners as well as predicting the classification functions they utilize. We collect three challenging new datasets from real human learners in order to evaluate the performance of different visual knowledge tracing methods. Our results show that our recurrent models are able to predict the classification behavior of human learners on three challenging medical image and species identification tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/ZZ6TRRP8/Kondapaneni et al. - 2022 - Visual Knowledge Tracing.pdf}
}

@online{VisualSearchAttentionalBlinkIntroduction,
  title = {Visual {{Search}} and {{Attentional Blink}} | {{Introduction}} to {{Psychology}} | {{Brain}} and {{Cognitive Sciences}} | {{MIT OpenCourseWare}}},
  url = {https://ocw.mit.edu/courses/9-00sc-introduction-to-psychology-fall-2011/resources/visual-search-and-attentional-blink/},
  urldate = {2022-05-19},
  keywords = {COGS 269,notion},
  file = {/Users/brookeryan/Zotero/storage/LA5I98CP/visual-search-and-attentional-blink.html}
}

@unpublished{VQGANCLIPOpenDomainImageGenerationCrowson2022,
  title = {{{VQGAN-CLIP}}: {{Open Domain Image Generation}} and {{Editing}} with {{Natural Language Guidance}}},
  shorttitle = {{{VQGAN-CLIP}}},
  author = {Crowson, Katherine and Biderman, Stella and Kornis, Daniel and Stander, Dashiell and Hallahan, Eric and Castricato, Louis and Raff, Edward},
  date = {2022-04-18},
  number = {arXiv:2204.08583},
  eprint = {2204.08583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.08583},
  urldate = {2022-06-10},
  abstract = {Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/VQGANCLIPOpenDomainImageGenerationCrowson2022.pdf;/Users/brookeryan/Zotero/storage/KKPLIXS5/2204.html}
}

@misc{WelcomeApacheLuceneApache,
  title = {Welcome to {{Apache Lucene}}},
  author = {{Apache}},
  url = {https://lucene.apache.org/},
  urldate = {2021-12-10}
}

@online{WelcomeApacheLuceneApachea,
  title = {Welcome to {{Apache Lucene}}},
  author = {Apache},
  url = {https://lucene.apache.org/},
  urldate = {2021-12-10}
}

@online{WentViralBadWay2022,
  title = {I {{Went Viral}} in the {{Bad Way}}},
  date = {2022-08-17T10:00:46},
  url = {https://newsletters.theatlantic.com/galaxy-brain/62fc502abcbd490021afea1e/twitter-viral-outrage-ai-art/},
  urldate = {2022-08-29},
  abstract = {A few lessons from my mistake},
  langid = {english},
  organization = {{Galaxy Brain}},
  keywords = {Key Literature Review},
  file = {/Users/brookeryan/Zotero/storage/LF3NVR38/twitter-viral-outrage-ai-art.html}
}

@incollection{WhatAILiteracyCompetenciesDesignLong2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  date = {2020-04-21},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3313831.3376727},
  urldate = {2022-06-09},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  isbn = {978-1-4503-6708-0},
  keywords = {AI education,AI Education Project,AI for K-12,AI literacy,artificial intelligence,computing education,machine learning,notion,ObsCite,systematic literature review},
  file = {/Users/brookeryan/Documents/Obsidian/reference/WhatAILiteracyCompetenciesDesignLong2020.md;/Users/brookeryan/Documents/Obsidian/reference/zotero/WhatAILiteracyCompetenciesDesignLong22.pdf}
}

@inproceedings{WhatAILiteracyCompetenciesDesignLong2020d,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  date = {2020-04-21},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376727},
  url = {https://dl.acm.org/doi/10.1145/3313831.3376727},
  urldate = {2022-04-20},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learnercentered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper’s contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-6708-0},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/WhatAILiteracyCompetenciesDesignLong2020d.pdf}
}

@article{WhatEmotionAdolphs2019,
  title = {What Is an Emotion?},
  author = {Adolphs, Ralph and Mlodinow, Leonard and Barrett, Lisa Feldman},
  date = {2019},
  journaltitle = {Current Biology},
  volume = {29},
  number = {20},
  pages = {R1060--R1064},
  publisher = {{Elsevier}},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/7QP23JKI/Adolphs et al. - 2019 - What is an emotion.pdf}
}

@article{WhatGoodStatementPurposeCallan,
  title = {What’s a {{Good Statement}} of {{Purpose}}},
  author = {Callan, Eamonn},
  pages = {3},
  langid = {english},
  keywords = {Ph.D. Applications},
  file = {/Users/brookeryan/Zotero/storage/8Z7NQ4UN/Callan - What’s a Good Statement of Purpose.pdf}
}

@article{WhatHowWhyNaturalisticBehaviorKennedy2022,
  title = {The What, How, and Why of Naturalistic Behavior},
  author = {Kennedy, Ann},
  date = {2022-06-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {74},
  pages = {102549},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2022.102549},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438822000435},
  urldate = {2022-07-25},
  abstract = {In the past few years, advances in machine learning have fueled an explosive growth of descriptive and generative models of animal behavior. These new approaches offer higher levels of detail and granularity than has previously been possible, allowing for fine-grained segmentation of animals' actions and precise quantitative mappings between an animal's sensory environment and its behavior. How can these new methods help us understand the governing principles shaping complex and naturalistic behavior? In this review, we will recap ways in which our ability to detect and model behavior have improved in recent years, and consider how these techniques might be used to revisit classical normative theories of behavioral control.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/4IGZI9MK/Kennedy - 2022 - The what, how, and why of naturalistic behavior.pdf;/Users/brookeryan/Zotero/storage/H3FDTGQN/S0959438822000435.html}
}

@inproceedings{WhatMakesGreatSoftwareEngineer2015,
  title = {What {{Makes A Great Software Engineer}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  date = {2015-05},
  publisher = {{IEEE}},
  location = {{Florence, Italy}},
  abstract = {Good software engineers are essential to the creation of good software. However, most of what we know about softwareengineering expertise are vague stereotypes, such as ‘excellent communicators’ and ‘great teammates’. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
  eventtitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}} ({{ICSE}})},
  isbn = {978-1-4799-1934-5},
  langid = {english},
  file = {/Users/brookeryan/Zotero/storage/Q73DXY4H/2015 - [No title found].pdf}
}

@inproceedings{WhatMakesGreatSoftwareEngineerLi2015,
  title = {What {{Makes}} a {{Great Software Engineer}}?},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Li, Paul Luo and Ko, Andrew J. and Zhu, Jiamin},
  date = {2015-05},
  volume = {1},
  pages = {700--710},
  doi = {10.1109/ICSE.2015.335},
  abstract = {Good software engineers are essential to the creation of good software. However, most of what we know about software-engineering expertise are vague stereotypes, such as 'excellent communicators' and 'great teammates'. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
  keywords = {Companies,expertise,Interviews,Knowledge engineering,Lead,Software,Software engineering,Software engineers,teamwork},
  annotation = {31 citations (Crossref) [2022-06-09]}
}

@article{WhatReinforcementLearningModelsMeasureEckstein2021,
  title = {What Do Reinforcement Learning Models Measure? {{Interpreting}} Model Parameters in Cognition and Neuroscience},
  shorttitle = {What Do Reinforcement Learning Models Measure?},
  author = {Eckstein, Maria K and Wilbrecht, Linda and Collins, Anne GE},
  date = {2021-10-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Value Based Decision-Making},
  volume = {41},
  pages = {128--137},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2021.06.004},
  url = {https://www.sciencedirect.com/science/article/pii/S2352154621001236},
  urldate = {2022-08-13},
  abstract = {Reinforcement learning (RL) is a concept that has been invaluable to fields including machine learning, neuroscience, and cognitive science. However, what RL entails differs between fields, leading to difficulties when interpreting and translating findings. After laying out these differences, this paper focuses on cognitive (neuro)science to discuss how we as a field might overinterpret RL modeling results. We too often assume—implicitly—that modeling results generalize between tasks, models, and participant populations, despite negative empirical evidence for this assumption. We also often assume that parameters measure specific, unique (neuro)cognitive processes, a concept we call interpretability, when evidence suggests that they capture different functions across studies and tasks. We conclude that future computational research needs to pay increased attention to implicit assumptions when using RL models, and suggest that a more systematic understanding of contextual factors will help address issues and improve the ability of RL to explain brain and behavior.},
  langid = {english},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/ZWH33NDY/Eckstein et al. - 2021 - What do reinforcement learning models measure Int.pdf}
}

@misc{WhatRNNLanguageModelsLearnWilcox2018,
  title = {What Do {{RNN Language Models Learn}} about {{Filler-Gap Dependencies}}?},
  author = {Wilcox, Ethan and Levy, Roger and Morita, Takashi and Futrell, Richard},
  date = {2018-08-31},
  number = {arXiv:1809.00042},
  eprint = {1809.00042},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.00042},
  url = {http://arxiv.org/abs/1809.00042},
  urldate = {2022-08-13},
  abstract = {RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance filler-gap dependencies and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler-gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler-gap dependencies, known as island constraints: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.},
  archiveprefix = {arXiv},
  keywords = {Advisor Literature,notion},
  file = {/Users/brookeryan/Zotero/storage/P6AX3CM5/Wilcox et al. - 2018 - What do RNN Language Models Learn about Filler-Gap.pdf;/Users/brookeryan/Zotero/storage/8Y4U7PLA/1809.html}
}

@article{WhatTheyLearnWhenTheyStrawhacker2019a,
  title = {What They Learn When They Learn Coding: Investigating Cognitive Domains and Computer Programming Knowledge in Young Children},
  shorttitle = {What They Learn When They Learn Coding},
  author = {Strawhacker, Amanda and Bers, Marina Umaschi},
  date = {2019-06},
  journaltitle = {Educational Technology Research and Development},
  shortjournal = {Education Tech Research Dev},
  volume = {67},
  number = {3},
  pages = {541--575},
  issn = {1042-1629, 1556-6501},
  doi = {10.1007/s11423-018-9622-x},
  url = {http://link.springer.com/10.1007/s11423-018-9622-x},
  urldate = {2022-04-20},
  abstract = {Computer programming for young children has grown in popularity among both educators and product developers, but still relatively little is known about what skills children are developing when they code. This study investigated N = 57 Kindergarten through second grade children’s performance on a programming assessment after engaging in a 6-week curricular intervention. Children used the ScratchJr programming tool to create animated stories, collages, and games. At the end of the learning intervention, children were assessed on their knowledge of the ScratchJr language and underlying reasoning. Specifically, we explored children’s errors on the assessment to determine evidence of domain-specific reasoning (e.g. mathematic, verbal, causal). Results show that while all students mastered foundational coding concepts, there were marked differences in performance and comprehension across the three grade levels. Interpretation of results suggests a developmental progression inherent in programming knowledge acquisition.; Implications for computer programming education and developmental research are discussed.},
  langid = {english},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/WhatTheyLearnWhenTheyStrawhacker2019a.pdf}
}

@inproceedings{WhatWeMeanAccessibilityResearchMack2021,
  title = {What {{Do We Mean}} by \&\#x201c;{{Accessibility Research}}\&\#x201d;? {{A Literature Survey}} of {{Accessibility Papers}} in {{CHI}} and {{ASSETS}} from 1994 to 2019},
  shorttitle = {What {{Do We Mean}} by \&\#x201c;{{Accessibility Research}}\&\#x201d;?},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mack, Kelly and McDonnell, Emma and Jain, Dhruv and Lu Wang, Lucy and E. Froehlich, Jon and Findlater, Leah},
  date = {2021-05-06},
  series = {{{CHI}} '21},
  pages = {1--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445412},
  url = {https://doi.org/10.1145/3411764.3445412},
  urldate = {2022-06-09},
  abstract = {Accessibility research has grown substantially in the past few decades, yet there has been no literature review of the field. To understand current and historical trends, we created and analyzed a dataset of accessibility papers appearing at CHI and ASSETS since ASSETS’ founding in 1994. We qualitatively coded areas of focus and methodological decisions for the past 10 years (2010-2019, N=506 papers), and analyzed paper counts and keywords over the full 26 years (N=836 papers). Our findings highlight areas that have received disproportionate attention and those that are underserved—for example, over 43\% of papers in the past 10 years are on accessibility for blind and low vision people. We also capture common study characteristics, such as the roles of disabled and nondisabled participants as well as sample sizes (e.g., a median of 13 for participant groups with disabilities and older adults). We close by critically reflecting on gaps in the literature and offering guidance for future work in the field.},
  isbn = {978-1-4503-8096-6},
  keywords = {Accessibility,AI Education Project,assistive technology,disability,literature review,notion,systematic literature review},
  file = {/Users/brookeryan/Zotero/storage/HSIUZDWI/Mack et al_2021_What Do We Mean by &#x201c\;Accessibility Research&#x201d\;.pdf}
}

@article{WhyHowBrainWeightsContributionsO’Doherty2021,
  title = {Why and How the Brain Weights Contributions from a Mixture of Experts},
  author = {O’Doherty, John P. and Lee, Sang Wan and Tadayonnejad, Reza and Cockburn, Jeff and Iigaya, Kyo and Charpentier, Caroline J.},
  date = {2021-04},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {123},
  pages = {14--23},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2020.10.022},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763420306266},
  urldate = {2022-07-25},
  langid = {english},
  keywords = {Advisor Literature,computational,notion},
  file = {/Users/brookeryan/Zotero/storage/C4Q3YA27/O’Doherty et al. - 2021 - Why and how the brain weights contributions from a.pdf}
}

@online{WhyHowEmailFacultyPriorLuck,
  title = {Why and How to Email Faculty Prior to Applying to Graduate School},
  author = {Luck, Steve},
  url = {https://lucklab.ucdavis.edu/blog/2018/9/17/emailing-faculty},
  urldate = {2022-08-10},
  abstract = {by  Steve Luck  and  Lisa Oakes   [Note: Our experience is in Psychology and Neuroscience, but this probably applies to most other disciplines.]  It is now the season for students in the U.S. to begin the stressful, arduous, and sometimes expensive process of applying to PhD programs. One common pie},
  langid = {american},
  organization = {{Luck Lab}},
  keywords = {Advice,notion},
  file = {/Users/brookeryan/Zotero/storage/NQ448DJ4/emailing-faculty.html}
}

@online{Womboa,
  title = {Wombo},
  url = {https://app.wombo.art/},
  organization = {{Wombo}},
  keywords = {AI Generated Art,notion},
  file = {/Users/brookeryan/Documents/Obsidian/reference/Womboa-zotero.md;/Users/brookeryan/Documents/Obsidian/reference/Womboa.md;/Users/brookeryan/Zotero/storage/W66VB2AG/Wombo_Ghibli_GroovyTimes.jpeg;/Users/brookeryan/Zotero/storage/ZJ6FF9TS/Dream_TradingCard.jpg}
}

@book{WritingMemoirCraftKing2010,
  title = {On Writing: A Memoir of the Craft},
  shorttitle = {On Writing},
  author = {King, Stephen},
  date = {2010},
  edition = {Scribner trade paperback edition},
  publisher = {{Scribner}},
  location = {{New York}},
  isbn = {978-1-4391-5681-0},
  pagetotal = {291},
  keywords = {20th century,Authors; American,Authorship,Biography,Horror tales,King; Stephen}
}

@book{WritingSolidCodeMicrosoftTechniquesMaguire1993,
  title = {Writing Solid Code: {{Microsoft}}'s Techniques for Developing Bug-Free Programs},
  shorttitle = {Writing Solid Code},
  author = {Maguire, Steve},
  date = {1993},
  publisher = {{Microsoft Press}},
  location = {{USA}},
  isbn = {978-1-55615-551-2},
  pagetotal = {256}
}

@inproceedings{ZeroShotTexttoImageGenerationRamesh2021a,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-07-01},
  pages = {8821--8831},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/ramesh21a.html},
  urldate = {2022-05-12},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/brookeryan/Zotero/storage/7D524AT3/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf}
}

@unpublished{ZeroShotTexttoImageGenerationRamesh2021b,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2022-04-20},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/brookeryan/Documents/Obsidian/reference/zotero/ZeroShotTexttoImageGenerationRamesh2021b.pdf;/Users/brookeryan/Zotero/storage/W478USTV/2102.html}
}

@article{ZoomIntroductionCircuitsOlah2020,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  date = {2020-03-10},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {3},
  pages = {e00024.001},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  url = {https://distill.pub/2020/circuits/zoom-in},
  urldate = {2022-09-06},
  abstract = {By studying the connections between neurons, we can find meaningful algorithms in the weights of neural networks.},
  langid = {english},
  keywords = {MattarLab RNN,Neural Networks Research Project},
  file = {/Users/brookeryan/Zotero/storage/58Z32CGW/zoom-in.html}
}

@book{zotero-undefineda,
  type = {book}
}

@online{ZoteroZotfileMdnotesObsidianDataview2021,
  title = {Zotero -{$>$} Zotfile -{$>$} Mdnotes -{$>$} Obsidian -{$>$} Dataview {{Workflow}} - {{Share}} \& Showcase},
  date = {2021-03-29T22:28:04+00:00},
  url = {https://forum.obsidian.md/t/zotero-zotfile-mdnotes-obsidian-dataview-workflow/15536},
  urldate = {2022-06-09},
  abstract = {This is a workfrow about setting up a smooth(ish) transition from getting a pdf into zotero (which is a reference manager), annotating it, extracting annotations (with or without color coding for that), pushing those into Obsidian using the @argentum’s mdnotes plugin - specifically, the latest alppha release at the time of this post: 0.2.0 alpha4 and have those readily usable with Michael Brenan’s Dataview plugin.  Some Caveats:   The mdnotes plugin I’m using is the current alpha build, that mak...},
  langid = {english},
  organization = {{Obsidian Forum}},
  keywords = {notion,Smart Notes},
  file = {/Users/brookeryan/Documents/Obsidian/reference/ZoteroZotfileMdnotesObsidianDataview2021.md;/Users/brookeryan/Zotero/storage/NHFRDGLZ/15536.html}
}

@online{Zylinska2020AIArtPdf,
  title = {Zylinska\_2020\_{{AI-Art}}.Pdf},
  url = {https://library.oapen.org/viewer/web/viewer.html?file=/bitstream/handle/20.500.12657/40042/Zylinska_2020_AI-Art.pdf?sequence=1&isAllowed=y},
  urldate = {2022-06-10},
  keywords = {ObsCite},
  file = {/Users/brookeryan/Documents/Obsidian/reference/Zylinska2020AIArtPdf.md;/Users/brookeryan/Zotero/storage/TJWQ4ZXS/viewer.html}
}


